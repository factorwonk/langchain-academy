=============================================
File: LCA-LangGraph-C1-M0-L1-v2-Setup.txt
=============================================

Lance: [00:00:00] Welcome to LangChain Academy. We're really happy you're here and decide to take the course, and we're really excited to offer this as our First LangChain Academy course focused on LangGraph. This is Lance. I'll be the instructor. I wanna give some context for the course first, which you may have already heard from the intro with myself and Harrison, but I do want to just reiterate it here.

I'll talk about the course structure. Then we'll kind of dive into the mechanics of how you set everything up. So overall context is that LangChain aims to make it easy to build LLM applications. Agents are one of the most popular LM applications, and there's a lot of interest in what you can do with agents to automate different types of tasks, but they are hard to actually build and deploy reliably.

So this was a really big motivation for us in building LangGraph, which is a framework for building agents and multi-agent applications. It's separate from the LangChain package and the core philosophy is to help developers add better precision and control into agent workflows. Now let's talk a bit about the core structure.

The core structure in a set of [00:01:00] modules. We'll be expanding them over time, so I don't actually lay out here what all the modules are because I want this to be future proof and we're gonna be adding more over time. So the main idea is it's gonna be modules. We're starting with four, and each module focuses on a particular theme related to LangGraph. In this repository you'll see a folder for each module, which contains a series of notebooks, which contain code, and they'll accompany videos which will be accessible to you through the course website. Now, it's also worth noting that notebooks are standalone. So they can actually be run without watching the video and they'll make sense because they contain explanations with the notebook.

But of course it's probably best to do them with the videos. Now, each module also contains a studio folder, which contains a set of graphs to be run with LangGraph Studio. We're gonna talking about all this in detail later, so don't worry about it too much for now. Just understand the course is structured as modules.

Each module explores a theme. There's a folder in the [00:02:00] repository for each module. The folder contains notebooks, which accompany videos, as well as a folder of studio graphs, which you can run in LangGraph studio. So that's the basics. Now I'm gonna walk you through the setup from scratch. And we're gonna go over and do that right now.

So right now I'm at the LangChain Academy, GitHub here. And first I'm just gonna grab the code. So I'm copying this here. I'm going over to terminal, clone it, and I'm just gonna create a new folder of my desktop called LangChain Academy. Cool. So there we have it. Now let's go ahead and open this in a IDE, I use Cursor.

Let's go over and check that out. Cool. Open a folder and let's go to my desktop. Go to the course. There we go. Nice. This is all opened up. Let's create a new terminal and let's look at the readme. So typically I like to create an environment and to install the required dependencies. So let's just go ahead and do that now.

Good. So now I'm in my new environment. Now I'm just gonna install the course dependencies to my environment. They're all laid out [00:03:00] here very nicely for you. Kick that off. This will take a little bit. Now the course is structured in modules. Each module, you're gonna open these folders up and you'll see a bunch of notebooks.

So you can see there's four of 'em now. Each notebook will company a video and you'll have access to all that through the course and to run the notebooks. Now that we've set up our environment, we've run here run Jupyter Notebook, and this should spin up in a browser for you. Cool. So now I'm in the browser.

Very nice. Go to module one. You can see all these notebooks are accessible. Let's open up the first one. There we go. So this is how we interact with notebooks. Let's go back now, let's talk about a few different external APIs we're working with. So we'll be using OpenAI. Just make sure you have an OpenAI key.

You can sign up for one here if you don't have one, and set it in your environment. Now you're, we're also gonna use Lang Smith. I'll go over to show you that page. Now let's have a look at that. So LangSmith is LangChain's platform for tracing observability evaluation. It's very useful for a bunch of things, and we're using it [00:04:00] occasionally throughout the course so you can get it. It's free to use. We already installed this, so this is done. Just go here, set up an API key, and you'll set it in your environment along with LANGCHAIN_TRACING_V2 as shown here. So you just need to do that. Now. This third one, I found it to be a very nice search, API for LMS and rag.

It's free to use. It's easy to sign up. We use it in few lessons in module four, so we don't need it for a while. Now there's other source tools out there. You're welcome to use other tools. You just then modify the code if you want to. But Tavily is pretty easy. Set your Tavily API key. So that's all you're gonna need to do.

Now I wanna introduce and briefly talk about LangGraph Studio. So since the original filming of these videos, we have improved studio and made it runnable from Mac. Windows or Linux platforms. Previously it was only available via Mac and required Docker. So we've removed all those dependencies, so now it's much easier to run studio.

Now [00:05:00] Studio is a custom IDE for viewing and testing LangGraph agents. As mentioned, it could be run on multiple platforms, and basically it works by spinning up a local development server and that server can be spun up in any of the module studio folders. So each module will have a studio folder, and the folder contains the.

Files or graphs that can be loaded in studio and all you're gonna do is just run LangGraph dev. In any of the studio directories. When you do that, you'll see this output and you just navigate to your browser to see the studio ui. So let's check that out now. So you can see I'm in module one studio. I'm in that directory.

I just run LangGraph dev. Now again, I'm in an environment and in this Environment Academy, I've just simply PIP installed the requirements. As we talked about previously, I run langgraph dev. You can see some things run, and you'll see all this in your terminal after you run the command. This shows you that studio is running correctly.

Now we can have a look in our browser to see what it looks like. So [00:06:00] you're gonna see this studio pane open up just right in your browser. It's pretty easy to work with. And just click here and you can browse the different graphs present in the studio directory that you clicked on. Which is pretty nice.

Now we'll show working with this later, but this just shows you how to spin up studio using the local development server, which will work on Mac, windows, Linux platforms. And again, you can spin this up in the studio directory located in any module of the course. So I can go back to the read me. That's really the final step.

So now that you've run through all those steps, let's just talk a little bit through the basics. You're gonna see I added this module Zero folder, which just has one notebook basics. And all you're gonna do is just run Jupyter Notebook. So once you see Run notebooks, you'll see this in your browser.

And I just click on module zero, basics Notebook. And there you are. So here's the basics Notebook. We already talked through some of these fundamentals. You've done the basic setup. Now I just wanna talk [00:07:00] through some of the foundational components for the course. So we're gonna be using chat models quite a bit.

Chat models. Simply take a sequence of messages as inputs and return chat messages as outputs. So LangChain does not host any of our own chat models. We rely on third party integrations. And you can see there's a kind of a nice list of a bunch of integrations that are available to you within LangChain.

So by default we'll be using ChatOpenAI 'cause it's popular, it's performant. Um. But of course there are other models that you can use, and I'll talk about a little bit more about that later. So in the intro, we just talked through setting up your OpenAI key, and this is just a simple little code cell you'll see in some of the notebooks that will check to see that, to make sure that actually exists or that's already set.

And if it's not set, you'll be asked to enter it. Now I do wanna make a note with chat models. There's a few standard parameters. Model is just the name of the model, which you can see we set here. Temperature [00:08:00] is the sampling temperature. The main point here is the lower, the more deterministic, better for things like facts or focused outputs.

The higher, for example, closer to one, better for creative tasks or generate varied responses. And so that's kind of up to you where you want to set that. I, I either don't set it right. I typically use zero because I'm interested in kind of factual responses for many applications I work on. But in any case.

You've already installed LangChain OpenAI, of course, with the initial PIP install here as noted. So all you do is just run this and then you have access to a few different chat models. We instantiate a four oh, we instantiate three five Turbo. Okay, so that's it. Now one thing to note is all chat models in LangChain have a common interface with a few default methods.

I provide a link here. You can browse a few that of interest. We're mainly working with stream and Invoke. Okay. So those are the two methods that we're gonna talk about for the most part in this class. And the main point is these are all different ways to pass inputs to the [00:09:00] model and to get output stream will stream tokens back or chunks and invoke will just call the, call the model and return, uh, the full payload.

So as mentioned, chat models, take messages. Messages have a role, so. IE who is saying the message and what we call a content property. It was just the content of the message itself. We'll talk a lot more about this later, but here's a good example, just as like a kind of simple use case from LangChain core, I import human message.

I, I create a new message that is hello World from name Lance. Okay, so this is the message I create, a message list I can invoke. The LLM or chat model with this list of messages directly and I get you see AI message output. Okay, so hello, how can I assist you today? You get some metadata, like token usage and so forth.

So this is a very basic way to instantiate a chat model and run it on a set [00:10:00] of messages. I do wanna make a note that you can actually pass a string directly to a chat model, and under the hood LangChain will create a human message from it. So this is kind of nice shorthanded in the case that you don't actually want to create a human message.

And so sometimes you may want to do this. For the most part, I'll be creating. Uh, full messages in the course and we'll be using a lot of message lists, but just keep in mind that you can, you can run it just with a string. We can also run three five just to showcase that. So cool. So those are kind of two, uh, nice LLMs, four oh again, $20 a month.

You have access to the four oh series models or the four series models. Three, five if you're on the free tier. Both are good models. I do wanna make a minor note that the interface, for example, these methods stream and invoke, are consistent for all chat models. So if here we ran from, for example, LangChain Anthropic, import [00:11:00] ChatAnthropic, instantiate, an Anthropic model, these methods are the same.

So you can still run all the underlying code in all the notebooks for the most part with different chat models specified. Okay, so that's like a nice point when I say for the most part, I do think depending on the model, you may see differences in tool use or structured outputs or some, sometimes specific chat models have odd requirements on message order, but these are kind of details if you want everything to just work outta the box.

Simply use OpenAI. If you wanna try other models, you're welcome to do so. There may be minor errors you encounter due to idiosyncrasies and tool calling or structured outputs or things like message order. So just be aware of that. But I understand if you have a strong preference for other models, you can consider them.

So I just wanna call that out. Now, there's one other thing I want to cover that was kind of presented in the initial Read Me. We do use search [00:12:00] in module four for our final assistant, and for one of the lessons related to parallelization, I mentioned that we are using Tavily by default. Tavily is a search engine optimized for LLMs and rag.

It's efficient, quick, it gives good persistent search results. Uh, it's easy to sign up and it has a generous, free tier. So I completely understand if you have strong preference against using this, that's okay. You can use other search tools as you see fit. You can swap out that particular search tool. It is only present in two of the lessons, uh, in module four.

So just be aware of that. If you're okay with using Tavily, just set the API key here and here's example usage. So all I'm gonna do is from LangChain community, import Tavily search results, instantiate it and invoke it with a question. Run that here so you can see it runs and you can see here's the output. I get some docs back.

Pretty cool. It has some content like natural language answer as well as the link [00:13:00] that the contents derived from. So Tavily is a really nice search tool I've always enjoyed. It is a good free tier, so it's a good option. But of course, other tools like Google. Uh, we also use Wikipedia, which is free to use.

And so, uh, that is another option. If you exclusively want to use Wikipedia, that's completely fine. Or other search tools also up to you. So this kind of covers basic usage of chat models, messages and search tools. And now we'll dive into the modules, which will really go more in depth on all these ideas.



=============================================
File: LCA-LangGraph-C1-M1-L1-LangChain-Academy-Introduction-Updated.txt
=============================================
[00:00:00] Welcome to module one. Before we start diving into the code, I want to give some brief motivations for LangGraph and also give a roadmap for the overall course so you kind of have a sense for what to expect. So first, a solitary language model alone is somewhat limited. It doesn't have access to tools, it doesn't have access to external context like documentation.

It can't alone perform multi-step workflows. And so many LLM applications use some kind of control flow with steps before and after LLM calls. This could be tool calls, this could be retrieval steps and so forth. And this control flow forms a chain. Now you've probably heard this term chain. You can think about this as some set of steps before and after an LLM call.

Now the nice thing about chains is they're very reliable. So the same flow steps occurs every time you invoke the chain. But we do want LLM systems that can pick their own control flow for certain kinds of problems. [00:01:00] Like you may want an LLM application that can choose its own set of steps depending on the problem it's faced.

And this is really what an agent is. This is at least one simple definition of an agent. It's control flow that's actually defined by an LLM. So you have chains, which are fixed control flows that are set by developer. You have agents which are LLM defined control flows. Now, the interesting point here is that there's actually many different kinds of agents, so you can think about dialing the amount of control you give to the LLM from low to high. On the lower end, you can think about things like routers. This is where an LLM controls a single step in a flow, and it may choose between a narrow set of options. In this toy example, I go from step one, either to two or three, based on the LLM's decision. On the other extreme, you may have a fully autonomous agent that can pick any sequence of steps through some set of given options, or even it can generate its own steps that it can take so it can [00:02:00] auto-generate its own next move, um, based upon some potentially available resources.

Now, here's the catch. There are practical challenges. We can think about application reliability on the Y axis, and you can think about the level of control you give the LLM on the X. Now, typically what we've seen as you ramp up the level of control that you give to the LLM, the reliability drops, so going from something really simple like a router to something much more complicated like a fully autonomous agent, the application does degrade in terms of reliability. So this is really kind of setting up the motivation of LangGraph. LangGraph is aimed to help you bend this reliability curve, allowing you to build agents that maintain reliability, even as you push out the level of control you actually give to the LLM or agent.

And this is kind of a simple kind of statement that summarizes to keep in the back of your head kind of why LangGraph and what are you trying to achieve with it. Now, some of the intuitions here. In many applications, we [00:03:00] want to kind of combine developer intuition with LLM control, and so you can very easily specify certain steps in application that you always want to be fixed.

So you start your flow, you go to step one, you end with step two every time, but you also can interject LLMs at arbitrary points, turning it into an agent, giving the LLM. Kind of arbitrary control over the flow of the application. Now these are expressed as graphs, and graphs are really nice. They contain nodes, which nodes you can think about as the steps in your application.

It can be a tool call, it can be a retrieval step, and edges are just the connectivity between nodes and there's a lot of flexibility as to how you lay out nodes and edges. And we'll get into this in a lot of detail throughout the course. So there's a few specific pillars for LangGraph that help you achieve this goal that we're talking about.

So LangGraph has persistence, it has streaming, it has human-in-the-loop and has a lot of advanced controllability features. And these pillars are gonna be kind of cornerstones of the [00:04:00] modules in this course. And so we're gonna be going to each of these in a lot of detail, and I'll give you a roadmap for that in just a minute.

I do wanna mention that LangGraph also comes with an IDE, so it's basically a visual environment that you can use to debug and visualize your agents that you build. We'll be using this quite a bit. You'll see it's a very useful debugging and kind of observability tool. And I do wanna highlight that LangGraph plays very nicely naturally with LangChain.

So you can think about LangChain as it's another open source library, of course. And it has many different integrations in particular. So there's many different LLM integrations, vector store integrations, and so on. We often use these LangChain components within our LangGraph workflows. So let's take a simple example

I show on the top, like basically a rag or retrieval augmented generation toy application where you have a retrieval step from a vector store, and then we have an LLM step that takes the retrieve documents and answers a question. The retriever itself is a vector [00:05:00] store, and that would be, for example, it doesn't have to be, but it could use a LangChain vector store.

Likewise, the LLM node can indeed use, um, a LangChain integration for that LLM, but you actually don't have to. So you do not have to use LangChain with LangGraph. But we'll be using LangChain quite a bit for these integrations in the course. And you'll see it does provide some, uh, particularly nice features, especially in terms of LLM interoperability.

So LangChain gives you a common interface. A common set of methods for invoking LLMs, which is really convenient 'cause it lets you swap out the model very easily. But you don't have to use LangChain with LangGraph. I just wanna highlight that the two do work very nicely together and we'll be, we'll be highlighting LangChain kind of incidentally throughout the course, but you do not have to use LangChain.

So let's kind of lay out what we're gonna do here. So module one, we're gonna lay the foundations. We're gonna introduce LangGraph studio. We're gonna introduce kind the core abstractions within LangGraph, and we'll actually build two interesting [00:06:00] agent architectures. We'll build a router and we'll build a generic tool calling agent.

And you see these little diagrams from LangGraph studio. So by end of module one, you'll have built two different agent architectures. You'll have spun up studio, you'll know the basics of how LangGraph works, and so we'll really be on our way. Um, and we're gonna be covering that next in the following lessons. Now, module two is gonna move more into memory.

So we're gonna build up to a chat bot with memory that can sustain long running conversations. And this will kinda show you how to use persistence and memory with LangGraph. Module three is gonna move into human-in-the-loop. So how do we interject human-in-the-loop for things like approval or things like editing the state,

in our LangGraph agents, and we're gonna augment our agent from module one with human-in-the-loop before the tool call. This will also introduce streaming. And finally, module four is gonna bring all these things together in an interesting and fairly complex research assistant that can be customized to your liking.

It'll use human-in-the-loop. It'll use [00:07:00] parallelization features like MapReduce, um, and it'll use streaming and it'll also use memory and persistence. So it'll kind of bring together all these different threads and it'll be a fun, kind of interesting, complex agent. That you could easily customize for your own uses.

So I do wanna highlight that modules one through three are kind of laying the groundwork. They're a little bit more foundational. Um, they do exhibit various agent architectures, router and tool calling agent in one. A chat bot with memory in two, and then a generic, generic tool calling agent with human-in-the-loop in three.

But if you're a more advanced user, you can maybe move more fast, more quickly through these modules. And kind of skip to module four where we kind of put them all together into more complex overall agent. If you really wanna make sure you under understand the fundamentals, then you can spend a bit more time on one through three.

But hopefully this gives you a kind of a roadmap of where we're going. It shows you some of the architecture we're gonna be building. And now let's go ahead and dive into the code.



=============================================
File: LCA-LangGraph-C1-M1-L2-V2-TheSimplestGraph.txt
=============================================

[00:00:00] So let's build a simple graph to introduce the core components of LangGraph. We're going to start, we're going to find three different nodes, nodes one, two, and three. And then we're going to end. So the connection or edge between start and node one is what we call a normal edge. So we start a graph, we always go to node one first.

Now you'll see this connection between node two and three is kind of interesting. It's this kind of branching from node one. And we call this a conditional edge meaning that based upon some condition we define, we choose either node two or node three. And we'll walk through all that in detail. And then finally we'll go from either two or three to end.

This is a simple graph we're going to build and just a nice way to motivate the core components of LangGraph. So first we pip install LangGraph. And first recall that we need to define state. So the state is just basically the object that we pass between the nodes and edges of our graph. And in this particular case, we're just going to define state as a simple dictionary, [00:01:00] and it's going to have one key graph_state. That's really it. Now, next we need to define our nodes. So as noted above, we'll have three nodes, nodes one, two, and three. And each of our nodes will take in state. And recall that state is a dictionary. We can extract the value of graph_state the key. It's just some string. And in each of our nodes we're just going to overwrite the value of graph state with something new.

So in this particular case, we take the current value of graph state, we append to it, I am. And we write that back to state. Node two, same thing, except we append happy. Node three, same thing, except we append sad. So that's it. So let's review. We have a state, it's a dictionary with one key called graph_state Each node, simple python function, takes in the state and overwrites the value of graph_state. That's it. [00:02:00] And edges are how we connect the nodes. We talked above about normal edges. So we'll have a normal edge between start and node one. And we talk about the idea of conditional edges. Now here's where we actually implement the logic of our conditional edge.

So we want some condition to decide when to choose node two or node three. And in this particular case, we take in the graph state just like we did before, but we actually don't do anything with it, 'cause this is just a toy example. And all we do is, based upon roughly 50 50 odds we go to either two, or three.

So you can see we just pick a random number between zero and one. If it's less than 0.5, go to two, otherwise go to three. So that's it. And this is just a really simple illustration of the kind of logic you can implement in a conditional edge. And of course, in many more complicated examples, you'll take the state and reason about it in some way to decide where to go next.

But this is just showing very simply how to set up a conditional edge. So again, it's just a function. So [00:03:00] if we go back. Our nodes were just functions. Our conditional edge is just a function. The nodes update the state. The conditional edge just tells us which node to go to next. Now we put all that together into a graph. So we're going to use the StateGraph class to do that.

And we're going to initialize that with state that we defined above. We'll call that builder. And first, just add our nodes. Add nodes, one, two, and three. We'll name them nodes one, two, and three accordingly. And now we define the logic that we talked about above. So we're going to go from start. This is basically import this.

This is just the starting node of our graph. We set a normal edge between start and node one. We then set a conditional edge between node one and node two or three. And see here we just passed the function we defined above this decide_mood and we can even go back and look. This decide_mood is just a function that says go to node two 50% of the time and node three 50% of the time.

That's it. So there's our conditional edge, [00:04:00] and then node two and three, both route to. END We compile our graph so that basically just perform some basic checks and then we can even display it. So this is kind of nice. You can see we start, we go to node one, and this dotted line denotes a conditional edge. So it's either going to do two or three based upon the rule, what you define in our decide_mood function. And then both two and three go to end. That's really it. So it's pretty nice and you can see this visualization matched what we saw, what we saw, and drew above. Now graphs implement a runable protocol. So this is just a standard way to execute various LangChain components.

It could be chains, and in this case, LangGraphs also adhere to this protocol. So this protocol is a few standard methods, which is highly convenient, and one of them is invoke. And all we need to do is just invoke our graph with an initial condition or initial starting value for our state. So recall our state's a dictionary.

It has one key graph_state and we can just simply invoke it with [00:05:00] some starting condition. So I'm going to start with the string. Hi, this is Lance. I'm going to go invoke it and our graph runs. And it appends, recall, in node two and three something to our state. So we go to node one, it appended I am. Nodes two or three appended, happy or sad.

So if I run this a few times, we'll see different values coming out. Now I'm sad, now I'm sad, now I'm happy. And you can see the different nodes that we go to are printed out. So this goes from one to two. Node two appended happy. And let's just do it a little, little bit more here. There we go. So we went and here we went from node one to three, three appended sad and that's how we're getting these updates to state.

Now you see Invoke runs the whole graph, what we call synchronously. That just means it waits for each step to complete before going to the next one. And you can see after the graph runs, just returns the, the graph state after all nodes of executed. In this case, you can see graph state is the full statement.

Hi, this is [00:06:00] Lance. Appends I am sad in this particular case 'cause it went from node one to three. Um, and that's really it. These are kind of the core components of LangGraph and this is just showing you a very simple graph that's just kind of a starting point for some of the exploration we're going to do next.





=============================================
File: LCA-LangGraph-C1-M1-L3-V2-Studio-Final.txt
=============================================

Lance: [00:00:00] So now I wanna briefly show how to work with studio. You may have already seen some of this if you watched our module Zero basic setup video. Um, but I do want to just show it again in case you didn't. So you can see our directory structure looks like this. Each module has its own folder, and within each folder there's this studio directory.

Now we've been working with the notebooks, but you're gonna see here, if you open up studio, what you're gonna have is there's requirements that text file, this has, the package will be using. There's a bunch of Python scripts. These indicate the various graphs we are working with in studio. And then there's this .env file which contains my API keys.

So these are all encapsulated and we can load this directory as a project in LangGraph Studio. So hopefully you've already downloaded Studio. 

The instructions are in the Getting Setup guide

and in the getting setup video.

So [00:01:00] now you're gonna see something interesting. This loads up and you see this agent. This is one of the graphs we're working with in module one. Scroll over to Simple Graph. This is actually the first graph we've worked with and we can go ahead and play with it.

So we'll see a few things here. One, this field allows you to actually input values to the state, just like we did in the notebook. Except here it's an IDE, so I can interact with it visually, which is quite nice. And this thread basically groups different invocations in my graph together. We'll talk a lot more about threads later.

But for now, just think about this as basically like capturing the history of any run of your graph. So let's go ahead and try to kick this off. I add some initial state, just like we did in the notebook. Hit submit and the graph runs nice and quickly. We can see what each node did, which is kind of cool.

So if you kind of go over here, you can see which node is actually running. This is the input, and if I want to create a new thread to hit this, there's a new run. So then let's [00:02:00] just try that again. Go back to simple graph, graph state,

change my name. There we go. So runs again, very nice. And then we can see in this time it goes to node two. So because our conditional edge chooses node two or three at random in this particular invocation, I go to two, we can scroll our prior invocations. I can go back. Cool. This is the one I previously did.

It went to node three, so it was a really nice way to visualize what's happening in your graphs to track the various runs of your graph in these threads. And there's a lot more advanced debugging that you can do here, which is obviously not as relevant for this simple graph, but is very relevant for future graphs we're working with.

And so this is just a basic introduction to how we're gonna be using studio. And just remember that in every module there's gonna be a studio directory and all you need to do to run any of the graphs for that [00:03:00] module, just load that directory in studio just like we did. And everything should work outta the box.

Now remember, if you check the Readme, you do have to set up this ENV file for each one. That's the only thing you have to do manually 'cause it, it contains your API keys. So just make sure you go through that process. It's also in the readme and it's obviously pretty simple. Just make sure that's done and everything else should work outta the box.



=============================================
File: LCA-LangGraph-C1-M1-L4-V2-Chains-Notebook.txt
=============================================
LCA-LangGraph-C1-M1-L4-V2-Chains-Notebook-Low
===

Lance: [00:00:00] So previously built a simple graph with nodes, normal edges and conditional edges. Now it's gonna build up to chains, which will combine a few core concepts. The idea of chat messages, the idea of chat models, the idea of binding tools and executing tool calls all within LangGraph. So first, let's introduce some of these ideas in isolation.

First messages, so chat models, interact with messages. Here's a simple example. I can create a list of messages. So in this particular case, it'll be a conversation between an AI and a human. You can say, I can assign a name to each message as well as content, and we can just print this out. So cool, now I just have this message list.

I can take this list of messages and just pass it directly to a chat model. So first I set, make sure my OpenAI key is set. Then I import ChatOpenAI I specify the LLM model and work with, so in this case, gpt-4o, and let's basically invoke the LLM with that list of messages. So [00:01:00] we can see, we get an AI message out.

Let's actually look at that result. Here's the full AI message. Here's the content, which is a string from the LLM, and we get some response metadata, which will tell us some information about the prompt tokens model name and so forth. So we saw, we can define a chat model, pass a list of messages, get an AI message back out with some content, some metadata.

Now let's introduce the idea of tools, which is another way to use chat models. And the idea here is simple. Sometimes we actually want to connect our chat model with some external tool like an API that requires particular payload to run. So it's pretty easy to do. Let's look at an example here. I'm gonna find a function called multiply takes A and B in.

And all I need to do is LLM, bind tools, pass it that function. Now we have an LLM that has access to awareness of that function. And like you see in the diagram, we can take natural language in [00:02:00] and it can produce the payload necessary to actually run that tool or function out. So let's see this in practice.

I'm gonna take this, LLM, invoke it just like before, but now with a message that just says, what is two? Multiply by three. Cool. And you'll see we don't get content out in that AI message, but we do get a tool call out. So that's kind of cool. And we can see right here the tool call gives us arguments as well as a function or tool name.

So that's pretty nice. We've seen how to use messages as inputs to chat models, and we've seen how to also bind tools to chat models and produce tool calls outputs. Now let's start rolling these pieces all into land graph. The first idea we need to do is how can we use messages as graph state? So here it's pretty simple.

We can just use ideas we talked about previously. Let's define this class messages state. It's a type dick and it has one key messages, which just a list of messages. I. It [00:03:00] seems easy enough. Now, the one catch is you'll remember that we overwrite the value of this key when we perform state updates by default and LangGraph.

But in our particular case here, we don't want to do that. We want to append to this list every time. For example, our chat model produces an output we wanna append to our state, so it preserves a full history of the conversation. This motivates the idea of reducer functions. So when we define our state and LangGraph, just like here, we can have a single key messages.

But we can actually annotate it with what we call reducer function, which tells graph to actually append to this messages list when it receives a new message. So this is pretty nice and we're gonna see a lot more about reducers later. But this is a very convenient built-in reducer function that we often use with messages.

And this is actually so common that we have our own messages state defined here, [00:04:00] which you can use, which has this built-in messages key and ad messages reducer. So just slain to keep in mind. Now let's see this ad messages reducer in isolation. So here's a list of messages. Here's a new message we want to add and let's just test it.

Let's just run this add messages, reducer and see. So cool. It basically appends this message to our list so we can see this is actually how it works. Now let's roll this into a graph. So just like before, let's define messages state. Let's define a single node, which is basically our chat model call with tools we pass in messages from state.

And we build our graph. So our graph simply has the state defined here. We add our node, which is the tool calling LLM, which we define here, and [00:05:00] we add edges. So we start, go the tool calling LLM and then go from tool calling LLM to end. Let's compile and view that. Cool. So we start, hit our tool calling LLM, and we end.

So now let's try invoking our graph with two different types of inputs. First, let's just invoke this with a simple input, like hello, and see what happens. We invoke our graph, so here is the human input and we can see the AI message is, hi, how can I assist you? So this just responds in natural language as expected.

Alternatively, we can invoke our graph with a, an a message that's expected to elicit a tool call. And let's just confirm that works. Cool. So here is the human input and the AI message here is indeed a tool call. So you can see here is the arguments and here is the name, multiply. So it's just like we saw above in isolation, but now we're running this as part of LangGraph.



=============================================
File: LCA-LangGraph-C1-M1-L5-V2-Router-Final-NewIntro.txt
=============================================

[00:00:00] We built a graph that uses messages as state and a chat, with bound tools to either do one of two things, return a tool, call or return a natural language response based upon the chat models decision about the input. So if the input is relevant to the tool, it'll return a tool call otherwise just responds directly.

Now, you can think of this as a simple router. The chat model is actually routing between two different options, a direct response or a tool call. Now, this is a simple kind of agent. The LM is directing the control flow of an application. Either by calling a tool or just responding directly. So here's kind of a cartoon of what a typical router looks like, where an LLM chooses one of two potential paths based upon the input.

So let's actually extend what we did a little bit using two new ideas. One, we'll add a node that'll actually call our tool. So if the model responds with a tool call, we [00:01:00] actually can execute that call in a separate node. And we'll add a conditional edge that lets you look at the chat model output and make a decision, it'll route to our, what we're gonna call our tool node.

If it's a tool call. Otherwise, it'll just end. So let's go ahead and make sure API key set here. Now here's the function we've been using as our tool. Again, I'm going to use our model here. I'll bind the tool. Cool. Now this is where you can use some nice built-in components from LangGraph. So we'll use the built-in tool node you see here.

This is the node we're gonna use to call our tool, and I basically show you here. All I need to do is just pass to that node, our function. There we go, as a list. And then now our tool node is basically instantiated with this particular tool so it can actually execute our multiply function. So that's nice.

And we also add this prebuilt tools condition right here. This is a conditional edge, so [00:02:00] it's gonna look at the output of our LLM, and if that output is a tool call, it'll route to our tools node right here. Otherwise it'll just end. So this is a nice built-in conditional edge that we can use and we can use it with a built-in tool node.

So let's now go ahead and invoke our router. Here we go. So here is an input that's relevant to our tool, recall our tool we defined up here is multiply. So I go ahead and pass an input, multiply three and four. Let's give that a shot. Nice. So here's our message in, the model response with the tool call, this is routed to our tools node as we would expect that the conditional edge sees it's a tool call. It goes to the tools node, it executes the tool, responds with the tool's message. Now, if I just input hello world.

Then you can see human message. The chat model [00:03:00] responds with just a response. Hello, how can I help you? We do not route to the tools node, we just route to end. So this is showing you how the conditional edge will look at the output of our tool calling model and we'll make a decision either to call the tool, if it's a tool call or just to end.

This is like a very basic router you can see that utilizes our built in. Conditional edge tools condition, which is very useful when working with tool calling models. Let's show briefly how to run this router in studio. On back in my IDE open up module one studio. We see the various Python scripts that indicate our various graphs, open up, router dot pi, same stuff we've been working with in the notebook, just captured as a Python script.

Nice and easy. Go to langgraph.json, we see that router is set here. So no problem. Now go over to Studio Studio's Open with this project. Go over to router. There we are. [00:04:00] So let's give it a shot. Hi, I'm Lance Run, and we can see no tool call, right? So we just respond directly. Nice. Try this again. Create a new thread.

Go back to router. Now let's try multiply two and three. Cool. Now you can see in this case there is a tool call. So we go to the tools node. This is a nice way that tool calls are represented in studio. You can see here is the the tool name that it wants to call, and here's the arguments, nicely formatted.

We can go in here and look at them. Very nice. Cool. So there's our tool call and we can see it goes to the tool node, multiply, and there's a result. We get this tool message out so clearly it's nice to be able to look at these different graphs in studio and interrogate how they work. [00:05:00] In this particular case, it's very easy to see how we can go back and look at the prior thread, direct LLM or chat model outputs are routed differently, in this case, directly to the user end, versus tool calls, which are routed to the tools node and then back to the user. So in this case, a user gets a tool message.



=============================================
File: LCA-LangGraph-C1-M1-L6-V2-Agent-Final-Update-Intro.txt
=============================================

[00:00:00] We build a simple router. Our chat model decides to either make a tool call based upon user input or just simply respond in natural language. And we use conditional edge to route between a node that will actually call the tool or just end. So that's the basics of a router. Now you're gonna see something kind of cool here.

We can make one simple modification to this router to turn it into one of the more popular generic agent architectures. So in the above router recall, we invoke the model. If it shows a tool, we return the tool message to the user. But what if we just take that tool message and send it back to the model?

That's the key insight here. So you can see in this cartoon, the router, this is kind of just the router piece. It either ended or returned to this tool's note and returned to the user. Now, what if we just take this output of the tools node? Send it back to the model in a loop. Okay. This is the intuition behind a very popular architecture known as ReAct, and it basically has three components.

[00:01:00] So one is Act. Let the model call tools just like we saw. So that's one. Two is observe, pass the tool output back to the model. That's this red thing. So you call a tool, you send it back to the model, and then reason, let the model reason about that tool output and decide what to do next. Now, what's kind of interesting is.

The model can continue to call tools in a loop until it sees fit or determines that the problem is solved, and then it can just return natural language response and end. But this loop continues as long as the model continues to call tools, unless you add other conditions. And in practice you do, you might say, you know, uh, set a max recursion limit, for example.

But this is the intuition behind a very popular generic agent architecture. Now let's show how we can actually build this pretty simply. We go ahead and set everything up. Just like before, we'll use LangSmith for tracing, so I just make sure I have all those set. Go ahead and run this first, run this.

Good. Now I'm gonna create three [00:02:00] tools and bind them to my model, just like we saw before. Cool. And here I'm just gonna set a system message. So I'm basically gonna give the, what I'm gonna call my agent some general instructions, your helpful assistant tasked with arithmetic. Okay? So this assistant node is basically going to.

Invoke based upon my state, which is a set of messages and this system message instruction. Cool. So that's all of it. That's basically the, the structure and the setup for my system node. Okay. So we can go ahead and build our graph just like before, but you'll see it looks a little bit different. Before we started, we went to our chat model now called Assistant.

Okay. Before we either just this conditional edge, either sent us to the tools node and then we ended, or we just [00:03:00] went directly to end. Now we have this linkage from the tools node back to the assistant. That's the key new piece, which we also showed above. Okay? That's the only thing we did. So what you can see is all we did was in this builder, we added an edge from the tools node back to the assistant.

That's it. Very minor modification, but actually has some profound implications. 'cause now this will continue to run as long as it's making tool calls. So we can go ahead and try to run our agent. Let's basically ask it to call each of our tools, add three and four, multiply that by two, divide that final result by five.

Now we can run this and what we can see is here's our input. Now. First our chat model responds to the tool call add three and four. Tool message is seven. That goes back to the chat model to reasons about it and combines seven and two in that second instruction. That goes to our tool message 14. [00:04:00] Then that 14 is passed back to our model, which reasons about it, and then says, okay, divide 14 by five.

We get the final answer. The final result after the operations is 2.8. So this is a nice, simple example of three sequential tool calls made by our agent. So now that we've run our agent, I wanna show how to go a little bit further under the hood using Lang Smith. So at the start of this course, you will have already set a few different things, but I just wanna review it here.

So Lang Smith is a platform that gives you access to tracing as well as evaluation. And there's quite a few things you could do with Lang Smith. But for now, let's just actually take advantage of the tracing feature and I'm gonna show you how to do that. So a few environment variables you should have already set.

Just if you haven't, make sure you do. So they'll set your Lang chain API key. Now, the most important thing for the purposes right now are just setting this Lang Chain project. This will allow you to log [00:05:00] to a particular project, and I'm gonna go ahead, log to L Chain Academy. Now all I need to do is just to go over to my Lang Smith account, log in with GitHub and we can actually look at the traces from this particular run.

So now I'm in my browser. We can see all we're doing is smith dot line chain.com and this will take me to my Lang Smith landing page. Let's check my projects. So I have this project list and you can see this project Line Chain Academy because I set the environment variable. My project name, it creates this project, which it logged traces associated with that project too.

So I can open this up. So here's the trace associated with that agent run. We just ran a notebook. I can open it up and this is pretty nice. So it logs everything that happened under the hood. I can, for example, click on the initial assistant invocation. So this is just of course chat open ai, and we can see, this is really nice.

We can actually see specifically the functions that are called. This is exactly what's expected. We can see the prompt. So here was [00:06:00] the system prompt. Here was that human input. And then this is again, the three different, uh, function calls that are made by the assistant. That's pretty nice. We can see the functions that are called here.

Of course, these are the payloads, and then we can see we hit the tools condition in our agent. We see that the output indeed contains tool calls, and then we can see these are the tool nodes they run. So each tool, node, add, multiply, and divide runs accordingly. Based upon the tool calls that are passed from the LLM.

Those all run. We go back to the LLM or chat model. We pass in the result from those tool calls like here. And again, we then get our natural language response. So this allows us to go a little bit further under the hood to look at the various steps in the process. [00:07:00] And you can see we get useful metadata, like token usage as well as latency.

So again, we're gonna be coming back to Lang Smith throughout this course at various points in time, particularly when we're using different chat models to look at the invocations, to look at the graph lobes. It's a very nice compliment to Graph Studio, which we're also gonna be talking about.



=============================================
File: LCA-LangGraph-C1-M1-L7-V2-Agent_Memory-Final-Re-shoot-Intro.txt
=============================================
[00:00:00] React is a popular general purpose Agent architecture that does three things that lets the agent act, in this case, call specific tools. Observe past tool outputs back to our LLM to reason about them. And then three, of course, like we just said, reason, reason about the output. Decide what to do next. Call another tool or just end.

This is a very popular, classic generic agent architecture. Now I'm gonna extend this in a very simple way to introduce the idea of memory. So just like before, we'll set our API keys and I'm going to again define an agent with three tools. Multiply, add, divide. Cool. And same as before, we'll set up my assistant node.

This is just our LLM, which has the bound tools. We pass a system message to tell it. Basically, it's at a assistant for arithmetic. Easy enough. Let's [00:01:00] now lay out our architecture. Here it is. We start, the assistant receives user input conditionally that goes to the tools note if it's a tool call or it ends, and then this loops until no tool calls made.

That's it. Now let's run our agents like we did before. Okay? At three and four. Okay, cool. That runs. That's great. Now I'm gonna do, I'm gonna say multiply that by two. Okay. Now this is a new invocation of our graph. Pass in my message. Let's see what happens. Okay. You can see something interesting here. When you say multiply that by two, it seems our agent doesn't know what that is.

We mean seven. We want to kind of associate that with the output of this initial operation. We say that by two. It just basically does two times two. Now, what's going on here? We don't retain a memory of seven from initial invocation because the state is transient to [00:02:00] single graph execution. This was one, a graph execution, which had some state resulting in seven.

This is a second independent graph execution, which has a new, new state and has no persistence between this execution and this one. That's our problem, so we can actually use persistence to address this. This is introducing the idea of a memory. So LangChain uses check pointers to save the graph state after each step, and this gives memory so one of the easiest check pointers to uses memory saver.

This is just an in-memory key value store, and all we need to do is just import and then compile our graph with checkpoint or set to memory, in this case, memory saver. Now let's show what's gonna go on here. Here's kind of a simple toy example. So here's a example kind of graph flow. And this is not a region, this is a more generic graph flow.

I have two nodes. [00:03:00] Each of those nodes is a step, okay? In our graph. Now, what these check pointers do. They actually save the state of the graph at each step as this checkpoint. Okay, so this checkpoint contains things like the state, but also has other stuff. It contains the next node to go to and some other metadata and has a checkpoint id.

Okay? So the key thing to know is this check pointer at every step in our graph. We'll write a checkpoint, which contains the state of the graph at that point. And these checkpoints can be associated together in what we call a thread. So this thread is basically a collection of checkpoints. Okay, so here's how this all comes together.

All I need to do is I supply this config, this configurable, where I supply a thread id. Message input just like before. But when I invoke my graph, now I pass my input just like before. And this config, what's best size A thread id, because I compile with a check pointer. All my checkpoints with the graph state [00:04:00] are gonna be written to, this common thread id, which you'll see very shortly, I can actually reference later. So let's try, let's go and kick that off again. So I go ahead and run this just like before add three and four. Cool. We get seven. Now here's where it's new. I go ahead and pass this thread ID in. So I'll see this config that contains my thread id.

I pass it into the second invocation. I say multiply that by two. But because we supply this thread id, we basically pass the state in that thread, which contains all this information into our agent to continue. So now we run again. There we go.

So we can see this prints out actually the entire state. This is what we had previously, add three and four seven. There it is. Now multiply that by two. We can [00:05:00] see that's what we added here, and it basically builds on this existing state. It knows what "that" is because "that" is seven. Multiply, we get 14, the answer is 14.

So it's a really simple showcasing of the fact that you can compile your graph with what we call a check pointer right here. This check pointer writes the state of the graph at every step. And it allows us to collect those state snapshots or checkpoints in a thread, which we can pass to our graph on future invocations, and it can pick up where it left off.

In other words, it has access to the entire prior state from that first execution. And it can use that too. We just say multiply that by two. It knows what that is because it has that whole initial state preserve. That's it. Simple example of how persistence works, and it is a very powerful mechanism that we're gonna be using in the future.[00:06:00] 

Now let's show how to interact with this agent in Graph Studio. So I'm back of my IDE Open up module one, open up studio, and I see Agent dot pi. Cool. This is the same code we've been working with. Now, one interesting thing you're gonna see here, when I compile my graph in studio, I don't actually need to supply check pointer, and that's because.

Studio is backed by the LangGraph API, the LangGraph API takes it code packages it for you, and it has its own persistence layer, which happens to be Postgres, but you don't have to worry about that. In any case, it's all done for you. So it's not something you have to worry about. I go to langgraph.json and I see there's agent.

Very nice. So when I open up this project in studio. These graphs are available to me. Let's go over studio now. Now I'm in studio. This is the Project Simple Graph Router Agent. We've looked at these two already. [00:07:00] Let's open up a new thread. Here's my agent. Let's give it a shot. Multiply two and three. See what happens.

Very nice user input. Chat model ie. my assistant formulates a tool call. This indicates the tool to run. This is argument one, argument two, which it all derives from this natural language input. This is a very nice way to see how agents or tool calling actually works. Natural language input is turned into a structured tool call, which is passed to this tools node.

The tools, notes, instantiated with my multiply function like we did in the code, and it's able to multiply these two values to produce a tool message out of six that goes back to my assistant. You can see in this final [00:08:00] step, and it answers a natural language, which goes to end, which just is the result of multiplying two and three is six.



=============================================
File: LCA-LangGraph-C1-M1-L8-V2-Deployment-1.txt
=============================================

[00:00:00] Now we've covered a lot of ground in this module. We've built up to an agent with memory that can act, as in it can call tools. It can look at the results of those tool calls, and it can reason about what to do next. It can make another tool call or it can end. And it can also persist state using an in-memory checkpointer.

So we're now going to talk about how we could actually deploy this. And this is one of the big questions that often comes up. Sure, I can build almost all this in a notebook, but then what? Could I actually put this into production? How could I actually do that? So I want to first cover a few concepts and walk through these a little bit carefully 'cause it's very important for understanding the process of deployment.

Okay. First and foremost, LangGraph. That's the library we've been working with. We've been working in Python. This allows creation of agents. We already know all that. Now, LangGraph has an API that basically is bundling the code, iE say, your Python code for specifying your graph. It will bundle that. [00:01:00] It provides a few things on top, which you don't necessarily need to worry about. It has a task queue, it offers persistence.

Now here's the thing. That API then can be hosted via LangGraph Cloud, and we'll show this shortly. This is as easy as connecting to your GitHub repository. And this provides monitoring, tracing, and documentation for deploying graphs, and it's accessible via our URL. So Studio, which we've been working with, actually just uses the API as the backend.

So we've actually already been using the API, we just haven't laid it out explicitly. Now what's cool is Studio can be run locally as we've been doing, or it can work with the cloud deployment. And the final thing is the LangGraph SDK. That's just a Python library for programmatically interacting with LangGraph graphs.

And this allows a consistent interface when working with these graphs, whether they're served locally or via cloud. [00:02:00] And we can connect to them via our cloud URL or our URL supplied by LangGraph Studio. So those are the core concepts. Let's actually just walk through this now to make sure it kind of makes sense.

So, first and foremost, local testing. We've been doing this already. We've been using LangGraph Studio. That has been taking our code, which we've been showing in the IDE, packaging it up with the API, and then we have a front end, this IDE, which is all great. Now we'll see something kind of cool. There's this little URL down here and this URL actually allows us to interact with our locally running API, via the SDK.

So from LangGraph SDK, I'm going to import get_client. I pass my URL from LangGraph Studio running locally, and I can get a list of all the graphs that are running locally in LangGraph Studio. So we can look at the list here. [00:03:00] And it looks like the second one is our agent, so we can go ahead and specify that here.

And there we go. So here's our agent. Now I can create a thread to manage the state and I can go ahead and run that just as I was doing previously in the notebook. So I can specify an input, and here I'm just supplying my thread_id. This was that agent. As you can see, here's the assistant_id. And let's go ahead and try to run it. Cool. So you can see it runs just like before.

Now let's talk a little bit about testing with Cloud. So we can deploy to Cloud via LangSmith, and there's a number of steps that you're going to follow. I'll kinda show the high level points here very briefly. So now I'm in LangSmith, and you see this deployments tab over here.

I can just hit new deployment and this is where I can just connect a GitHub repo. So in this case, I'm in the lanchain-ai GitHub, and I can select a repo that I want to work with. I can name it and [00:04:00] so forth. Now, I've already done this. And I have a deployment called langchain-academy. This is running and it's just connected to that langchain-academy repository that we've been working with.

So you can see this gives us a URL, which is pretty nice. And we have recent traces, which is fantastic, and we have some monitoring here. So this gives us a little deployment for the project we've been working with and allows us to interact with it through a few different ways I'm going to show you.

So first you'll see up here LangGraph Studio. I just click on this, and pretty cool there is Studio. Here's our assistant. Let's go ahead and try this out. So multiply two and three. Let's give this a shot. It's running. There we go. Tool call. There it is. Fantastic. So this all works. Now let's go back and see this API URL? I'm going to copy this over and we'll show that we can also use the SDK to interact with this.[00:05:00] 

So now I'm back in the notebook. We just saw this in our LangSmith account. Now all I need to do is just make sure my LANGSMITH_API_KEY is set. I supply the URL that I just got from my deployed app. So there it is, just change the URL, previously working with the local URL from Studio, and now this is the remote URL from Cloud.

We can see our hosted graphs, just like before. Let's check which one is the agent. So it's the first. There we go. Cool. We'll create a thread just like before and let's go ahead and try to run it. Cool. That all works. So this is pretty nice. It shows in really simple terms how we can go from a GitHub repository to a deployed hosted LangGraph agent.



=============================================
File: LCA-LangGraph-C1-M2-L1-State-Schema-Final.txt
=============================================
[00:00:00] So module one, we laid the foundations. We built up to an agent with memory that can act, for example, use tools. It can observe the output of those tool calls. It can reason about what to do next. Based upon tool call outputs, it can persist that state to enable long writing conversations and we can even deploy it using LangGraph Platform

So now we're gonna go a little deeper into both state and memory in this second module. And first, let's talk a little bit more about your state schema. So when you define a LangGraph, state graph, you specify schema. The schema is just the structure and the types of the data the graph will use. So we've largely been using type dict, which is actually quite convenient and often recommended.

Type dict is just a dictionary where the keys have specific type hints. Now note that these are not actually enforced at runtime. And, but they're very flexible. So in this particular case, let's just try create a new uh, type dict state with name a string and mood as a literal that should be [00:01:00] happy or sad.

Okay. And of course we already showed a lot of this, so you can take your state and you can pass it directly into state graph, just like we saw before. So here's a graph with nodes one, two, and three. And you can see we just have kind of a decide_mood decision rule. And this is looking pretty similar to some things we've seen in the past.

So we're gonna start, we'll go to that first node, then we're gonna go to nodes two or three based on the conditional edge that decides the mood. Okay? So we'll invoke it with a dict. 'cause remember our state is of course type dict invoke with a state Lance, and we see Lance is mood happy. So that's a very simple invocation of this graph using type dict as our state, which we've seen previously.

Now, it's worth noting that type dict is not the only way to establish schema for your graph. For example, Python data classes are another way to define structured data, [00:02:00] and they offer pretty nice, concise syntax for creating data classes. We can see that here. So again, name, string mood, just like we saw before.

Um, we only modify the graphs slightly in this particular case, instead of state name to get it from a dict. In this case, we just dot name to get it from a data class. So very minor difference in terms of how we actually access values from the object. But that's pretty simple. Now let's go ahead and run it.

There it is. Just select before and now we just pass the data class in. And this runs cool runs just like before. So that's pretty nice. Now, one of the problems with both data classes and type dicts that we've been working with is that the type hints are not actually enforced at runtime. So you can potentially assign an invalid value without raising an error.

So let's actually show this. Remember we define data class and that mood we set as a literal, and the type hint said, Hey, this should be happy or sad. [00:03:00] But let's go ahead and assign it mad. No error. So that's an issue. Of course, type checkers will catch it, but at runtime we won't. So pydantic is a really nice way to deal with this problem.

It provides data validation and it's quite popular. So let's define our state use in pydantic. You can see that here 'name' is a string, 'mood' is again, literal, but we can provide this validator on mood that actually confirms that mood is happy or sad, otherwise it throws an error. So let's then try to create an object.

'John Doe' mood 'mad'. And what happens? Good, we give this validation error. Only happier status permitted. So that's really nice. And of course we can supply this to LangGraph as state. There we go. Just like before, and let's try to pass an invalid value for mood. There we go. We get our error just as expected and sad works.

So we can see particular, [00:04:00] pydantic is nice if we wanna apply validation to any of the keys in our state.



=============================================
File: LCA-LangGraph-C1-M2-L2-V2-State-Reducers-Final-Intro-Re-Film.txt
=============================================

[00:00:00] We covered a few different ways to define LangGraph state schema, including TypedDicts, Pydantic objects, or Dataclasses. Now we're gonna divide and dive into reducers, which specify how state updates are performed on specific keys or channels in your schema. So let's go ahead and use TypeDict as our state schema to start.

Here we go. We'll have a simple graph with one node. Cool. There we are. Invoke it and we can see. We return foo of two, we pass foo of one. That's because in our single node, we basically increment the value of foo by one. Super simple example. Now let's actually think about this a little bit more. We look at what we input. We look at what we output, and you can see in the node, all we're doing is basically overriding the value of foo with foo plus one. Now, this is actually kind of an interesting point. By default, LangGraph doesn't [00:01:00] actually know the preferred way to update the state. So by default it just overwrites the value.

So you can see the final state of the graph is just two, because we've overwritten one. With our update that's performed here. So that's just an interesting thing to note that when you define your schema. By default, when you make updates, you're gonna overwrite the prior value in this particular state key or channel.

Now let's look at a simple case of branching. Start. Node one goes to node two. Node one goes to node three. They both go to end. Okay, now let's go ahead and try to invoke that just like we saw before. And I'm gonna do something interesting here. I'm gonna try to catch invalid update errors if they occur.

Uh, so basically you see here, InvalidUpdateError occurred at key foo: Can receive only one value per step. Use an Annotated key to handle multiple values. Now, think about, this is kind of interesting. We talked a little about steps previously. These two nodes two [00:02:00] and three run in parallel. So actually they're part of the same step.

Now let's think about what's happening. If I go to those two nodes, what I'm doing is I'm actually updating that same foo state key in both of them. I'm trying to increment the value of foo by one, but you can see this causes a problem. If I'm incrementing that same shared state key in both places at the same time, it's ambiguous as to which one I keep, right?

So that is actually the reason we get this InvalidUpdateError. It can only receive one update per step. If you try to make simultaneous updates the same key in the same step, it's ambiguous by default because you're overriding the value with two different nodes at the same time. Now this is where reducers come into play.

Reducers give us a general way to address this problem and other problems because they allow us to specify how to perform state updates. Now, all we need to do when we define our state schema, just like this, we just [00:03:00] need to supply this Annotated type to our key and include a reducer function. In this case, we use operator.add function from Python's built-in operator module. This, when applied to lists, this will just perform a list concatenation. So what we can do is we can say our state key or channel foo is a list of events and we're gonna concatenate values to that as we proceed through our graph. So let's go ahead and try this with a single node.

In this particular case, I'm just gonna take whatever's in foo. And I'm going to go ahead and increment that by one and basically update that as a new list. And let's see what happens here. I run it. Now you can see one, two. I don't actually overwrite the value of one in this node, but rather I increment the list or I append a new [00:04:00] value to the list. Which is two. That's it. So because our key foo now has this reducer function, it knows how to handle state updates. In this particular case, because the state key is a list, it will basically just append values to my list. Now let's go ahead and look at an example graph with three different nodes. And build this out.

Same as before, we're gonna do branching, just like we saw previously. And let's go ahead and run this. There we go. No problem. So you can see now this is, this has no issue running because in each of these nodes I'm just appending the value in that node to this foo list. foo has a reducer function as we defined right here, which will just append values to our list foo as we proceed through the nodes.

Therefore, it's no problem to go ahead and update that simultaneously [00:05:00] in the same step. So that's how reducers are extremely useful and actually critical in cases where you have branching and need to perform state updates simultaneously in multiple nodes within one step. Now I wanna show kind of an interesting exception here.

What happens if I pass None to foo I catch the error here and you can see you can't do a concatenation between lists and NoneType right? So you can have certain cases where the input value in this particular case is invalid given our current reducer. Now this motivates the need. Sometimes we want actually define custom reducers to handle particular types of updates.

In this particular case, you know, an edge case around handling null inputs. Okay, so we can do is we can just define an arbitrary function that handles safely combining multiple lists. Um, and we can just simply use that as our reducer. I'm gonna define this class CustomReducerState to [00:06:00] show the one that's using our custom reducer.

Let's try this out. So if I just use my default reducer state and I do what I just did, we get our error just like we saw before. Now let's try my custom reducer state. You see it works. So it, even though we pass in None, it can handle that without a problem. And in our node we go ahead and our we append 2 to our list. So again, just a very simple example of how you can use a custom reducer to handle interesting cases. In this, in this particular instance, our graph can't by default handle null values passed in, and so we have a reducer that allows us to handle such cases. Now, in module one we showed a bit about using add_messages to handle messages in state, and we also showed that the MessagesState is a useful shortcut to work with messages.

It has a built-in messages key and has a built-in add_messages reducer. So let me show you two examples that are actually [00:07:00] equivalent. If I want to, I can always define a custom MessagesState. In this particular case as a TypedDict it has a key messages with that, has a list of messages, and that uses the add_messages reducer.

Okay? And I have some arbitrary keys I can add. Alternatively, I can just use the built-in MessagesState which already has this messages key built in for me. Then of course, I can add whatever keys I want. So typically we'll use this built-in MessagesState for brevity, but I do just wanna show that they're equivalent.

You can always define a TypedDict with, you know, a key called messages if you want to kind of enumerate this every time, rather than using the built-in MessagesState. But I do wanna show that these are actually equivalent. Now let's actually talk about the add_messages reducer a little bit. I have an initial list of messages.

I have a new message I want to add. I just call add_messages the initial list, new_message and you can see it just appends. So it's pretty nice [00:08:00] and kind of works as expected.

So in LangGraph when you're using messages, each message will be appended with an id, allows allowing you to do a few interesting things. So one is if you supply the id of a message and pass it to. Um, the add_messages reducer, it'll simply overwrite the value of the existing message with that same id. So let's look at this right now.

So there we go. We can see it doesn't just append this to the list, but actually changes the value of that human message with that same id. So that's like one nice trick is rewriting. Now the second one removal is a little bit more tricky, but also very interesting. So here's a message list. Each message has an id.

Now, let's say I wanna delete some number of those messages. I can actually delete messages by id using this built-in [00:09:00] RemoveMessage thing. Okay? So I just import that from language from langchain_core and I can iterate through my list. In this case, let's just say I wanna remove all but the two most recent.

Okay? So I go ahead and run this, and you can see that I now have two RemoveMessage Objects with ids specified if I run this with the add_messages reducer. So I've supplied that initial messages list with these delete messages. Let's see what happens. So what's interesting is it go has, and it deletes the two messages per id as specified here.

This is actually a very nice trick for culling and managing your message history, and we'll show how this can be used in a future session.



=============================================
File: LCA-LangGraph-C1-M2-L3-V2-Multiple-Schemas-Re-Film-Final.txt
=============================================

[00:00:00] Typically all graph nodes communicate with a single schema and a single schema contains the graph's input and output keys or channels. But there are cases where we want more control over this. So consider the case one. Internal graph nodes might pass information that's not actually required in the input or output of the graph.

It could be kind of internal communications between nodes, which we don't wanna actually expose to the user at the end of graph operation. We also might use a different input and output schema. Depending on the application, the input might just contain like a user input. The output may be contained several other values that are not relevant to the user at input time.

So let's show a few different ways to customize the schema. So first, I'll show the case of private state. So this is useful for anything that's needed as part of intermediate working logic of your graph, but not relevant to the overall graph input or output. So I'm gonna define an OverallState with one [00:01:00] key and a PrivateState with another key.

So what you're gonna see is my first node will take in OverallState. Now you'll see something interesting here. When I call state colon OverallState. This type hint actually specifies the schema of this state value, so it basically specifies the schema of the node input. Now, this type hint, basically tells us the state that we're writing out to.

Okay, so in this particular case, we're writing out to baz in the PrivateState. Okay, so likewise here in this particular case, we're taking in the PrivateState and we're writing out to OverallState, which indeed has foo. Okay, so let's go ahead and build this graph. Cool. Node one, node two, invoke it. Nice. So we see foo is in the output.[00:02:00] 

Cool. We can see, we define our state graph with OverallState, and because of that, OverallState has foo. Foo is present in the output. Baz is only in that PrivateState. That the nodes interact with in between one another. So node one talks, node two, and they use PrivateState to communicate. Okay? But PrivateState is not in the OverallState, which is passed back to the user at the end of graph operation.

Now let's actually make this a little bit more explicit, or if we want to have a specific input and output schema that may be different, um, on our graph. So first I'm gonna define an OverallState question, answer, notes. I'll have two nodes, a thinking_node, an answer_node. Both are gonna use OverallState, and I go ahead and run my graph.

Cool. Let's run it so you can see when I invoke my graph, [00:03:00] the output is OverallState and it contains these intermediate values, the question, the answer, and some notes. Okay. Now what I can do is I can actually define a specific input and output schema of my graph if I want. Think about this input and output schema as filters on OverallState that specify what I want, the input and what I want the output.

Okay? So think about it like this. Here's my OverallState. The input, I only want to contain question. Okay? The output, I only want to contain answer, kind of intuitive. So the user will pass in a question and it'll get back in an answer, and everything else will be hidden from the user at answer time. So all I need to do is basically define my thinking_node, which takes in the InputState.

Okay? Now this writes out to my OverallState. Answer node takes in the OverallState. Now. This [00:04:00] is where I provide the type hint, because when I define my graph, I supply the OverallState, but I provide these filters, input and output, input as the InputState, output as the OutputState. So what happens is when my graph finishes, this output filter is applied to the state that's returned to the user.

That's why I include this type hint for OutputState. So because my OutputState can answer, only answer is actually returned to the user. So let's have a look at that. I run my graph. And we see only answer is returned to the user because of this output filter that I've applied on my OverallState. So this is a very simple but useful mechanism for restricting what is present in the input and the output schemas of a graph.

And in particular, if you want them to be different, you can apply input and output filters effectively on your OverallState. And we're gonna be using this a bit later.



=============================================
File: LCA-LangGraph-C1-M2-L4-V2-Filtering-and-Trimming-Final-Update-Intro.txt
=============================================

[00:00:00] By now we have a deeper understanding of a few different things, how to customize our graph state, how to define custom state reducers and how to use multiple graph state schemas. So now we'll start using these concepts within LangGraph and we're gonna build towards a chat bot that has long-term memory.

I first wanna talk a little bit about ways to work with messages, because our chatbot's gonna use messages and handling messages with chatbots is actually quite a challenge in some cases. So we're gonna go ahead and get set up here. Now, first let's define some messages. So here's some questions about ocean mammals.

Okay, cool. Now, recall, we can pass a list of messages to a chat model. Great. And we can run our chat model in a simple graph using MessagesState, just like we saw before, that predefined MessagesState. Define a StateGraph pass and MessagesState. We have a single node that invokes our LLM based upon the messages [00:01:00] contained in our graph state.

That's it. Very simple. We can define our graph. Cool. And we can go ahead and invoke it on our list of messages, which we defined back up here. Let's run that. So we just here as your output. This is kind of the input. There we go. And the model's response. So here's where the challenge lies. A practical issue in working with messages is managing long running conversations.

So you have a long running conversation with, for example, an agent. It can actually be very token intensive. Imagine you have a dialogue that lasts for dozens of messages. That's actually a lot of tokens that you're passing into The LLM, which can be costly. In terms of both time IE latency and also actual cost or token usage.

So a few different ways to address this. I wanna show one trick that we previously saw this, remove messages. So [00:02:00] what you can do is we're gonna define a function filter_messages that takes in our state and basically decides, let's delete all except the two most recent messages. And all I'm gonna do is basically apply, RemoveMessage based upon the message IDs in my list of messages, and just simply preserve all.

Preserve only the last two or the two most recent ones, and I go ahead and return these. Okay, so this is a little bit subtle and actually very interesting. The add_messages reducer that is used by default MessagesState recognizes this, RemoveMessage and it will actually remove messages from state based upon the IDs specified in your remove messages.

So this is a very nice and simple way to remove messages from your message queue. Simply by applying this, RemoveMessage by [00:03:00] message id and then using the prebuilt add_messages reducer. Okay, now we saw this previously, but let's put this in practice with a simple graph.

Cool. So there's our graph. Now let's go ahead and create a list of messages. And in this case, I'm gonna kind of create an arbitrary preamble set of messages. Hi. Hi. You know, right. So let's go ahead and invoke that. And you can see we finished invocation because we applied this filter_messages. You can see our state clips those first two messages and preserves the two most recent.

So we clip out this preamble and there we go. Then we get our final answer provided. So a very simple way to use the prebuilt add_message to reducer that we have built in with MessagesState, along with RemoveMessage to delete messages by ID from your message queue. [00:04:00] So if you don't need to or want to actually modify the graph state, you can do really trivial things like just for example, invoking your chat model with some subset of the message list.

So here's an example doing that. We grab the message from state and we just say, okay, just grab me the last one. So easy enough. So let's actually build a message list. So we had a messages. We had a set of messages that we defined previously. We'll append the most recent LLM output to it, and we'll append a new question.

So let's go ahead and do that. And let's go ahead and look at that whole message list now. So here it is. So we have this kind of long preamble. Hi. Hi. You're researching ocean mammals. I wanna know about whales. Here is the. Uh, LLM output. And here's a follow-up question, right? So now what I'm gonna do, let's invoke our graph with this long list, okay?

Now what we want to happen is. In our node, we only want to invoke [00:05:00] the LLM with the last message in the list, but our state will be unchained. So we're gonna actually look in LangSmith to see and confirm the LLM only ever sees that last message. So this is also a good case study for using LangSmith in your development.

So let's go ahead and run. Cool. So that ran, and you can see the state still contains all these messages and it has some answer about Narwhals. Okay, so that's great. Now let's go over to LangSmith and see what really happened under the hood. So now I'm in LangSmith again. Langchain-academy is my project of interest.

Open that up and let's look at this last invocation. And you can see here's our graph. It has a chat model, here's a chat model, and I can open this up to really look at what went on. So you can see this is pretty interesting, right? The input to the chat model is only tell me more about Narwhals and then it goes ahead and gives us a nice response.

But that whole message history is [00:06:00] indeed excluded, which is exactly what we want. So that's a nice illustration and validation that this very trivial example of message filtering worked in our case. So a nice complimentary approach relative to the other methods is trimming. So trimming is cool because you can actually trim messages based upon a specified number of tokens.

And this is relevant because LLMs have context windows with with a particular token length, and we may want to specify the input accordingly. So we can just import trim_messages from langchain_core. You can specify the number of tokens you want, and there's a few tricks here. So you can basically use a strategy last to start from the most recent or final message in the list.

You can allow partial true meaning you can kind of cut messages, uh, in the middle of them. Um, and so let's go ahead and try this out. So first, I've already done these depends. And let's just try trim_messages on this whole message list and see what happens. So if I [00:07:00] don't allow partial, what does it look like?

We only grab the last message, tell me where Orcas live. So that's fine. And if I allow partial, we'll get some of the prior message, but not all of it. So for now, I'll just go ahead and, um, set this to false. Cool. Now let's go ahead and invoke this. I'll set that to false and we can run it. Now I'm back in LangSmith and let's have a look at that.

So here's our invocation and here's our graph. Let's check OpenAI. And there it is. We only pass "Tell me where orcas live!". And this went ahead and ran quite quickly because the tokens were quite small. So we're back in the notebook. And that showcased that trimming is another nice way of restricting the total number of tokens that are passed into your LLM from long running conversations of messages.



=============================================
File: LCA-LangGraph-C1-M2-L5-V2-Chatbot-memory-Update-Intro.txt
=============================================

[00:00:00] We've covered how to customized graph, state schema and reducer. We've shown a number of ways to trim and filter messages. Now, I wanna take this one step further and show a very practical and useful trick that actually uses LLMs to produce a running summary of a conversation. It's another means of compression that tries to preserve information better than just filtering or trimming old messages.

So here, let's just get set up. As usual, define our LLM. And we're gonna use MessagesState just like we have, but I'm gonna add this key summary. So remember, MessagesState has a built-in messages key, just like we've been using. But I'm gonna go ahead and add this new key that's gonna contain a running summary of the conversation.

So I'm gonna create a node here called call_model. And you're gonna see just a few simple things. It gets a summary if it's present. If there is a summary, it adds it to the messages. That's really it. And. If not just [00:01:00] grabs what's in messages as it's messages, invokes the model. That's it, and then goes as and then returns the response, writes that out to messages.

And we're also gonna create a summarize_conversation node. Which will take in the state. Again, see if there're an existing summary. If there is, we'll basically add our existing summary to the new summary prompt. Okay. So basically saying, Hey, this is a summary of the conversation to date. Extend the summary by adding new information.

Okay. Otherwise, we have a really simple summary prompt that just says, create a summary of the conversation above. That's really it, and we go ahead and. Basically pass our messages as well as our summary message, the instructions as to what to do, invoke our model, and this will just produce a summary of this, whatever's in our messages and.

Now we go ahead and use this remove messages trick like we've seen before to delete all the messages except the two most recent. This is of course, tuneable. [00:02:00] So then we update our messages key with delete_messages. Remember we saw this previously. This will just remove all the messages by ID from our queue in messages, uh, that we specify here.

And we're gonna add our summary to the summary key. That's really it. Very simple. This is just gonna be a, a, uh, conditional edge that'll determine whether or not to continue or to produce a summary. And so basically we set a, we set a, a arbitrary filter here, if greater than six messages, go ahead and produce a summary. You can tune this as you see fit though.

Now we know that state is transient to single graph execution, which limits our ability to have multi turn or long running conversations. So as we talked about the end of module one, we can use persistence to address this. So we talked about check pointers before, and we're gonna go ahead and use the MemorySaver checkpointer, which is an in-memory key value store.

And all we need to do is just [00:03:00] compile our graph with our checkpointer and it gives us memory. So let's draw this. Cool. So we're going to ask our chat bot questions and over time the chat bot will accumulate a message history, and this will be aided by our checkpointer, which we'll see. And if our message history exceeds some number in our particular case, six messages, we'll create a conversation and we'll call all but the two most recent messages from our state.

And we'll proceed. So this is a way we can basically create a running summary of the conversation that's compressing all the old information we've talked about and it can proceed indefinitely, particularly 'cause we're gonna use a checkpointer or which persists the memory of our conversation over time.

Now, I do wanna refresh something. We talked about this a little bit before, but the checkpointer saves our state at each checkpoint. So think about basically each step. [00:04:00] Okay, in our graph, and at each step our checkpointer saves a checkpoint, which contains the state of the graph, and that's all collected in this thread, which is referenced by a thread id.

We talked a little bit about this previously, but I just wanna show this diagram again to kind of bring it home. We create a thread. We specify an ID here. Now here's where I can start a conversation. So here's my initial invocation. Okay. "Hi, I am Lance". And because I'm passing a thread, I can pick up from where I left off.

So we're gonna test it. What's my name? And then we're gonna say, "I like the 49ers", a football team. Okay, so let's kick that off. Cool. So passing our name, the AI responds, and then we ask what's my name? And it responds with "You mentioned your name's Lance". How can I help you? I pass in. "I like the 49ers".

And AI responds, "That's great to hear". Cool. [00:05:00] Now you'll notice something interesting. We don't yet have a summary because we still have less than six messages. Right. As we defined here. We can check that right now. So let's say I, I'm gonna add another uh, kinda response here. You know, I ask about one particular player on the 49ers.

So there we go. So this is one more human input. We'll have one more model output that should get us to, I believe, six messages or more. And let's check now. There should be, and here we go. There is indeed a summary of the conversation. So, you know, it talks about, we introduced myself, we expressed support for the 49ers.

We ask about Nick Bosa, right? So this is a compressed representation of the entire conversation we've had. So if we kind of recap, we've set up an assistant that has the ability to summarize conversations once they get too long. IE beyond six messages. And this running summary lives in state. We use a checkpointer to persist state through time so we can keep having a conversation with this [00:06:00] assistant over time.

And A, we're just, we're going to use. Uh, our checkpointer to maintain that state over time. And B, we're gonna produce this running summary so the conversation within state never gets that long. Those some nice tricks. So let's go and have, and have a quick look at this in LangSmith to confirm this is actually what's going on.

So now we're in LangSmith, just like before we're our langchain-academy project, and you'll see something kind of interesting. So now there's a few different graph invocations for each of our inputs. You see these were independent invocations and we use that external memory to persist this conversation across each invocation.

So let's have a look at the first one. So the first one was just pretty simple. It was, hi, I'm Lance. The model responds, so, fair enough. Let's go to the next one. So then I ask what's my name? So, and it has the full message history. So it [00:07:00] has, hi, I'm Lance, the response, what's my name? Your name is Lance. You introduce yourself as far the conversation.

Now this is a subtle point. If you look at the, this particular invocation, the only thing that I passed was what's my name? But because we had the full conversation in state, we actually appended my initial input. What's my name to. That full conversation that lived in state and that entire conversation is actually passed to the LLM.

So this whole thing is because my checkpointer saved all these updates and I'm just appending my most recent input to that saved state and passing it in. So that's kind of the way to think about what's going on here. Now I can kind of keep going here we can see this third invocation. So now again, this is, start a conversation, AI response.

Human, AI. Human, AI. So now we're at six. [00:08:00] So let's look at one more. And now you recall after six we'll start producing the summaries. And that should happen here. So first we're gonna see, here's the whole conversation and I ask about one particular player and it answers. Now this should continue, will now trigger.

Go to summarization. So now it goes to summarize_conversation node and we can see what happens here is we can look at ChatAnthropic. We basically pass in this full conversation to the model and it produces a nice little output for us. So what's pretty cool is this is exactly showing you how we go from a kind of simple conversation that takes place across different invocations.

To, a kind of continuously updated internal summarization state, uh, that allows us to have a long running conversation with our assistant without incurring a great deal of token usage because we compress that conversation as we move forward.



=============================================
File: LCA-LangGraph-C1-M2-L6-V2-Chatbot-external-memory.txt
=============================================

[00:00:00] So we built a chatbot that supports long running conversations in two different ways. One, it has memory. We use an in-memory checkpointer that allows us to persist the memory of the conversation as long as in our case, this notebook session is in existence. The second is we do a summarization of the chat messages.

After some number of messages have been created, which reduces the overall token usage, making it more feasible to have very long running conversations. Now one catch is that, of course this chatbot can't persist indefinitely. It only lives throughout the lifetime of our notebook session here because we're using an in-memory checkpointer, but we can do a bit better than that.

LangGraph supports a few checkpointers that work with external databases, so here we'll show SQLite, but there are some others to be aware of, like Postgres, for example, that support production. So SQLite is a small, uh, fast, very popular SQL database, and it's a [00:01:00] very good starting point for working with external database checkpointers.

So all we do is import SQLite here, and if you pass memory here, when you connect to it, this'll work in memory, just like we saw before. However, if you pass this, it'll actually connect to a database at this pass. Let's try that out. Cool. So now we can define our checkpointer as noted here, and we'll call that memory, connect to it, and then we're gonna create our, our chatbot.

Just like we saw before, this is the same code we already worked with. Let's just create that, have a look at it and we can visualize the flow. So we start, the AI responds and after some number of responses in our particular case, you can look at the edge here, six messages. It'll produce a summary [00:02:00] and that'll occur in this node here.

And that summary will be saved to our state because we define the summary key and it'll just be saved there and it's accessible. And then. We call the model, we check the summary exists. If it does, we append it to our messages and we keep going. And that's how this is able to sustain a long running conversation.

Now let's kick this off. I'll create a new thread id. Again, I'm using my SQLite checkpointer. I'll pass in my input.

There we go.

So I kick it off with "Hi I'm Lance". We get the AI message response. So if I wanna get the state of my graph, I can just simply pass this config with a thread ID to get_state and I can see the state. So there we go. Now we get a StateSnapshot and we can see the, the current, uh, set of messages in the graph.

Now, the benefit of using a check pointer like SQLite, is [00:03:00] that we're actually writing to a local database on my machine, so it's persisted over time. So, for example, I'm just gonna go ahead and restart this notebook. So I'm gonna interrupt the kernel, restart it. And so now, if I was using an in-memory checkpointer like we've been using previously, the history would be removed.

It'd be wiped, right? Because it'd be living in memory in my notebook session. But because, and now we're using SQLite checkpointer. This state is actually saved locally in my machine. All I need to do is go back up, redefine my graph, because again, our notebook has been restarted. So let's just go ahead and redefine everything we went through.

We've recreate our memory saver and we recompiled our graph. So now we have a graph again, but note, I'm not gonna rerun it. I am just gonna go down and check that I can grab the graph_state, even though we're in a new notebook [00:04:00] session by supplying this thread id. There it is. So that's the state that we've saved locally to disk.

That's persisted even though we're in a new notebook session. So we've shown how the SQLite checkpoint allows you to persist state across different notebook sessions because it's actually living on disk in a SQLite database. Let's go ahead and show now the same chatbot running a LangGraph studio. So LangGraph Studio has its own built-in persistence layer that you can utilize that gives you kind of similar functionality across different sessions.

So now I'm in my IDE, we're looking at the raw code, and here's chatbot.py. This is just the chat chatbot that we've defined. Same code does in the notebook. Remember, I go to my langgraph.json. I add it. So this is all pretty standard. We've already done this previously. Now remember, this py file is gonna get packaged by the API and it's gonna add a persistence layer automatically for [00:05:00] us.

That can be used and is used with Studio. And that persistence layer actually is Postgres in the case of LangGraph Studio. So it's kind of pulling all these threads together. We've been using Studio this whole time. It has built-in persistence for us through the API, which is actually Postgres. So it's really nice.

And now let's go over to Studio and explore this a little bit further. So now we're in Studio and some of these concepts are gonna make a little bit more sense. You can see we have a new thread. We have our code here. This is our chatbot, which has been packaged by the API providing persistence and we can interact with it.

This is all running locally, so just like before, let's just say, Hi, I am Lance. Cool. Submit that and good. Okay. The AI responds. That's totally fine. I, I can keep interacting. Say, I like the 49ers. Great. So it's gonna go ahead and respond [00:06:00] fine. And I'll ask who is their coach?

Yep. Great history. So there's the head coach, Kyle Shanahan. Yep. So we get some interesting information there. And recall, we're gonna, we should be producing this summary once we hit over six messages. So that should kind of hit at the next, uh, round of conversation here. So tell me about his father, who is a famous coach as well.

Evidently. So there we go. We learned about Mike Shanahan here. We're getting some information now. It's still running and out. There it is. That's pretty cool, right? You can see summary's conversation hits. Now we just running summary of the conversation. We nuke a bunch of the older messages. Pretty cool. So we can kind of see how this is all working in real time.

Now you can see right here we have a thread ID associated with this conversation. Let's go ahead and create a new thread.

Hi, this is. [00:07:00] Hi, this is Lance.

Tell me about the 49ers biggest. Tell me about the biggest rival. Right. Okay, so start playoff battles with the Cowboys and the, yep. Eighties, nineties, super Bowl contenders, the whole thing. Right? So kind of cool, and let's go back. We have persistence here. I can go back to that discussion about the 49ers, and in this case, Mike Shanahan, Kyle Shanahan's father, and let's just convince ourself we can pick up where we left off.

Um, who was Mike Shanahan's best player. Right? And there it is. So, you know, we have local persistence through Postgres in this particular case because the LangGraph [00:08:00] API is packaging our code with a persistence layer automatically that is serving the Studio here and it can manage all our threads here through Studio.

Looks pretty nice.



=============================================
File: LCA-LangGraph-C1-M3-L1-V2-Streaming-Interruption-Final.txt
=============================================

[00:00:00] In module two, we built a deeper understanding about state and memory. We built up to a Chatbot with external memory that can sustain long running conversations. So this module's gonna dig into human in the loop, which builds on memory and allows users to interact with graphs or agents in many different ways.

Set the stage for this. We're really gonna talk about streaming first. So LangGraph has first class support for streaming, and we'll showcase this using the Chatbot we worked with in module two. So let's just spin that back up. Just gonna rerun this and you can see this is the flow we talked about previously.

Now let's say I wanna stream the full state of this Chatbot as it's running. Okay? So in LangGraph there's two general methods for streaming .stream and .astream. They're sync and async, but interestingly there's different ways you can stream the state. So one's what we call values. So this streams the full state of the graph [00:01:00] after each node's called.

So here's a cartoon of that. We go A, A B, A B C. Each node is only adding A, B, or C, but the accumulated full state is printed. When we stream values. Okay. Alternatively, we only stream updates. So that's basically just the state updates by the node after each graph is called. In this case, the first node adds A, the second node adds B, the third node adds C.

So updates versus values. That's kinda the first concept to understand. Let's have a look at that. Let's try updates. So I'll create a new thread_id. I'll stream, call graph.stream here, passing an input, specify updates, and let's see what happens. Cool. So you can see something interesting here. We get a dict, the node name, and then the state update by that node.

Cool. Now we can just print state update, of course. There you go. Very [00:02:00] nice. So then here we're just selecting the node that we wanna look at and we're just grabbing the state key as noted. So that's easy enough. Now let's try values. So this is the full state after, of our graph, after the particular node's called. I will create a new thread_id and I'll kick that off.

There you go. So here we start with, Hi, I'm Lance. And then we see the full state. Hi, I'm Lance. And then the AI message of response. How can I help you? When that conversation node is run. So we've seen there's two different ways to stream the state. One is updates where we just stream the updates to state after each node's called.

The other is values, which is the full state after each node is called. So the difference here being when the conversation node's called [00:03:00] in the second case of values, we see the full state both the user input and the AI message response printed out. Whereas we use updates, we only see the AI message printed out here because this is what the, the state has actually been updated with once that node runs.

So let's talk about streaming tokens. Many graphs contain LLMs. LLMs produce tokens. And it's common to want to stream the tokens as they're generated by the LLM. So we can do this with the .astream_events method. And the events that come out have, uh, are a dict with a few different keys.

So we can see there's an event key, which is the type of the event, name key, which is the name, data associated with the event. And then there's this metadata key, which has a few useful things in it, including langgraph_node.

So let's have a look at this. Create a new thread here. We'll go ahead and pass in input, Tell me about the 49ers, as they're one of the [00:04:00] NFL teams. Again, I just call graph.astream_events, messages. And let's just print some information about each event as it comes out. I'm gonna print metadata. And if the, if the langgraph_node is present, print that out. I'll print the type of the event. I'll print the name of the event and let's have a look at this.

Okay, cool. So we see lots of things coming out here. Now here's where it gets a bit interesting. See this right here? We can see when we hit that conversation node. That's where our LLM is running. We see a whole bunch of events of type on_chat_model_stream name, OpenAI, so this is kind of a hint to that.

This is probably the events that are being emitted by our LLM, which is pretty nice, within this node. So we can actually isolate those, of course, because we have all the information we need. Define a new thread here. Pass in the same input. But now let's just have a few checks. So if it's an [00:05:00] on_chat_model_stream event, as noted here, these are events that come from a chat model, in this case, OpenAI.

And, this is the key point. We can look in metadata, get the langgraph_node. And let's say we have a node in particular that we wanna stream from. In this case, let's only stream from the conversation node. Now in our graph, that's the only node that actually has LLM calls in it. But you can imagine a graph that has many different nodes with different LLM calls and you may wanna isolate streaming from one particular node.

You can easily do that exactly as we show here, and let's go ahead and print that out. Nice. Okay. This is cool, right? So you can see these, this content. This looks a lot like LLM answer, as you might expect. Pretty nice. So let's take this one step further and we're gonna create a new thread. Same input, and same deal here, but let's just do something simple.

For every event. If it meets this criteria, [00:06:00] let's just get the data and we'll go ahead and print. This data object has a key chunk, which you can see right here, right? This is just a partial AI message. Let's get the content. We'll print that content. We'll add a little delimiter here. There you go. So now you have kind of nice streaming of just the content in each AI message chunk, which you can look at really nicely here.

So let's look at streaming with the LangGraph API. So our Chatbot is running locally in Studio. And the API serve as the backend of Studio, so it's accessible via the SDK. And all I have to do is supply this URL, which we've talked about before. So I supply the URL, I use the SDK to get all the graphs that I'm running.

So let's have a look at them. So we can see, I have this whole list of graphs. That's great. And we're gonna go ahead and specify. I wanna look at the agent graph. And what I'm gonna do, I'm gonna create a new thread here. So all I have to do is [00:07:00] client.threads.create to create a new thread with for the API supply an input message.

And I'm just gonna use stream_mode values just like we did before. Now let's print these raw events and just have a look at what's going on here. Okay, so these are the events coming up from streaming. I can see that I have this event metadata, event values, and I have data. Okay, so you can kind of intuitively see what's going on.

This event appears to be the type. The data appears to be the payload and that's it. The event indeed is the type, data. Is the state. Pretty straightforward. So let's just run this again and we can clean it up a little bit. We'll go ahead and get any, get our state from data, noted here. And we'll just check if there's a messages key in there.

In this case, there is. If there's messages, we'll go ahead and convert them to messages using [00:08:00] this convenient little helper. And let's go ahead and look at that. Nice. There it is. So there's your user input. There's the tool call, tool response, final chat model output.

Now, because messages are such a common state when working on chat models. In LangGraph, there's actually a streaming mode for messages specifically that is only supported in the API. So this is actually a really nice thing to be aware of and let's talk about it a little bit right now.

So, the mode assumes you have a messages key in your state, which we've been talking about quite a bit. That's actually very common in many graphs. And these emitted events have two attributes. The event, the name of the event. Data, the data associated. So that's actually same as above. Now I wanna go ahead and show you, let's look at this very quickly here. I'm gonna run with messages mode. Same graph. I'm gonna run agent, same input.

You're gonna see some interesting things here. So now the types [00:09:00] are a bit different. You're gonna see metadata, you're gonna see messages/complete. You're gonna see messages/partial. So let's kind of walk through these. Metadata is just metadata about the run. Messages/complete is a fully formed messages. And messages/partial are LLM tokens that are emitting from the chat model.

Now we talk a little bit more about these types in our documentation, which I encourage you to have a look at here. But let me just show you kind of the, the key point here. This is just a minor helper function to format tool calls. And what we can do is, let's run the same thing again, but we'll just basically if the event's metadata, we'll print that out.

And if the event is messages/partial, we're just gonna do some processing here. So we'll basically get the data from that. We'll look at the role and we're gonna go ahead and format that output accordingly using this logic. So let's have a look at that here quickly.

So here's the [00:10:00] metadata printed. What's really neat here is you can see we're actually streaming the LLMs tool call, which is pretty nice. Okay, so you can see this is a tool call and we're actually streaming as it's being formatted or being generated. And then likewise, we're also streaming the AI model response, and this is the final output. The result of multiplying two and three is six. So what we're doing here is streaming the output or messages from the chat model, whether they're tool calls or whether they're just, they're just natural language responses.



=============================================
File: LCA-LangGraph-C1-M3-L2-V2-Breakpoints-First-Half-Final-1.txt
=============================================

[00:00:00] Streaming allows us to emit the graph state at every step. And this sets up human in the loop. There's at least three really good use cases for human in the loop. One is approval. So what if we want to approve certain steps that our agent should take, like tool use as an example. Some tools are sensitive, like write into databases or external services.

And we may want to actually approve it as a human before we allow our agent to execute those types of actions. Another is debugging. So that's like rewinding our graph to reproduce prior issues or avoid future ones. And a third is directly editing or modifying the state of our graph or agent with human feedback.

So LangGraph offers, uh, a number of ways to execute human in the loop. And let's first introduce breakpoints. So let's reconsider the simple agent we worked with in module one. And let's assume we're concerned about tool use, so we actually want to approve the agent to [00:01:00] use any of his tools. Now, in this case, the tools are pretty trivial, but this shows the concept.

All we have to do is compile a graph with this interrupt_before and select the node, in this case, the tools node, which is where our tools are executed. So this means the graph will run up until this node and it will stop. You can also use Interrupt_after. In this case, we'll use before, but they're both available to you.

So first I'm gonna redefine the tools you worked with previously. This is the same agent we worked with back in module one. Now let's lay out the graph. We've already seen this. And now let's see something cool. I've built it with this interrupt_before tools. And you see now in my tools node, interrupt before is flagged.

So I'm gonna run it with initial input. I'll set a new thread. And let's go ahead and see what happens here. Cool. So we see human message goes in. That's what I pass. The AI message returns to [00:02:00] the tool call, but then we stop because we're we passing our input here. Our assistant or chat model responded.

And because it's a tool call, we went to the tools node and then we stopped. Now because we're stopped at a particular point. We actually can look at what the state is. And we can see what Node is going to be called next. Let's run that quickly. So we can see. The next node to call is tools. This is a way to confirm that we're actually stopped.

Now. I wanna pause here a little bit and just walk through a few of the things we've been talking about. So here's a graph. A graph's a control flow of nodes and edges. We've been talking through that quite a bit. Each graph has some number of steps. Okay. In our case, we can look above, the steps are assistant, tools and so forth.

Now, each step as our graph executes has a state, which is [00:03:00] written by our check pointer as a checkpoint. This contains the state and some other useful metadata like we see here, where to go next, and a checkpoint ID and other things. Okay. The thread is just the collection of checkpoints. Okay.

StateSnapshot, it's just a type for checkpoints. Now here is where we're segueing to what we just did. When I call graph.get_state, I'm just getting the current or most recent checkpoint. That's all I did get_state, and we can even look at that. Let's have a look at that quickly. There it is. So you get this StateSnapshot.

This is the current state of the graph. Pretty nice. I can also call, get_state_history to get the full list of checkpoints. Now here is where we're gonna introduce a nice new trick. If I call graph.stream, pass in None, give it a thread id, it will re-execute from the current checkpoint. Pretty nice.

That's exactly what we're gonna do right now. [00:04:00] So let's go down here and look. I'm gonna call graph.stream. Pass in None. Pass the thread ID. We know based upon get_state that the current state is. I'm gonna run the tools node next. I'm currently here. I have the AI message in my state. So what's pretty neat is, when I run stream, pass in None with my thread id.

Let's see what happens. Very nice. So let's walk. Look at this. We reemit the current state, which is pretty nice, just as like a refresher reminder of where we are. And then we go ahead and execute the next nodes. We go to the tool node. We execute the tool, pass that back to our, uh, chat model, and we get the answer.

It's pretty nice. Now let's create a new thread id. We'll bring these all together and we'll have a specific approval step. I'm just gonna add this user_approval input here, and if it's yes, just continue just like we just did. [00:05:00] So let's go ahead and look at this. There we go. So we get the human message in, the AI message responds to the tool call, and then we ask for our human input.

So I'll say, yes, I want to continue. Then you're gonna see, we're just gonna invoke the graph with the current thread, which we'll just start from our current state, which is this AI message here, and it'll continue. There we go, remit the current state, and then we proceed. Tool message, AI message, and we're done.

So that's how breakpoints work. In simple terms, it's very nice, very easy to use. You can stop a graph at any node. You can then proceed or continue just by passing None with a thread id, and it'll pick back up from the current state of the graph.

Now let's talk about breakpoint using the LangGraph API. So shown in module one, my agent is running in Studio locally. And all I have to do is go ahead and get the URL for my Studio. [00:06:00] I can go over and show you that right now. So here we are. Here's agent. Grab that URL right here, and I'm back in the notebook. Paste that in. There we go. The assistant_id is just agent in this case, can see here, and I'll create a new thread.

Cool. Now I interact with it just like before. Pass my HumanMessage in. Now here's a new thing. I can pass interrupt_before to my agent through the API. This is pretty nice. Recall previously, we had to define it when we compiled the graph. With the API you can actually pass it in. So you can interrupt_before whatever node you specify and you can pass that in as one of the arguments to the API, which is quite nice.

So let's give that a shot. I'll run this. Cool. So just like before, same deal. Here's the user input. Here is the tool call, and this is the same thing we already [00:07:00] did. Pass in None input to proceed to say, I'm, I'm satisfied. Keep going. And there we go. Reemit the current state, the tool call, go to the tools node, get the response back. Go to, uh, go back to the chat model for the final response.

So the real new thing here is that I can apply this interrupt_before via the API, which is quite nice. You don't always have to define it in code, but of course you can. Now, let's go show that right now. Let's say I want to actually define this in code and work with it in Studio.

So now I'm in the repo. Go to module one. Go to studio, go to agent.py. Let's scroll down here. Now see this compile. Let's just add to interrupt. There it is. Save this. And what's nice is we can go over to Studio and see.

[00:08:00] Just restart this and now let's run it. There we go. So you see we hit the breakpoint at the tools node. And this is kind of a human approval step so we can say Yes, I approve. Continue. There we go. So we finish. So you can see with the API, there's two nice things you can do. One, you can actually pass interrupts via just the arguments.

Let's go ahead and look back at that in the notebook here. You can pass interrupt_before directly to stream method, and this is one way to implement a breakpoint. Or you could, of course. Can define it in code, just like we've already been doing in the notebook.



=============================================
File: LCA-LangGraph-C1-M3-L3-V2-Editing-State-Final.txt
=============================================

[00:00:00] We discussed a few general motivations for human in the loop, like approval, like debugging, like editing. We've shown the first ones. We talked about how you can use breakpoints to stop the graph and approve certain actions like tool calls, but we don't yet know how to actually edit the graph state once it stopped. Which is really that third use case we talk about above.

So let's discuss it here. We know we can use breakpoints. We know we can use them for human approval. But breakpoints also give us an opportunity to actually modify the graph state because the graph is stopped and we can manipulate and grab the state as we want. So let's redefine our agent. Again, perform simple arithmetic and, and now we're just gonna add interrupt_before assistant.

So very minor change relative to what we did before. So now you can see same flow except we'll interrupt before we actually run the assistant. Let's go ahead and run this. Create a new thread, [00:01:00] pass the input, multiply two and three, and let's give this a shot.

Now you can see nothing happens. We see the human message go in, but we've stopped before the assistant has an opportunity to do anything. Now what's kind of cool is, again, like we showed, you can run graph.get_state, pass in the thread, and you can get the states. We can see here's a StateSnapshot. It only contains that human message input. Now here's where I wanna show you something new. We can apply state update.

So here we're gonna go ahead and pass a human message that says No, actually multiply three and three. There we go. All we called is graph.update_state. Now let's just go ahead and look at the new state of the graph. Here we go. So there it is. Now we have two human messages. Remember, when we update the state, we pass in a message.

It will use the existing reducer [00:02:00] that's already defined for this particular key messages, in our graph. The messages key used as the add_message reducer, which will append messages to this key or channel on our graph. Unless certain conditions are met. Like for example, if the message ID is identical, it'll overwrite it.

But in this case, I'm just gonna append because I haven't passed a message ID here. So now we're passing in two human messages. Multiply two and three, no, actually multiply three and three. Now all we need to do is pass in None to proceed from our current state, which is applied by this thread. Again, let's stream and see what happens.

So there we go. This is correct then. The LLM sees this updated message, formulates a tool call, it says multiply three and three. We get the tool message output as nine. And now actually, if we want to finish this, all we need to do [00:03:00] is run this again because we've actually stopped again at the assistant node.

If you recall, we've an interrupt before assistant. So what's happened is we've, we've. Gone. We've updated the human input. We've then gone to the tools node, tools node has returned the tools output to the assistant, and we've stopped again. So now to finish this off, we invoke a graph one more time. There it is. You get the final AI message.

Now let's briefly show how to do this in Studio. So you can say, I'll start off and I'll say multiply two and three. Now, recall we have an interrupt before the tools node here, but what's kind of nice is in Studio I can also manually a, a add interrupts at any point in my graph.

So let's say, I wanna just add one here before the assistant node. So now we'll interrupt before the assistant node and in code already specified a [00:04:00] breakpoint before the tools node. So let's see what happens here. I run and we've stopped. So let's just do what we did before. Here I'm just gonna go to edit and let's just say I'm gonna change this to multiply three and three.

Now all I need to do is fork my thread because I've changed the state here, so I fork. And there we go. I've gone ahead. We still have that prior interrupt before running the tool node, but that's fine. We actually like this input. It looks correct. Three and three. We go ahead and continue to tools. We let the assistant run because the tool node has returned nine, which we believe to be correct.

We'll keep that. Rerun the assistant node, and there we go. The result of multiplying three and three is nine. So it's very easy to manually add breakpoints in your graph in LangGraph Studio just by using this interrupt. So we've clearly seen how edit graph state in Studio. Now let's go ahead and edit graph state via the [00:05:00] LangGraph API.

So recall LangGraph API under Studio. We can interact the API directly via the LangGraph SDK, and all we need to do of course, is just pass in the URL for our local Studio. We will we'll supply our assistant id, we'll create a new thread and we'll do the same as we did before. Uh, let's add an interrupt_before assistant, just like we did before.

We'll pass in the same input, two and three, and let's see what happens. Cool. So we've stopped just like it's expected before we hit the assistant node just as expected here. And again, just like we did before, we can get the current state. So there we go. We can look at the state. Again, just contains this human message and we can modify the state.

So let's just grab that last message in our state. Here it is. This is that human message right here. And we can modify it. Say last message, content, No, multiply three and three. Cool. So [00:06:00] now we can look at that last message. There it is. So now it is, No, actually multiply three and three. Now again, we talked about, this will just be applied as a state update given the messages reducer.

But there's one interesting trick here that we've done. We've taken the existing message and only modified the content. We've kept The message ID consistent. So what's gonna happen is when we update the state here. We're actually gonna overwrite the value of the existing message because we've supplied a message id.

So let's run that. There it is. We updated state. Now let's try resuming. We'll go ahead and pass None just like we did before. We'll stream the values and we'll look at what goes on. So, we proceed with a tool call and we get the result of the, the tool invocation. In this case, the tool returns nine [00:07:00] as expected.

And to finish that off, we can just call it again, input None as mentioned before. And there it is. You get the final assistant invocation. The result of multiply nine three by three is nine. Remember that we stopped here because we add this interrupt_before assistant. So what happened is our assistant formulated the tool call that went to the tools node.

The tools node ran the tool call, returns nine to our assistant. Then it stops again, and we go ahead and pick up where we left off. Here we get the final answer. So we've shown you can create a breakpoint in your graph. You can edit the graph state at a given breakpoint. But I wanna show a nice trick.

What if we actually want to explicitly get user input to modify the state? So we can do that really easily by supplying what we're gonna call a dummy node, which will basically be a no-op [00:08:00] node, as we mentioned here. That will basically accept the user feedback and inject it into our graph at a particular point.

So let me just lay this out and show it in detail here. So we're gonna create that same agent we just talked about. But I'm gonna add a little human feedback node before the assistant. Okay. So before, remember, we interrupted on the assistant and then we just edited the graph state in the assistant node.

Let's just add a node here prior to the assistant that will actually allow us to inject human feedback explicitly as if this node was running. So let's go ahead and define an initial input. Multiply two and three, we'll create a new thread. We'll run the graph until the first interruption, which we're gonna expect to come at that human feedback node.

Okay, now here's where [00:09:00] I'm gonna add this input. Tell me how you wanna update the state. This is gonna be passed from the user. Now I'm gonna call that graph, that update_state, just like we showed before. But you see this as_node that will apply this update to state as if this human feedback node ran it.

That's really it. It's super simple. So let's run that right now. So, okay, here's the initial human message. Multiply two and three. That goes as human feedback node, let's say. I wanna modify that. So I say no. Multiply three and three. There it is. Cool. So the tool calls modified accordingly. There it is. The tool message is returned.

So we've gone back to the human feedback node. There's no more feedback I actually want to add, so I don't need to update the state anymore. So I can just continue basically. There [00:10:00] we go. Yeah, the AI responds and we're done.

So this is a very nice way to systematically inject human feedback at specific points in your graph to create what we call a dummy node that interrupt before. And you'll basically add feedback as if that node is running, and you do that really simply with this update_state. Pass in your thread Id, pass in the state update you want and just call as_node human_feedback and that state update will be performed as if that node was doing it.



=============================================
File: LCA-LangGraph-C1-M3-L4-V2-Dynamic-Breakpoints-Final-1.txt
=============================================

[00:00:00] Breakpoints allow a developer to set particular nodes that the graph will stop at. And this is really useful for human approval, for debugging and replaying, for editing graph state. But what happens if we actually want the graph to be able to interrupt itself? You can think of this as an internal breakpoint. And it could be based on some condition, like, you know, something about the state of the graph that flags or triggers. We might call this dynamic breakpoint.

This can be achieved using NodeInterrupt. We're gonna talk about this right now. So first I'm gonna define a really simple graph where in step two I'm gonna throw a NodeInterrupt based upon some condition. In this case, it's based upon the length of the input. So it's a really simple example, but let's have a look at this.

So I build the graph. Again, three steps. It's super simple. I'm just printing the state. Um, in each case. I'm just returning the state in each case, and I'm printing the step. So it's [00:01:00] nice and easy and we'll go ahead and run that. So there we go. We can see the graph stops at the second step. It's never printed because of this NodeInterrupt. The input exceeds five characters. We passed in Hello world.

Now let's actually convince ourself to see that we're actually stopped here. So we can get the state, pass in the thread_config to get it. Run this. So we can see step two is next. Okay. And we also can look at the tasks. So the state.task tells us kind of the task schedule for the graph, and we can see that the node interrupt is logged here. We can see here's the message. So let's try to actually resume. We can run and we see nothing happens. This is because we're actually just rerunning the same node repeatedly. Unless the state has actually changed, we're basically stuck here.

So we talked a bit about how to update state. We [00:02:00] can do that pretty easily. So we can just call graph.update_state, pass in the thread_config, and let's just pass a new value to the input. We'll call it hi now instead of hello world. Now the state is hi, and we can try to continue, and it works. So we proceed to step two and three now with hi. So basically we've been able to bypass that NodeInterrupt in step two because we've updated our state and we've continued the graph.

So let's show dynamic breakpoints with the LangGraph API. So I have this dynamic breakpoints simple graph here that we defined above running in Studio. I can connect to it via the SDK. Here's the URL of course, and I can go ahead and look so we can see. Here's all my graphs. And dynamic breakpoints is here.

So that's great. I can connect to it and all I'm gonna do, create a new thread. I'll specify an input just like we did above. Hello world. I'll specify dynamic_breakpoints and I'll just stream the outputs. So there we go. So if we look at the current state, we just run [00:03:00] threads.get_state. And let's look at it here. So we can see that we're stuck at step two, just like we were before.

And we can see the NodeInterrupt message is logged here. So again, we can look at what is the next node. And it's again, step two as noted. So let's try actually just updating the state. We can do that just as we talked about previously. We'll go ahead and pass a thread_id. We'll pass an updated state of hi.

We'll run that, and now we'll continue. We'll pass None again. So we've updated the state and we're gonna pass it in via the thread_id and let's see what happens. So there we go. We proceed and let's go ahead and get the current state just to see where we are. And we are. We're at step four, so we finish.



=============================================
File: LCA-LangGraph-C1-M3-L5-V2-Time-Travel-Final.txt
=============================================

[00:00:00] So we've been talking about a few motivations for human in the loop. One is approval where you can interrupt the graph and approve certain nodes. The other is editing, so you can manually edit the graph state. Now let's talk about how LangGraph supports debugging by viewing, replaying even forking prior states.

And this is collectively called time travel. Let's build our agent again. We have three different tools. We've been working with this one previously and we'll go ahead and lay it out. So there it is. Classic, simple kinda react style agent and let's go ahead and run it. Pass this basic user input, multiply two and three, new thread_id.

And now we have the final state of our agent, contains the human message, the AI response, tool message, AI final response. Okay, now if you call graph.get_state, pass the thread_id. Let's have a look at what happens here. Okay. Cool. So you see the state, StateSnapshot object. This is just the checkpoint at the current state of the graph, and in our case, the [00:01:00] graph is finished running.

You can see the full state's printed out here, and this is just reproduced. So you can see, if you look at this messages key in our StateSnapshot. It has this whole set of messages, which we can look at here. But this snapshot has a few other nice things. It has some metadata. It has, for example, the next node to go to.

In this case, there would be no next node because we've terminated the graph. And you can see things like checkpoint_id. So that's the first thing to note is that this graph.get_state gets the state of the graph at a given point in time and you can look at it. But you can do more than that. You can actually get this, use this get_state_history to look at the history of states at every step that your agent took.

So let's do, go ahead and do that. We can look at the length of this. So there's five steps, and let's look at the ordering. So this first is actually. It looks pretty similar to this. So the first element in this list is just the current state, and you can kind of go back in time. So this was the start of our agent.

So this is just the [00:02:00] start node and then we can look further. Boom. There's our human input and so forth. So we have the whole state history to every step of our agent captured using this graph.get_state_history. It's pretty nice.

So let's visualize this to build a little bit more intuition about what's going on.

Here's a toy graph, two nodes. So here's the control flow. I start, go to node one, node two, and then end, and I modify the state as I go. So your graph is just the control flow you laid out nodes and edges. Now the steps or super steps that they're technically called are just basically every sequential nodes own step.

And that's what we've been working with for the most part. Now, parallelizations we'll introduce a bit later. And parallel nodes also runs the same step, but in this case, these nodes are sequential, so they're separate steps. Now, checkpoints just wrap the state and relevant metadata at every step. So these StateSnapshots, just [00:03:00] service checkpoints, that's it.

Pretty straightforward. And as noted StateSnapshot is just a type for checkpoints, so that's cool. Now we saw above, when we call graph.get_state, we get the current and most recent checkpoint. Graph.get_state_history, we get the list of StateSnapshots or checkpoints that the graph has executed.

Now let's talk about this notion of replaying. So we can actually replay our agent from any of its prior steps. And it's pretty straightforward. Let's think about it like this. When we run graph.stream, we pass in None, but we give it a thread_id. It just picks up from the current state in the thread_id, and we've actually seen that before. Pretty straightforward. Now, what if we just pass in a checkpoint_id, along with the thread_id?

In that case, the graph will kick off from a particular checkpoint. So it's a subtle difference, right? [00:04:00] If you pass in just a thread_id, it'll just pick the current state of the thread. If you pass a thread_id and a checkpoint_id, you can rewind to any checkpoint within the thread. So here's a toy example.

Let's say I pass in. If I just run, graph.stream with a thread_id, it picks this blue guy. The most recent StateSnapshot runs from there. In this toy example, it, nothing would happen 'cause it's finished. So it's a subtle but important point to make here. When I pass in a checkpoint_id that the graph has already executed in the past.

We actually replay, we don't re-execute. Which is to say we just play back through those checkpoints because the graph knows, Hey, I've already executed this in the past. So this is known as replay. Okay, so let's just say we passed this red checkpoint here. We would replay the graph from red to blue and then finished.

So let's go ahead and give that a shot. Now let's, in our agent example, let's just pick one checkpoint that we wanna look at here. So this is the [00:05:00] human input. We can see the state at this input. We can see next. So this indicates the next node to go to. We can also look at the config, which contains the thread_id and the checkpoint_id.

So to replay, all we need to do is just pass that in. This replay.config contains our checkpoint_id and the thread_id, and we just can run from there. There we go. So you can see it's very fast because we're not re executing anything. We're just replaying exactly what happened from, that human input. And we can see we went to the AI uh, model, which returns the tool call, tool node, back to AI and we're done. So that's just a simple example of replaying from a specified checkpoint in your state history.

Now let's talk about the idea of forking. So we've actually run this already, we call graph.update_state. You pass in the thread_id you pass in the state that you wanna update with. You actually create a new checkpoint, [00:06:00] but you're just creating a checkpoint on whatever the current state is. So you can think about this as you're just forking the current state and you're adding that as a new checkpoint to the history.

Easy enough. Now there's an interesting thing I can do though. What if I wanna rewind and fork a prior checkpoint, not just the current one? You can do that. All you need to do is update state pass the checkpoint_id in. And this is in our cartoon here. If you kind of go back again, we're looking at this kinda like little two step graph, right?

Let's say I want to go create a fork from that first step. Well, I can do that. Just pass in that checkpoint_id and now I have a forked checkpoint that I've created. Now, let's talk about something interesting here. If I call graph.stream with this forked checkpoint_id, what happens? This has not been executed before. LangGraph knows that. And then you actually re-execute your graph with this forked checkpoint, which makes a lot of sense, [00:07:00] right?

So it's a very subtle thing, but important to understand, replaying versus re-executing. If I pass in this red StateSnapshot, okay, my graph knows, I've already run this. I've already executed it before and it replays. If I pass in this purple StateSnapshot, the graph knows this has never been executed before, and it actually re-executes, it re invokes the chat model and let's see that right now.

So let's fork that, same, you know, in, in this case it is the, uh, human input. Okay, um, the checkpoint that contains the human input. In our, in our agent, we have the configuration, no problem. We have our thread_id, we have our checkpoint_id. Now let's just call that update_state. Just like we saw up here in the cartoon. We'll call update_state and we'll pass that to_fork.config, which just contains a checkpoint_id. And, okay, here's something I want to explain a little bit carefully. [00:08:00] We're gonna pass in a, a new message.

So we're gonna say multiply five and three. Okay? Um, but what we're gonna do is. So again, we're, we're placing two and three with five and three, so that's cool. But what I'm gonna do is also supply the ID of my message from the initial checkpoint and what that's gonna do. It's just gonna overwrite it.

Okay. Now why is that? Let's think a little carefully here. Okay. This is subtle. The messages key uses that add_messages reducer, we've talked about this already. The reducer will append new messages to a list. So if I didn't supply this ID, it would append this message to this one, and I'd have an updated state with two messages, two human messages in a row.

Okay? Now I don't want that. I [00:09:00] actually just wanna overwrite this existing one. Okay. And to do that, all I need to do is pass the ID and then it'll be overwritten. Create a new checkpoint. So subtle thing, but something to remember, when you're doing an update_state, remember what state key you're updating and what's the reducer on that state key?

In this case, the reducer is add_messages, which will append unless you supply the ID in which which case it will overwrite. Nice. Okay, so we got that clear. Let's make that update. Now we have a new config. We have a new checkpoint_id. Okay. Remember this last one ends in 13. This one ends in 66. Okay, cool.

New checkpoint_id. Um, I wanna show you something else. This is interesting, right? Let's look at our states. Now, okay. We've made a few different updates. So now there's actually nine states here. Okay? We've updated this a few different times, um, and previously when we first ran, remember that above.[00:10:00] 

There's only five. So what's happening is as we're calling update_state, we're actually adding new forked checkpoints to this list in our thread. Let's look at our diagram. Our thread, just a collection of checkpoints. We're calling update_state, and we're creating new forked checkpoints that just, they're just getting added to the thread.

That's it.

Cool.

So we can look at our current state. There it is, multiply five and three. Exactly as expected from our state update. That's good.

Okay. You can confirm that by looking at the messages. Yep.

We can look at the current state. Yep. So again, the current [00:11:00] state will match the first value in our state history. So we're looking good here. Now let's rerun with our forked config. Okay. The forked config supplies, this checkpoint_id that we've modified. Let's confirm this works. There it is. Perfect. So we've modified that human input, five and three.

The tool call is five and three, fifteen. The output's fifteen. Very nice. Now let's look at the current state, and it's been updated to have this full list of messages resulting after the agent runs. So we've just been able to fork a specified checkpoint and rerun our agent with that fork.

Now let's look at time travel using the LangGraph API. Just like we talked about before, our agent's deployed locally to Studio. We have the URL [00:12:00] here and we can connect to it just like we show here. Now, let's talk about replaying. So first let's do an initial run of our agent from scratch. Again, I'm just gonna stream updates.

I'll pass in this human input and let's kick that off. So here's my agent running for the first time. Human input, tool call, tool node responds, back to the assistant. Same as we've seen a lot. So this is just my initial run. Now I wanna replay from one checkpoint in that run. So all I need to do is get my thread history.

This gets my list of states, and let's pick a state to replay. Cool. So this is just that human input. You can see that's the only thing here. The next node calls the assistant, just like we have seen before, this is great. Now let's just replay from here. So all I'm gonna do is, the only new thing is this checkpoint_id.

I just pick the [00:13:00] checkpoint_id from my replay config. So this config has a checkpoint_id. Boom, just pass it in. Supply None. And it'll just start replaying from the checkpoint. Remember it replays because it already knows this has been executed. I'll use stream_mode values. This will stream the full state at every step.

Let's give it a shot. Cool. So we'll reemit the current state. Good. Multiply two and three, and we will just replay what we did. Very good. Tool call, tool node, final response. Very nice. So it's like we showed before. It's pretty simple. The only new thing to track here is to replay from a particular checkpoint.

Just apply the checkpoint_id. That's really the only new thing here. Likewise, we can replay by streaming updates, just only a minor aesthetic [00:14:00] difference. In this case, we only look at the updates to the state. Assistant node, tools node, assistant node. So easy enough. We've seen this quite a bit before. Now let's show forking.

So let's do what we did before, rerun from scratch. New thread_id. Cool. Human input, assistant, tool call, tool node runs, back to the assistant. Now let's pick a particular checkpoint. We want a fork, so same as before. We'll look at the human input. Multiply two and three. And we can look at the, this is the message ID for this particular human input, and we'll see why we're gonna use this in a little bit.

Let's just confirm to call next is assistant and the checkpoint_id, we also have. These are all the raw materials we need. Now all we need to do is just edit the state. So remember, we're [00:15:00] modifying the latest state in our messages key. And message key uses the add_message reducer and this will override a message if you supply the ID.

So all we're gonna do is call messages, update state on messages, we'll supply a new message in, so in this case, multiply three by three. We'll supply the ID of our existing message. And it'll overwrite this. So we'll overwrite two and three with three and three because we supply the id. And then we just call update_state.

Again, supply this forked_input right here. Supply the checkpoint_id and the thread_id. There we go. Now we have a new forked_config. So this is basically a new checkpoint. Okay, and we can look and see. If we look at the state history now this new checkpoint has been added. Perfect. [00:16:00] Look at that. Multiply three and three.

So it's a new checkpoint that's been added. Now this is our current state. Now we just rerun from here. This new checkpoint_id. Now because the graph knows that this has not yet been executed, we're actually gonna re-execute or rerun every, uh, node in our graph following this one. So you can see the next node to call is gonna be the assistant. And you can see right here, and let's go ahead and run that.

Very nice. Tool call runs with these forked arguments. Tool node executes it. Final response, three and three is nine. Perfect. So this shows forking with the API. Very similar in concept to what we did previously. So now let's have a look at forking in Studio. [00:17:00] I'll go over to Studio. So here we are. Here's our agent.

We have a fresh thread. Let's just kick this off.

Multiply two and three, just like we've seen quite a bit. Let's run it. Nice. There we go. So this is our first run, nice and easy. Now let's say I want to explore the result of different tool calls. Let's add an interrupt, at my assistant. Now I can go over here and see this edit. I hit edit and I can just modify the payload here if I want to.

Okay, cool. Four and four. Now all I have to do is just run this fork and it's gonna run from this particular state forward in my graph with this updated output. Let's give this a shot. Cool. So what happens is, now I've gone to my tools node and I see, okay, here's the updated tool result of sixteen.

It's gone back to my assistant, which I've paused. That's why we're paused right now. [00:18:00] Nice. I can do that again. Let's just say I want to examine different tool calls here. Eight, fork again, I can look at the tool result. Thirty two. Cool. Now, with a simple graph like this, there aren't too many places where you actually want to fork and explore alternative trajectories.

It's very basic. It's only effectively, you know, um, you know, just iterating between the assistant and tools nodes. But if you have a more complex graph with many steps, there may be many places you want to interject and fork the state to explore alternative trajectories, and it's very easy to do in Studio.

In fact, in some ways using the UI, it's almost easier than using, for example, the SDK. So I often like to do kind of exploration of alternative trajectories here in Studio rather than just with the API in this particular case. So this is a very good thing to be aware of if you want to play with and explore alternative trajectories.

Forking in Studio is a really nice way to do that.



=============================================
File: LCA-LangGraph-C1-M4-L1-V2-Parallelization-Final.txt
=============================================

[00:00:00] In module three, we went in depth on human in the loop workflows. We talked about three popular use cases including human approval of certain actions for an agent like tool use, debugging through things like time travel where you can go back and replay or fork prior states, and also editing state. So we're actually gonna move now into a new module that ties together these themes with a few other ones related to controllability and multi-agent workflows.

And we're gonna kind of build all this into a pretty interesting multi-agent research assistant that will kind of tie together all the aspects of the course. Now to start, we're gonna talk about a few controllability topics, and let's start with parallelization. So the main idea here is, fanning in and fanning out is something that we often want to do in graphs.

I'm just gonna define a really simple graph here. Okay. This is something we've kind of [00:01:00] looked at before. Here's just a linear flow, A, B, C, D, right? I have a single state key, and if I run this, what happens? Okay, so as we go from A to B to C to D, we just overwrite the state at each node, so that's fine. That's simple, and we've talked about that a little bit before.

Now, let's say we actually want to run, for example, B and C in the same step. Let's just go ahead and build a graph that does that. And all you have to do is just create an edge from A to B and A to C, and that's it. And then B to D, C to D, D to end. So this is actually doing fan in and fan out in the sense that we are fanning out.

Let's just draw it out to show you. We're fanning out from a. So we're running A as a step, we're running B and c together in the same step of the graph, and we're running D as a [00:02:00] third step. So let's go ahead and try to run this and I'm actually gonna catch any potential errors to highlight something interesting to you.

So you're gonna see an error occurred. At key state can't receive, um, can only receive one value per step. So what's happening is B and C are both trying to update state in the same step, which is ambiguous. If you think about it, both B and C, think about it simultaneously are writing to the same channel or same key in the state.

So the graph doesn't know which one to actually keep or use. So this brings up an important point about parallelization. Which is that if you're running certain nodes in the same step, IE in parallel, and they're writing to the same channel or key, you need to use a reducer that [00:03:00] can aggregate those updates.

We talked a lot about custom reducers, we talked a lot about reducers in general. So let's just do something simple. Let's just define state with a reducer that will just add, uh, updates to a list. And now we're gonna draw that same graph. Here we go. So we're gonna fan out. We'll fan back in. So now nodes B and C, we'll be updating this state key, but because we have a reducer, all these state updates will just be appended to the list.

So let's give this a shot and make sure it works. Very nice. So we can see, we just get this list of elements and we're able to run steps B and C in parallel because even though they're writing to the same channel or key, we have a reducer that handles those updates and just appends them to the list. So the key point to remember is [00:04:00] when you're doing parallel, uh, node operations and you're writing to the same key or channel in your state.

Just be very careful and make sure your using a reducer that can actually handle those simultaneous updates. Now let's walk through another common architecture in parallelization. We'll go ahead and draw a graph out here. We'll modify it slightly. We're gonna show two branches here. Now one branch has two steps and the other only has one.

So what's gonna happen? What you're gonna see is that even though steps B and C will be run roughly at the same time, that is, they'll be the first step in each branch. We will wait to run D until everything in this other branch is completed. Now let's go ahead and see that right now. Nice.

So we can look at the final state list and what we can see is first A writes to state, then B writes to state, then C writes to state. [00:05:00] And then B two writes to state. So even though we ran C, we don't go ahead and run D. We wait for everything in this other branch to finish because these are all run, that is B, B two, and C are all run as part of the same overall step. Okay? So you get B, C, and B two all updated before D updates the state.

So it's just an important point to remember that when you have parallelization. These parallel branches of your graph are run as part of the same step, and we will wait until they all finish before running D, when they fan back into. So let's look at the state after our graph ran. We have A, B, C, B two, and then D.

So you might ask, well, why is. C coming before B two. Okay? They run as part of the same step. [00:06:00] So you can see B here, C here, B two here. And the answer is this order of updates is actually set by the graph. So LangGraph decides how to order these updates when they're within the same step. You don't have control over that.

But what if you wanna customize that? So we can do that pretty easily using a custom reducer. Let's define a reducer that's actually just going to sort the values in our state after each update. So it's pretty simple here. You can see we basically perform the concatenation and then we just sort the values in the list.

So it's pretty simple. So recall, the ordering above was B, C then B two, and now we're gonna find that same graph. Cool. Let's run it and now you can see A, B, B2, C, D because our reducer actually sorts the values as they're added. So even though by default we don't have the ability to to control the ordering of these updates within the [00:07:00] same step here we do if we supply a reducer that will control or manage the order of those updates as we make them in the state value.

So let's pull all these threads together into a realistic example, which we're actually gonna end up using a bit later as well. So it's kind of a nice little precursor. So I'm gonna define an LLM. I'll use OpenAI just like we've been before, but of course you can use any model you care about. I'm gonna define a really simple graph here.

So here's my state. It's gonna have question, answer, and then context. So you're gonna see context is a list, and I have this add operator, which means we're gonna append to it. Okay? So this gives you a clue that this context thing is probably what I'm gonna be writing to in my parallel nodes. So that's just a nice little hint you can see there.

So I'm gonna define two different search tools. I'm gonna define a search web tool. I'll use tavily_search for that. So this is a very useful search [00:08:00] service. You set your TAVILY_API_KEY, and then you can use it. And what we're gonna do is hit Tavily with a question that's in our graph state, get docs back, and we're just gonna format them in kind of a nice way and write them back to state context.

We're writing it back as a list because again, we look at our key and it's a list. So there we go. We'll do the same thing with Wikipedia. Again, we'll just define this WikipediaLoader, we'll call it with question, and then we'll format the output right into context. So you can see these two nodes are gonna write to the same key in our state.

And that's fine because our key has a reducer, and we're gonna run these in parallel. Then we're gonna pass it to answer, which will take the context and answer a question based upon the context. Let's draw a graph out here. So there we go. We start, we search the web, we search [00:09:00] Wikipedia. We do those in parallel.

We write both of those results to the same context key like we talked about. And then our answer generation grabs from context, grabs the an that grabs the question, formats a prompt with those, passes it to an LLM, right here, gets an answer, writes that answer back out to state. So I can run that pretty easily.

Let's ask a question about Nvidia, the GPU company, and we can see the result. So this is a good practical use case for parallelization. For example, you want to retrieve information from multiple sources and aggregate it in a particular key, in this case context, and use that later for answer generation using an LLM.

So let's interact with this graph using the LangGraph API. We have our graph with parallelization deployed to Studio locally. We can interact with via the SDK, just apply the URL and the assistant_id. [00:10:00] So we pass into questions like before we create a new thread. And if you recall, each event we run stream has this data object, which is just the state.

So all we're gonna do is as we're iterating through our graph and it's producing state updates, let's just go ahead and get answer, and we will only return answer. That's really all we care about, right? We're not gonna return intermediate updates. So we could, of course, you know, we add to context here, for example, we could print those, but let's just print the final answer in this particular case. We run it and there's our answer.

So we can also look at this in Studio to see some of those intermediate outputs. So now we're in Studio. We can see our graph here. Let's just open up our inputs. Let's just go ahead and ask a question how were Nvidia 2024 earnings? Let's have a look.

So now we can see we're going to each of our so, uh, sources, we're doing web search and we're searching Wiki, [00:11:00] Wikipedia, and this is pretty cool. So we get a whole bunch of information in our context and we get our final answer.



=============================================
File: LCA-LangGraph-C1-M4-L2-V2-Sub-Graphs-Final.txt
=============================================

[00:00:00] We're building up to a multi-agent research assistant that ties together all of the modules in this course. And to build that, we just covered parallelization, which is an important controllability topic. And now we're gonna cover sub-graphs, which is another important controllability topic. So sub-graphs allow you to create and manage different states within different parts of your graph.

And this is really useful for multi-agent systems, like teams of agents that have their own state. So let's just consider a toy example. I have a system that accepts logs. It does two separate things. It summarizes them. It finds failure modes in them. And I want to perform these two operations in two separate sub-graphs.

So here's really the key thing to understand and think about. How does the parent graph, or in my case, the entry graph, connect with or communicate with the two sub-graphs in terms of state, right? And likewise, how do the sub-graphs communicate back to the parent graph? [00:01:00] In short, this is done with overlapping keys.

That's the key concept to understand. So here's a toy example from below. My entry graph, or my parent has a key docs. I want both sub-graphs to have access to docs. All I have to do is just add that key to the state of each sub-graph. Easy enough. I want my parent, to also access like the reports from each of the sub-graphs so it can return them in the end.

All I need to do is just have the report keys present in my parents' state. So you can see the sub-graphs can have their own state keys, like summary or failures that we don't care about in the overall parent state. But as long as the key things I want to communicate between the parent and the sub-graphs are present in both places, they can effectively talk.

So in this case, docs I want to communicate to my sub-graphs. Just make sure the key is overlapping. And I want the report, summary report, failure report [00:02:00] to be accessible to my parent. Just make sure those are overlapping. That's really the key idea in construction of sub-graphs, and we'll walk through that in detail right now.

So let's actually make this concrete. First, I'm gonna define what a log looks like. This is just a TypedDict, so nice and simple. Now let's define my sub-graphs. So we kind of talked about it previously. Here's my failure analysis sub-graph. It's gonna have some state here, and I'm gonna specify an output schema for the state.

We'll talk about why that's interesting a little bit later. But we talked about that in a prior section. And what this does, this simply sets the output schema of the sub-graph. It's only gonna obtain information that I really care about servicing back to the user at the end. Okay, now this contains all the internal information I want available in this sub-graph.

So let's just draw this out and see what it looks like. So there we go. It's very simple. [00:03:00] We get the failures, we generate a summary, we end, and these are all just toy functions. It doesn't matter, it's just to illustrate the concepts. Um, so there'd be some toy function that, you know, takes in the logs and isolates the failures, and then looks at the failures and generates a summary.

Okay, so that's all's going on here. So there we go. That is our failure analysis sub-graph. Now we also have this summarization sub-graph, and again, we're gonna define our summarization state and our summarization output state. We're gonna have this generate_summary, which again is just a toy, uh, node here.

Here's a summary generated, writes back out to summary, and we have the send_to_slack, which will, you know, it's just again, a toy function sends to slack. Let's draw that out. Nice. Okay. So to add these sub-graphs to our parent graph, all we need to do is this. First, let's [00:04:00] define our parent graph state. Now you're gonna see something interesting.

Here's our input. Here are the cleaned_logs, which are gonna be passed to our sub-graphs. These are what we want to service to the user. So this failure analysis summary is produced in the failure analysis sub-graph. This report is produced in the question summarization sub-graph. processed_logs are produced by both sub-graphs. So in this case I want a reducer because both sub-graphs will be writing into the same key processed_logs.

Cool. Now you're gonna see something kind of weird. Why does cleaned_logs have a reducer? This is just the input to each sub-graph. So let's go back and look at that quickly. If I go back to my sub-graphs, you can see they both use cleaned_logs. It's just a [00:05:00] list of logs, right? And that makes sense.

We're gonna get logs in our entry graph here, we're gonna pass them to both the sub-graphs, but why the reducer here? Why is that actually needed? So the interesting point is that both sub-graphs' output state will contain all their keys, even if they're unmodified. So we're not actually modifying or returning explicitly cleaned_logs.

We're not updating it explicitly, but it is present in the state of each sub-graph, and it will be returned by each sub-graph to our entry graph. And because of that, unless there is a reducer that can cause a collision in our state key or state channel. [00:06:00] So there's a really simple way we address that, which is that we used the output state schemas.

So if you go back to our sub-graphs, we defined this output state, which does not contain cleaned_logs. Same up here does not contain cleaned_logs. And because we're using this output schema that doesn't contain cleaned_logs, we won't expect any issue. Now, of course, they both do return processed_logs. Okay. We, they both will write to that particular key, but that's fine because processed_logs has a reducer.

So that's really the way to think about it. So actually, when we define our output state or our, we define our overall state for the entry graph. cleaned_logs does not need a reducer because that's not present in the output schema of each sub-graph. And we're just gonna add this one simple clean_logs node here and [00:07:00] let's put it all together.

Nice. So we're gonna start, we're going to go to clean_logs, and then we're gonna run both of these in parallel. And then we're gonna end. So let's go ahead and try that out. We'll take two dummy, uh, logs. We will pass them in and everything runs. Very nice. So one of the nice thing about sub-graphs is they make your traces much more readable, particularly if you've large graphs with many important sub components.

I'll show you that right now. So I'm in LangSmith and I'm looking at the trace of this sub-graph right now, and you'll see something immediately that's pretty nice. The two sub-graphs are encapsulated here and they're collapsible. So I can open this up and look at what's happening under the hood. I can look at the various steps of this particular sub-graph.

Likewise of the second sub-graph, but they are nicely contained here. And if you imagine I have a complex system with many [00:08:00] components, I can easily compact, collapse the common sub-graphs together and kind of open them up and drill into them, but then close them back down when I wanna look at other parts of the system.

So it makes your traces is a lot more readable when you're working with large and complex graphs.



=============================================
File: LCA-LangGraph-C1-M4-L3-V2-Map-Reduce.txt
=============================================

[00:00:00] We're building up to a multi-agent research assistant that ties together all the modules of this course. And to build this, we've been discussing a few LangGraph controllability topics. We coverd parallelization, subgraphs. Now we're going to talk a little bit about MapReduce. So as before, we're going to use LangSmith for tracing.

And let's kinda lay out what MapReduce is. So MapReduce is basically an efficient task, decomposition and parallel processing method. And it has two phases. One is what we call map phase. So that is take some task, break into a bunch of subtasks and do them all in parallel. Then the reduce is basically aggregate the results from all those parallelized subtasks and bring them back together.

That's the main intuition. So here's a toy example that we're going to kick off. In the map phase, we're going to create a set of jokes about some topic. Reduce phase, we'll pick the best one. It's a really simple toy example. So here we'll just set up a few prompts. We'll set up a prompt for generate a list of [00:01:00] subjects related to some topic that user provides.

Then we're going to have a prompt, basically, hey, create a joke for each subject, a third prompt, select the best one outta the list. Okay, so that's kind of the setup of the problem. Let's talk a little bit about state and kind of how we're going to lay this out. Subjects and BestJoke are just two schemas that we're going to use and you'll see a little bit later why this is very useful.

Base is going to be used with our LLM to produce structured outputs that adhere to these schemas. One of them is going to be a list of subjects, one of them is going to be, and the id or index of the best joke from the resulting list of overall jokes we've made. Now, the most important one that's interesting is this overall state.

This will contain the topic provided by the user. It'll contain subjects, which is list of subjects we'll produce using this schema. Now, this is really the one to watch. This jokes, you can see it has a reducer on it that will append [00:02:00] to the list. And what's happening here is we're going to fan out or map to a bunch of nodes that will produce jokes and they're all going to write to this same list.

So this list is kind of going to be the aggregation for all the individual map nodes, jokes. And then we can use that list later to select the best one. So let's talk through the individual pieces now. This is just generate_topics. So we take in state, we will, uh, take, grab the topic from state, form a prompt from that, and run it with our model.

And again, we're actually using this with_structured_output method, which is very convenient. It allows you to produce a structured output from an LLM. And we talked about a bit about this previously. Um. Adhering to a specified schema. So this subject schema we define up here, and it's just going to be a list of strings.

So it basically [00:03:00] constrains the output to be of a particular type, which is very important here because we want this response to be a list. So we can go ahead and run that. Cool. Now here is where we're going to do really all the work. We now have this subjects list that lives in state. Now this Send API and LangGraph is really the magical piece here.

So what's going to happen is we have this list in state. You can see subjects. Now all we need to do is iterate through that list and for every element in that list, for every subject in that list, we can use Send to send it to a particular node. And you can see we have this generate_joke node. Which we'll define in a little bit.

It can take in that subject and it can do some work on that subject. In particular here, it'll generate a joke and you're going to see, very briefly, we can write that back [00:04:00] out to this jokes key, which can append to our list using this add reducer. So this Send API is really convenient for at least two reasons.

One is that this list can be arbitrarily long. So this could be any number of subjects and it'll automatically spawn a generate joke node effectively for every element that list and run generate joke. So that's one. It's very flexible in how many of these you can kind of spool up in your graph, which is really nice.

We don't have to manually create all those edges. It'll automatically generate them all for us based upon the size of this list here of subjects. The other cool thing is we can pass arbitrary things in. So this subjects key is in the generate_joke state, which we'll define very briefly. But that state can contain anything [00:05:00] we want and we can just simply plumb in here.

It's very flexible. It doesn't need to adhere to what we see here as the overall state. So this generate_joke node that we're mapping to could have any state we want and we can arbitrarily write to it in this via the Send API. So let me show you that in particular. We're going to define that generate_joke node right here, and it has its own JokeState, right?

In this case, it's only the subject which we write to right here, but it could be anything which is actually quite nice. So you can actually decouple the state of your mapped node from your overall graph state. And actually we're going to be doing that in when we kind of go and build our final assistant.

Um, again, we're going to use structured outputs here. So basically we're just going to have the LLM, uh, produce a joke. And it's just going to be a string based upon the prompt. The prompt is here again, we have that joke prompt, which you format with the, with the, uh, subject that we pass. We get the joke and we [00:06:00] return it.

Now see something a little bit subtle here. We return jokes. We're writing this back out to our key here in the overall state. This jokes key, and again, we're passing it as a list. So that it is indeed appended to our overall jokes list. That's it. So those are the key things to understand about the Map step, which uses the Send API here.

And this is just the the generate_joke node that has its own state and that will be duplicated arbitrarily via the Send API, which is quite nice based upon the length of the list of subjects. Now the Reduce phase, best joke selection, is really simple. All we need to do now is we have jokes written to state now as a list from here, and we just join them all into [00:07:00] a single list, single string, and we pass them to our best_joke_prompt.

And again, we just asked an LM to pick the best one. This will respond with the index. If you recall the. Um, best joke schema here is just an int indicating an index value. So this returns an index and that's really it. We just index that jokes list with the ID of the best one, very simple and return it.

So it's pretty nice. Now let's try to compile that and see what happens. There we go. So we generate our topics. This is a list. Now here's the send eight. No, this is actually very interesting. This dotted line indicates a conditional ledge. So you can see we apply, add_conditional_ledge, generate_topics, continue_to_jokes, [00:08:00] goes to generate_joke.

Now this continue_to_jokes is, defined right here. That's this edge that uses a Send API. So this again, sends from, as we defined it down here, generate_topics to generate_joke, and it'll spawn up some number of generate_joke nodes based upon the length of the topics that we want to, that we want to generate a joke for.

So that's really the new piece to recognize here. This is effectively the Map step, and then, those all go to best_joke. So best_joke, like we just saw, receives the list of jokes that the Map step produces and it picks the best one, updates the state with the best one. And this we get out so we can see this all running.[00:09:00] 

There it is. So we can see generate topics. There's a list of topics. Here is the jokes. And here's the best joke. So this is a great example to also look at in LangSmith. Now I have my LangSmith open. Here is this trace, and let's walk through it in step by step. First we going to generate topics. Let's look at that.

So here's the generate_topics. Lion, elephant, dolphin, penguin. Uh, given the entry topic of animals. Now I can expand this and I can see this continue_to_jokes. Remember, this is that conditional edge that contained the Send API. Let's look at what happened here. So this is the key point. You pass this list of subjects and it creates basically a bunch of calls to the generate_jokes node that passed each of the subjects from that list in.

[00:10:00] That is quite nice. So it automates that parallelization for us. Now we can see those four generate_joke nodes run. That's great. And this is going to be kind of as you expect, it's basically just going to generate a joke about the topic passed in. Lion it again. It performs a tool call to structure the output according to what we specify as a joke and it's just like a string.

Um, and we can see, you know, each of these is kind of identically laid out with different topics. And then we have this best joke, which very simply we can look at the call here. Here is our prompt. Here's a a bunch of jokes about animals. Here's the idea of the one it likes the best, and then the graph goes ahead and returns the joke id two from this list as the best joke, and then we finish. So let's show this in studio for one further illustration to really [00:11:00] bring this home. So this is the same graph we've been working with, pass in the topic, animals. Now we generate a list of subjects. That's cool. This is, yeah. So let, let's just walk through it.

Generalist of subjects. Each subject. We run this Map step, generate a joke for each one. Boom. So we have four jokes generated, one for each subject. That's great. And then this Reduce phase goes and selects the best one. In this case, it predicts it, it selects one particular joke. So I can even rerun this and let's pause it here.

Okay, we'll create an interrupt. I'll create a new thread. Let's go back to my MapReduce graph. Um, I'm adding an interrupt here. There it is.

Animals, generate topics. Now we're [00:12:00] paused. Okay, so let's just look at this carefully. What's going on? We generate our subjects. Here's our four subjects. That Send API automatically, then spools up four different joke nodes, right? One for each of our subjects. And this is really nice. It does this all for us, right?

We didn't actually draw this out in the graph, that Send API takes care of it for us. Now let's just go continue. There we go. Each, they all generate jokes in parallel and that's very fast, obviously. And there we go. We go ahead and generate the best joke, picks the best one, and we end. So this is a nice way to kind of see MapReduce working visually and I encourage you to play with it using map_reduce.py in the repository.



=============================================
File: LCA-LangGraph-C1-M4-L4-V2-Research-Assistant-(1).txt
=============================================

[00:00:00] Let's pull together the threads of memory, human loop and controllability in a realistic, interesting multi-agent system that's capable of open-ended research. So research is often laborious. It's often offloaded to analysts, and AI can really help with this. But just raw LLMs are often poorly suited to real world workflows or decision making because A, you may want to provide specific sources for research.

And B, you may want a specific output format. For example, reports that actually aid in high quality decision making. So we'll build a system that can do all that and can automate it for us, and it can do it on any task we give it. First, source selection. We're going to give the system access to arbitrary sources.

For this. We'll use a few different web search tools, but you can actually customize this. We're going to use a planning process where we'll take a topic given by the user. We will break that up into some set of subtopics. And assign an [00:01:00] AI analyst to each subtopic. We'll use human-in-the-loop to refine those subtopics.

Then we're going to orchestrate a dialogue between each of our analysts and an expert that has access to the sources. So this is going to be like an AI, AI role play where the analyst will ask questions, the expert will answer questions. That'll be a multi-turn conversation that we'll capture.

We'll pull all those conversations together in one, a parallelized process, so we'll run them all in parallel. We'll use MapReduce to basically bring the results together at the end. And we'll finally synthesize it into a final report. So flow will look like this. We create some analysts based upon user provided topic, human feedback refines them. We then kick off research automation.

Each analyst conducts research in parallel with an expert that has access to external sources. And then the reduce phase brings all those independent research results together into a final report. So that's our overall flow. [00:02:00] Let's just set things up. Define an LLM. We'll use LangSmith for tracing like before, we'll log to this project.

Now let's just build the analyst generation graph first, and we'll show how human-in-the-loop is really helpful here. So here's our analyst schema. You can see each analyst will just have an affiliation, a role, a description of what they want to focus on. And this is just the list of analysts and this is the state we'll use just for this sub graph to kind of experiment and play with it in isolation.

So here's some instructions to create our analysts, and here's the overall topic. Here is optional human feedback, which will come in through human in the loop, define the most interesting themes based on the topic, and create this number of analysts. That's really it. So this is where structured output comes in.

We'll call LLM with_structured_output and pass perspectives. So remember that perspective [00:03:00] schema is just a list of analysts. So what happens is the LLM will produce a list of analysts that will focus on subtopics of our theme, and each analyst adheres to this schema, which is very useful as we'll see later.

So, this human_feedback is a no op node where we're going to interject human feedback. And this should_continue is where we're going to basically look, Hey, is there human feedback in the state? If there is, go back to create_analysts, and if there's not, we end. That's really it. So let's make sure we've defined everything.

Run that, run this. Cool. We can see our graph right here. We start, create our analysts. Go to human feedback and we're going to pause here. See this interrupt before we're going to pause at the human feedback node, await feedback. If we pass in any feedback, we'll go back and recreate analyst and start again. So this is actually important because we [00:04:00] want to make sure that the analysts we create have the right focus areas before the whole heavyweight research phase starts.

This is really intuitive. It's kind of how you would think about any kind of research if you have an analyst or a team of analysts. You make sure their scope is set correctly before they go do all the work. That's the intuition here. Let's test this out. Create three analysts. Here's our topic related to LangGraph.

We'll create a new thread and let's kick this off. 

Cool. This creates three initial analyst analysts for us. You can see they adhere to our schema, which is pretty cool. Now we can get our state, we can see that the next state is human feedback. So now we're the human feedback node. We're awaiting human feedback. So here's where I can do this state update.

Pass it as human feedback to basically run it as that node. Now let's say, add in someone from a startup entrepreneurial perspective. Okay, so make sure I have a startup entrepreneur in there. I update the state. [00:05:00] Now I just continue, pass in None with a thread id. The state's been updated as a human feedback node.

Let's run it. Cool. So now I can see I have three new analysts. There it is. Alex Johnson is the founder of a tech startup. Good. No further feedback. Finish, get our final state, confirm we're done. Print out our final analysts. There we go. Alex Johnson's the entrepreneur, and we have an academic, we have a tech corporation, like a kind of, a big company, um, perspective.

So, nice. This is the first step of our graph, the analyst generation phase with human feedback. Now let's talk about the interview. So the interview is going to between, going to be between each analyst we just created, and an expert. And we'll see, this expert is effectively a system that has access to some sources that we give it to answer questions.

So here's the state we're going to [00:06:00] use for interviews, and this is a schema we're going to use to perform search queries with our expert. We'll come back to these in a little bit. First, let's just walk through the process. So each analyst will need the ability to ask questions. Here's a prompt we're going to use to produce high quality questions, and you can tune this as you see fit.

But this is just a starting point. We give them, um, some instructions about interesting and specific questions. We give them their goals and this is actually one interesting note. We instruct them to respond with, thank you so much for your help. When they're done, when they're complete, when their understanding is, is kind of finalized.

Now we're going to go ahead and format that with, we grab in our analyst from state. The analyst persona is concatenation of the analysts, uh, identity and goals. We pass that in so that when we're creating questions, the analyst knows who they are and they [00:07:00] know what their perspective is. So that's an important aspect of this process.

And this is all we do. We go ahead and grab all the messages from state, and you'll see why that's interesting shortly. And we append the system message just like here. Boom. And we go ahead and ask a question. That's it. Now let's talk about the answer generation part. So that's just the question generation.

Now the answer generation. So for this you can select different sources that you'd like. You can use websites, you can use index documents, web search, Wikipedia search. For now, we'll just use Tavily for web search, and I'll use Wikipedia as well. So let's just set up the web search tool Tavily. I'll set up a Wikipedia tool.

Cool. Now this is just a simple little query that I'll use to produce high quality search queries. And these are just the [00:08:00] instructions. Look at the conversation, distill it into a single query. So this is search_web, search_wikipedia. Two different nodes that we're going to use to perform search on the different sources.

They're set up very similarly. I pass in that search query schema. We define up here. Right here. That'll ensure that we only return just a search query string and no preamble or anything else. So what's going to happen is, we're going to use that schema. We'll pass in these search instructions right here as well as the conversation, and we'll distill that into a query.

Then we'll hit our source, our sources Tavily search, or Wikipedia search with this query and produce some documents. And we're going to write them to this context key right here. So this context key [00:09:00] is right here. Cool. And you're going to see something kind of interesting. This context key has a reducer, operator.add, and it's a list.

So what you're going to see is we're going to aggregate sources from both of these nodes, search_web, search_wikipedia to the same key. We pass them back as lists, so they just get appended. So this is actually kind of nice. You'll see in a minute that we can actually run this in parallel because we're using a context key with a reducer that will allow us to append all our sources to a common list.

Then comes the answer. So you're going to see we just pass in those contexts that we've logged here to state. And this is, kind of like a, you could think of it as a RAG or retrieval, augmented generation prompt. We're basically passing in some context to the LLM and we're going to say, answer the question. We give a bunch of instruction here.

You [00:10:00] can read through this, you know, preserve sources where you can and so forth. Um, in any case, this is really just generate_answer, so we're going to grab context. We're going to format our instructions with contexts here. Very good. And the analyst persona. So then we're going to produce our answer. That's really it.

That's going to be an AI message. We'll append it with the name expert and we'll return that back to our overall messages. When the interview's done, we'll save it. Now, here's where the logic comes into place. So we're going to count the number of times our expert responds. You can see every expert response is going to have this answer.name expert, and this max number of turns is set in the [00:11:00] state.

So if it's not set, we default to two, but you can tune this as you see fit, so you can allow these conversations to go quite long. It'll go as long as this max number of turns, or if this, Thank you very much for your help, is in the last, what we might call question from the analyst, then we also end. And remember, we instruct our analysts to respond with this when they're satisfied.

So these are the two conditions at which we end our interview. Now, this is the final part. Once the interview is done, we're going to write this all into a section of our final report. You can see here's a bunch of instructions to do that, and we have this write_section prompt. So why don't I go ahead and let's just make sure that our state's defined.

Defined all these, run this, run this. I have my search tools. Yep, there it is. There's our full graph. Now let's [00:12:00] kind of rehash what we just talked through. So we're going to start. We've created our analysts already. Our analyst will ask a question. That question will go to search Wikipedia and search the web.

Remember in both of those, we'll use this nice little trick where we'll take the, right here, we'll take the message history, and we'll distill that into a high quality search query. Then we'll hit our sources with that search query and return the results to context. So that's just a little nice thing to remember.

Ask a question. Search. Results are written to context. We go to the answer question, they'll use context, and then we hit that conditional edge to say should continue or not. If [00:13:00] one of two things is true, the experts already answered. Remember that flag we set num turns? There it is, max_num_turns. If this has been exceeded, we finish or, if our analyst said, thank you very much, we finish. That's it.

So this'll continue until one of those two is met. Then all we do is we have this interview. It's just a set of messages. We save it to state, and then we write a section based upon the interview. That's really it. That's all that's happening in this interview process.

So let's give this a shot. Let's pick one analyst.

Cool. Create a new thread and let's kick off this process. I'll set max number of turns to two. Bring in my thread id, pass in my initial message so you're writing a, an article on topic, [00:14:00] and I'll kick it off with this one particular analyst. Cool, and that finished. Now here is that section that we wrote that summarizes the conversation.

It's pretty nice. Let's dig in a little deeper and look in LangSmith to see what happened under the hood. I'll go over to LangSmith here. Here we are. This is conduct interview. We just ran this. Nice. Now let's just collapse everything. What happened here? Right. First ask a question. Let's open this up.

We can look at the prompt. Boom. This is our prompt for the analysts to ask questions. Nice, nice. This is like, this is the input that we added as a user to start the, to kick off the interview. [00:15:00] And this is, this is it. This is our analyst's first question. I'm an analyst researching LangGraph. Provide a brief overview of what LangGraph is and how do you choose to use it. Really cool, right? Pretty nice.

Then we use the, we basically use our tools to search the web. Now we also, this is actually a little nice trick here. Remember we create those search queries based upon the question. Let's go back and look really quickly at the question here. So this is kind of verbose, right? We don't want to pass this whole thing into a search tool, so that's why we use that little trick where we created a nice search query first. Look at this, overview of LangGraph, agent benefits, uh, frameworks or startups.

Nice tight query. That's why we do that query writing step before we actually use a search tools. So it's a nice little trick here, and this is used as Tavily to get some results from the web [00:16:00] and we can look through those results later. Um, search Wikipedia. Same deal here. Same thing. We use a search query.

Nice. Now we go to this answer question. Let's open this up. So this is where. Let's kind of look at the prompt here. This is where we pass some instructions. You're an expert being interviewed by an analyst, and here is the context that we got from search that we're going to have the expert use to answer the question.

So look, here's the raw documents we got from using the search tools. Really nice, right? This is all of external context from search that we pass into the expert and there we go. The expert gives us a nice, rich response.

Cool. There we go. So there's our final expert response rendered output. Very nice. [00:17:00] Now you can see, this goes back to ask_question, search again, answer. And we, we only allowed for two answers in this case. So we let the expert answer twice and then we finish. That's all that's going on here. We save the raw interview, which is basically saving this raw message, this whole message stack, and we write our final section.

You can see we had an LLM do that. That just distills this entire conversation into a final nice section. That's exactly what we see here. Very nice. So that is the interview flow. Once you understand this, everything else is pretty easy because this is the core kernel of work. But the big idea here is. We create an AI analyst, we create an AI expert.

The expert has access to sources, and we let them converse. The only thing we need to do now is add control to parallelize this for all of our analysts, and that's really it. Now, let's go [00:18:00] back. So now we're back in the notebook. You can see where LangSmith is really nice to dig in under the hood to understand what's actually going on in detail, but that's the big idea.

We get this nice summary out. Cool. Now really this is the final bit. We're just going to parallelize all those interviews. We'll use MapReduce to do it, and now we're going to define kind of an overall graph state. Okay. So we'll do that. Now, here's where this initiate_all_interviews is a conditional edge we'll use.

So this will follow what we looked, we looked at previously. If there's human feedback, we recreate our analysts. Okay? And otherwise we can use MapReduce with a Send API just kick off all those interviews in parallel. So look at this. This is pretty nice. We've already talked about this Send API. [00:19:00] We're going to say basically, um, conduct an interview and here is the state that we're going to pass in.

Okay. This is pretty cool. So for every analyst in our set of analysts define an analyst, pass in the topic, and then pass in, uh, the initial starting message, and then we go ahead and kick off that interview. So once we received all those interviews, each interview's been summarized into a nice section. We have one final, we're going to call this, reduced prompt, which is basically going to take those individual sections and distill them into a final report.

And this is really it. You're a technical writer creating a report on overall topic. Here's the topic. You have a team of analysts. The analysts have just done these interviews. You'll be given the collection of memos or sections, however you want to frame this. [00:20:00] Think about them, summarize them, and some instructions for creating the report in terms of, uh, formatting.

Okay. Markdown and so forth. So that's really it. And then we're just going to have, uh, some minor steps to write a conclusion. Write an intro and put it all together. There we go. So we're going to do a few nice things here. First, we're going to create an interview as a sub graph, because remember, that interview builder has its own state.

It has the interview state with that internal message history. We don't want that as part of our overall graph states. We'll encapsulate that as a sub graph. That's kind of concept one. Concept two is that we're using MapReduce like we show right here to initiate all those interviews in parallel. Very nice, and we can see that all rendered in the final graph.

So first, human in the loop [00:21:00] to create your analysts. Concept one. Concept two, encapsulate this interview as a sub graph. It has own internal state, has its own internal message history, own internal sources. We don't want that as part of the overall graph state. Encapsulate that as a sub graph and use parallelization within the sub graph to basically do web search and Wikipedia search at the same time.

Very nice. And, MapReduce to parallelize these interviews across all of our analysts. So we're tying together a bunch of cool concepts here, and then the final reduce phase where we basically write the report based upon all the sections. So it's kind of like a reduced distillation of all those sections.

Write a conclusion, write an introduction, put it all together, and we're done. Very nice. So let's try running that all. Kick it off. Cool. We get our three initial analysts. Let's add in some [00:22:00] feedback. Add in the CEO of a Gen AI native startup. Fair enough. Add that as human feedback. Cool. Now let's rerun.

Cool. Now we have three new analysts. Very good. Update the state to indicate that we're happy and we're going to finish. And there we go. Interviews conducted in parallel. We wrote the intro, we wrote the, we wrote the conclusion. Finalize everything and we can just look at the report. Grab it from state. Here we go.

Benefits of adopting LangGraph. We get a nice intro written, nice rich report body. A bunch of sources in here. Nicely formatted. Conclusion. Source list. Not bad. Let's go and look at LangSmith. Here we go. We're in LangSmith. Now let's close this prior thing down. Let's go ahead and look at, [00:23:00] this is it. This is our final run.

You can see 50,000 tokens, 30 cents took 35 seconds. Now you can tune these things, of course. You can reduce the number of sources. You can reduce the content responded or returned from each source. You can modify the number of interviewers, so it's highly customizable. Let's open this up. Collapse everything.

There we go. There's a whole flow. Each interview conducted, these all ran in parallel. Open this up. This is just like we saw before. Here's each interview. Boom. Here's the report writing. You can just, you can dig into this if you want. This is kind of distilling all those sections into a final report we see there.

Boom. The intro, the conclusion. Finalized, tie it all together. Pretty nice. Can look at that. Yep. And the rendered output down here, there's our final report. Very nice. So again, LangSmith in this case, LangSmith is really useful just to look at overall [00:24:00] latency, overall token usage. Now here's what's cool, right?

Each of these took around 23 to 24 seconds. The overall latency is only 35 seconds. These clearly ran in parallel. Nice. Right? Otherwise you'd run these sequentially. The, the latency would be, you know, three x higher. So. Um, quite nice, right. Now I'm back in my IDE Let's say I want to run this in Studio. Go to module four.

You can open the, open up the studio folder, look at research assistant. And this is the same thing. Everything we've done in the notebook is all captured here. Very nice. I do want to show you one thing in this initiate_all_interviews conditional edge, I had one little thing. For the human feedback, I had a specific keyword, so if we're satisfied, just add a type in, approve, and then we can go forward.

You'll see it's a little bit less ambiguous than just keeping something empty. I have a specific approval keyword, and that's the only difference that we're going to see [00:25:00] working with this in Studio. Let's go over to Studio now. Open this up. Cool. Here's that full graph. Very nice. Human in the loop phase, human feedback, interviews, and so forth.

Very nice. Let's go ahead and kick this off. Set a topic, the benefits of using LangGraph as an agent framework. Three analysts, kick it off. Cool. Analysts are being created. Very nice. Now we're stopped, of course. We're at that human in the loop phase. Now let's say, I do want to add in a little bit of feedback here.

Add in a CEO of a Gen AI startup. Submit that. So we'll regenerate. Okay, so we have this guy, Alex. He's our startup guy. Cool. Now this is the only thing I changed a little bit. Now just say approve A-P-P-R-R, approve. [00:26:00] And we should kick off interviews now. Boom. There we go. So we've approved it. We're going to kick off interviews, so this is kind of cool.

You can see the different phases being done and we're finished. Very nice. And there we go. Final report right here. Pretty neat. See, when you have these larger flows, it's pretty nice to have in Studio because you can kind of go back through the history, you can look at what happened at every node. Very nice. This is what the conclusion wrote.

This is kind of the raw report content. Cool. And I can go back and look at all the different interviews. These are the final sections written from each interview and so forth. So it's pretty nice.

So we're back on our notebook. Just to wrap things up, you can see we've created a pretty cool [00:27:00] multi-agent system that can automate research. Research is really a kind of tedious and often time consuming task. AI has a lot of promise to actually assist with this, and this is a practical system that could be adapted to all sorts of different research use cases.

And, um, it's a nice illustration of the concepts in LangGraph, bringing everything together from, I'll go back to the top here. Memory, human loop and controllability all kind of rolled together in a neat, interesting, customizable multi-agent system.



=============================================
File: LCA-LangGraph-C1-M5-L1-V2-Conceptual-(3).txt
=============================================

[00:00:00] Welcome to module five of LangChain Academy. We're really excited to talk in this module about long-term memory. Now, memory is one of the most interesting ways to personalize AI applications. I want to show you this hands-on with this application I built for myself called Task Maestro. Now, this is an application that I use to manage my own ToDo list, so it's something I've built personally for myself.

But we're going to unpack this over the course of the module and build it from scratch. Now, here's how it works. I can interact with it and give it general information about myself. I'm Lance. I live in San Francisco with my wife. I have a 1-year-old. And we can see, it calls this update_profile node and it updates my general profile.

I can also give it general preferences or instructions for creating tasks. Like, for example, I always want it to give me local businesses or services I can use to accomplish things, if it's applicable. Now I can give it tasks, and again, just in natural language. It adds it. It also registers some specific places I can go.[00:01:00] 

I can update tasks just in natural language which is something I really like. I just like the ability to kind of tell it things naturally that I want done and it automatically knows what to update under the hood. And we can see it confirms the updates to this particular task. Now I'm running this all on my laptop using the LangGraph Studio application, which means that these ToDos as well as my instructions and my profile are all stored locally on my laptop.

Now, let me show example usage of this. Often I'll come to my app and just say, What do I need to get done this afternoon. What's nice is it has all my tasks stored in memory and it remembers who I am, and so it can just look at those tasks and tell me what to prioritize.

For example, in this toy case, I have a single task in there. It resurfaces that and tells me this is what I need to get done because I set a deadline for end of day. Now this is really highlighting the power [00:02:00] of personalized long-term memory and AI agents. Now everything necessary to build this we're going to go over in this module and we're actually going to build this app from scratch.

Which hopefully will be both instructive and useful because you can very easily take this and modify it for your own use case. If you want a task app like this, you can very easily modify this to customize it for your own preferences, or you can take these principles of long-term memory and apply them to other applications.

But I do want to highlight that long-term memory and AI agents is really one of the most powerful things to learn, and we're really excited to dig into it in this module. Let's first go over some conceptual foundations and then we'll dive into the code. So now that you have a taste for memory with the application that we just showed, let's dig into some of the conceptual foundations that are necessary to kind of build up to where we want to go.

So first, what is memory? The classic cog sci definition is memory is a cognitive function, allows people to store, retrieve and use information to understand their present and future. [00:03:00] Now in the context of AI applications, which we actually just introduced, you can think about memory in two different ways.

So threads in LangGraph store, for example, a conversation history between an AI bot and a user within a single chat session. The user can come back to that chat session at a later point in time, and the thread contains the entire history. Now, alternatively, there's also what we might call a cross session or cross thread memory.

This is the idea that some information can be persisted, for example, about the user across all sessions with that user or all chat threads. In the case we saw previously, these would be, for example, ToDos. It could be a user profile, user preferences, or other things. But the point is that this is information we want to be present across every session with the user.

So when you think about short-term versus long-term memory, the scope is within session or within thread, or across session, across thread. Example [00:04:00] use cases for short-term, persist conversational history, for example, throughout a chat. Alternatively for long-term memory, remember information about a specific user across all sessions with that user.

There's different mechanisms for short-term, long-term memory in LangGraph. We use checkpointing for short-term memory. We talked a lot about that in prior modules of LangChain Academy. Now for long-term memory, we're going to be introducing something new called the LangGraph Store. In either case, we can take information from short-term memory, for example, a check pointer or long-term memory, for example, the Store and pass it to an LLM, to get responses to personalize them.

We saw a bit of that in the demonstration app at the start of this. Now, short-term memory, is something we actually talked quite a bit about in some prior modules. And you'll really remember that short-term memory management is often about managing chat histories. There's different techniques you can, for example, filter or summarize chat histories.

Summarization produces kind of a condensed representation of a [00:05:00] chat. Filtering basically truncates, for example, older messages. Now for long-term memory, it helps to think about this in a few different ways. Here's a framework that I like to think about it. Two questions. What is the type of memory we care about and when do you want to update memory?

Let's talk about memory types first. So when you talk about human memory. So when we talk about human long-term memory, there's three types, semantic, episodic and procedural. Semantic memories are facts, what I learned in school, the bike model I have. Episodic memory is actual memories or experiences, bike rides I took. Procedural memory is instructions like the motor skills needed to ride a bike.

Now, there's been some nice work to map these ideas over to agents, the CoALA paper in particular, which is linked here. So in the case of agents, you could think about semantic memory as facts, like facts about a user or ToDos in my ToDo list.

Episodic memory, in the case of agents, you can think about as past agent actions or what the agents actually try to [00:06:00] do. Tools it tries to use, for example. Procedural is instructions like the agent's prompt. Now, let's talk about these a little bit one by one. First, for facts, how do you structure facts? Well, there's at least two ways to think about this.

One can be you can structure facts as a single profile. For example, in the app that we talked about, it saved a user profile that contains some general information about me, like my family, my interests, where I live, right? And basically as I converse with the chat bot, for example, it can update that profile.

That's kind of the intuition behind kind of semantic memory being managed in a single profile. And you can imagine that profile just being some general information about a user. Now alternatively, semantic information can be stored in what we might consider a collection. So in the app we talked about, for example, a collection is my ToDo list.

So the collection has a bunch of different ToDos, and I want the ability to add to it. I want the ability to edit existing items in the list. So. You know, the mechanism is [00:07:00] basically the same. You can take a conversation, you can take an old list, pass both to an LLM and update to create a new list. But this is kind of, uh, just slightly different way to think about semantic long-term memory.

And there's some kind of pros and cons between them both and that they really are for different applications. So, profile as a single document, a single easy to retrieve representation, for example, of a user, but it can be challenging to maintain as it grows larger. Alternatively, a list varies, or something like ToDos, which I want to natively be something I just add items to. ToDos are small, narrowly scoped.

Now the con is, if that list were go to were to grow really large retrieval can be a challenge. So for example, like imagine I have hundreds of ToDo items, how do I retrieve them, uh, for reasoning. And that you kind of push on the challenge from management over to retrieval.

Now let's talk a little bit about episodic memory. [00:08:00] Again, this is kind of past agent actions. We've done some work on this and there's a lot of literature on the topic of few shot examples. That is one very nice way to provide examples or episodic memory to agents. But as an example, you can pass an agent prior reasoning trajectories that it undertook with specific tools, to give it a sense for how to use that tool better in the future.

In our blog posts, we talk about some ways to use few shot prompt to improve tool calling performance. And in doing so, we show a lot of cases of agent attempting to call a tool, the mistake it made, for example the user correction. And indeed the correction in the tool call and the final correct answer.

So it just gives the agent some specific examples about prior things It's done, specifically prior tool calls to help guide its future use of that particular tool, as one example.

Now let's talk about [00:09:00] procedural memory. So there's a lot of interesting work showing that LLMs are actually very good, uh, prompters or prompt engineers.

We actually did this by teaching Claude to write a high quality summarization prompt. The intuition is actually really simple. We start with initial prompt. It's not that good. We pass in papers, we want to summarize them into tweets. Now it does a first pass. We review the tweets. And we basically give the prompt engineer model some manual feedback.

We give it the tweet generated and we say, I'd like, or I don't like this about it. And then it uses that feedback to regenerate a new prompt, and then we try again. Run that a few times. We found that we could get very high quality kind of paper summarization, matching the style we want just by giving an LLM feedback to then rewrite its own summarization prompt.

Now this can be used very nicely in the context of long-term memory, where we can take a conversation, we can take some old set of instructions, pass both to an [00:10:00] LLM, and have an LLM write new instructions, are then saved to long-term memory. And in fact, the final agent we're going to build, we'll use this to update the way I want to kind of manage my ToDo list.

So again, we can see the different memory types, semantic facts, episodic experiences or memories, past agent actions and procedural instructions. For example, the prompt. Now the second question is, when do you want to update memories? There's two different ways to think about this.

One is in the hot path of the application. I'm interacting with it, user message comes in, a memory's updated, the chat bot responds and so forth. That's exactly how the ToDo app that we just showed works. Now alternatively, we also can abstract the memory creation entirely away from the user. It can be a separate process that runs some period of time later.

And there's some trade-off between these approaches. In the hot path style of memory updating, it's what we see, for example, ChatGPT do, it's what, um, [00:11:00] our application example does, it provides real time memory updates and it can be transparent to the user, which is good in a lot of ways. However, memory updating can affect the user experience or latency and may degrade the performance if you have, for example, a complex agent with many other tools, and now you give the agent the additional responsibility to manage memories.

Alternatively, you can run memory creation in the background as a separate process. Lower risk of of user experience performance or degradation. But the frequency of memory updating that needs to be carefully tuned. If you want memories to be immediately accessible, then you need to make sure that your memory service is running at sufficiently high frequency, such that the memories are created as you need them.

So there's some trade-offs, but for now, we're going to be focusing on in the hot-path style memory, for example, in what we're going to build here in this course. And we do have a nice template you can refer to that talks a lot about, uh, background memory updating.

And at the start we kind of laid the [00:12:00] motivation for where we're going here. We're going to build a memory agent. It has the ability to write long-term memories. In this case, long-term memories are going to be my ToDo list. This can be easily extended for many other types of applications, just by changing the memory schema you want to save.

It's also going to update a profile memory, for example, a static user profile. Along with the collection of ToDos, like we just said. And it also will have the capability to update its own instructions. So it's going to be using both semantic memory for ToDos and a profile as well as procedural memory to update its own instructions. And over the course of the next two, three lessons, you're going to have all the foundations necessary to build this yourself.

Let's dive into the code.



=============================================
File: LCA-LangGraph-C1-M5-L2-V2-Take-2-(2).txt
=============================================

[00:00:00] We discussed that memory is a cognitive function, lets people store, retrieve, and use information to understand their present and future. We talked about various types of long-term memory that can be used in AI applications. Now let's actually dig in here and show how the LangGraph Memory Store can be used to save and retrieve long-term memories.

We're going to roll this into a chatbot that has the ability to save memories about a user. Now first we're using LangSmith for tracing just like some of the prior modules of this academy, and so just make sure the API Key is set. Now let's introduce the LangGraph Store. So LangGraph Memory Store just provides a way to store and retrieve memories across threads in LangGraph. It's an open source base class for persistent key value stores. Now we can import it here, this. And now we can work with this store inside this Jupyter Notebook session, and it'll persist in memory throughout the lifetime of this particular session.

Now, in storing objects to the Store, you're going to want three things.

One is a namespace. You can think about this [00:01:00] as a directory. One thing is the key. It's like a file name, value. It's just a dict with the contents. Here's a cartoon, for example. Here's the store, two different namespaces, one being user profile, one being user memories. In the profile, I'm saving key profile, value, some dict. And in memories, I'm saving a bunch of memories, keyed accordingly with some dict, for each memory is the value.

Now we can use the put method to save objects to the store by namespace and key, so we can kind of see that here. Nice. We can use search to retrieve objects from the store by namespace and we can use get to retrieve specific objects by namespace and key.

There we go. So you can see in both of these cases, we're getting back out the value, the key that we saved, the namespace and some metadata. So now we've introduced the LangGraph Store. Let's actually incorporate it with a [00:02:00] chatbot. So this chatbot will have two things. We'll have short term within thread memory using a check pointer.

Now, we've already talked about that quite a bit. In prior modules you should refer, especially to module two. But the check pointer loves to store the chat history within a given thread. And in addition to check pointer, we use the store for cross-thread or long-term memory.

Now the flow of our chatbot's going to go like this, receive an input from the user, call a chat model, respond to the user, and then we're going to write any memories that are, for example, user specific information to the store.

We can continue the chat and any memories in the store are going to be resurfaced when the model's called and the memories written to the store will be updated as the chat proceeds. So look at that first node call_model. All we're going to do, we're going to take in state, which is just MessagesState. We've seen that before.

We're going to take in a config. Now this [00:03:00] config is going to allow us to get the user id. We also take in the store. So this is really the new thing. You can actually pass store into the nodes of your graph and it's accessible. Now, just like we saw above with the user id, we can of course define a namespace.

We'll define a key and we'll use that to get an existing memory. Now this memory's going to be a dict. We'll just format it accordingly. So if it exists, just pull out the memory value. Otherwise, we just say there's no memory found. And we pass that existing memory to our system prompt, and let's have a look at that. Model system prompts just right here.

So you're a helpful assistant with memory that provides information about a user. If you have had any memory, use it to personalize your responses. Here's your memory, and we just plumb it in right there. So it's easy enough. We respond and return that [00:04:00] to our state key. Now, this is the part that's a little bit more interesting.

When we write memory, we take in state the config in the store. Just like before, get the user id, we get the existing memory from the store. Now here's the interesting point. We want to keep updating this memory. We don't want to overwrite it, so we get the value of existing memory, just like before. We passed it into a create memory prompt.

Now, this is where we're basically just going to ask an LLM to look at the chat. Look at any existing user information and update it, and you can be as specific as you want about this. This is just an example of what you might do, but we're basically saying you're collecting user information and review the chat history, identify any user specific details, personal details, preferences, interests, and so forth.

Merge any information with the existing memory and write it out. So we're basically just prompting a model to look at the chat history, look at the existing memory, and update the memory. That's really all. We get the new memory. [00:05:00] And again, we're going to write that to the store with that same key we've been using above.

So namespace, key here, and then we're going to write that as a dict memory, and then the content of the model output. That's really it. So two things I want to flag that's important here. Building our graph is very familiar. You've already seen a lot of this, but what's new now is when we compile it. We're going to supply a check pointer like before, but we're also going to supply a memory store as we introduced above.

We pass that in with the store flag. That's the new thing to know. So builder dot compile, pass in a check pointer, pass in the store, and then that store is accessible to us in all of our nodes. So we can interact with our graph. We supply a thread for the check pointer. We supply user ID for long-term memory.

Remember that user ID [00:06:00] is grabbed in our nodes via the configuration. Pass in an input. Cool. So we get a nice response here. I say something about, I like to ride around San Francisco. Great. Now we can actually always use graph dot get state passing our thread to look at the current state saved by our check pointers.

This is just the chat history. This is our short-term memory. So this is what the check pointer's tracking, and it's all basically saved to this thread id. Now remember because we compiled with our store, we can always look and see what's in the store, where we pass this in, when we define the, when we compiled the graph.

So let's have a look and there we go. So it's actually saving the memory here. Updated user information. User name is Lance. User likes to bike around San Francisco. There we go. Now let's start kicking off a new thread. So we're going to have a new thread id, same user id. Where [00:07:00] would you recommend that I go biking?

So this is pretty cool. It knows my name. It knows I'm in San Francisco. Gives me a lot of personalized responses. I don't need to restate that information because it pulls it from memory. That's the main idea here. And let's look at LangSmith to really see what happened under the hood here. So here I'm in Lang Smith.

This is that first input I provided when I created a new thread. Hi! Where would you recommend I go biking. Let's see what happened under the hood here. So we can see there's call model, write memory, open call model. Let's look at the model call itself. And this is really where the magic happens. You can see the system prompt.

We have plumbed in. The memory, which is extracted from the store. That's why it knows my name was Lance. I'm in San Francisco, and was able to personalize the response. You can see there's no other chat history here. It's all coming from [00:08:00] long-term memory, which is supplied via the system prompt. Now let me show you a little bit more in Studio.

This is the same graph we've been working with, but Studio's a nice interactive environment where we can actually see the progression and also look at the memories. So if you open this configurable field, you can always add a user id. I'll add test user chatbot memory, and I can introduce myself. Cool.

Model responds. Memory is written, so if I ever want to look and see what's saved to the store, I can just open up this memory tab. Now, here's what it looks like when I open up that tab. Now remember, we are saving memories. We were name spacing them as memory, comma, user id. That's why this first field you see here is memory.

Now these are some other namespace that I've been working with in this same overall project, which is why they're also saved and accessible here in the store. We'll be talking [00:09:00] about those later, but what's cool here is you can see memory user id, which we just input as the configuration. Open this up.

User memory. That's my key. Boom. This is the value. It's a dict memory. And then that's the user profile we've been working with. So let me test this out and open up a new thread. I'm in a new conversation. Same overall chatbot. Set configuration. Let's set that same user id. 'cause remember, memories are named spaced by memory, user id. User ID is set. And let me ask, Where should I go biking? It knows I'm in San Francisco. There we go. Gives me some nice recommendations. So again, it's using memory saved to the store to personalize responses because I provided the user ID and memories are namespaced in this case [00:10:00] by memory, user id.

So this is a very simple, kind of foundational illustration of how the store works. And how you can incorporate it in, in this case, a very simple chatbot to give that chatbot long-term cross thread memory. Now we're going to be building on this towards the agent that we're going to showcase at the end, but this is kind of foundations. How you work with the store, how the store API works, and how you can incorporate it in graphs really easily and visualize memories very nicely in LangGraph Studio.



=============================================
File: LCA-LangGraph-C1-M5-L3-V2-Memory-Profile-(3).txt
=============================================

[00:00:00] We just built a really simple chatbot that uses short-term or within-thread memory to persist conversations and long-term across-thread memory to save facts about a user across all threads with that particular user. Now it's saved these semantic memories or facts about a user just as strings, and it did it in the hot path while the user was chatting with it.

Now, we don't always want memories just to be a simple string. We often want to update, for example, a single schema with new information throughout interaction with the user, and we're going to show that now. We're going to introduce a really nice library called Trustcall to help support this.

Just like before, we'll be using LangSmith, set my API key. So Python has many different types of structured data, TypedDicts, dictionaries, JSON objects, Pydantic objects. Let's go ahead and create a TypedDict for an example UserProfile that has a name and some interests. The name's a string, TypedDict [00:01:00] allows for type hints, and interest is a list of strings.

Cool. We can create an instance of this, name, interests. Just like we saw before, we can save this dict directly to our store. Nothing new here. We can search the store for any memories by namespace, just like we saw before. Again, we'll namespace user ID, memory. And we can go ahead and get memories by the key.

Key noted here is user_profile. There we go. We get our dict back out. This is a very simple example of basically saving memories with a particular schema. In this case, a TypedDict saving that directly to the store. It's already a dict, no problem. And retrieving it. Cool. Now that's all well and good to go ahead and define a memory schema, just like we saw. But how do you actually get an unstructured chat with a user into a particular schema?

In LangChain, the chat model interface has a [00:02:00] with_structured_output method that allows you to pass an a schema and will use whatever provider you're working with to enforce that the output conforms to that structure. Now, let's see an example of this. I initialize a model, in this case gpt-4o. I call model with_structured_output, and I pass in the schema we defined right here. Creating this new model_with_structure.

Okay, now it can evoke that with a list of messages in this case. Hi, my name's Lance. I like to bike. And what do I get out? So I get, very nice, structured object out username, Lance interest biking. So it takes unstructured information from messages and will compress that into the schema that I care about.

This concept of structured output generation is extremely useful for all sorts of things. Memory creation is just one of the very narrow use cases of it, but it's extremely important and useful here.[00:03:00] 

Now, let's go ahead and use this with a chatbot. This is to me, just like we saw before. The only difference is that in our write_memory node right here, we're just going to use that model_with_structure, invoke it with SystemMessage, which we saw before, is just going to instruct, create a memory profile, and pass in our set of messages. This is the current conversation and it's going to create a new memory.

This new memory is going to adhere to this schema. So, because it's a dict, we can just write that directly to our store. Very easy. Let's do this. Create the graph, create a thread_id, new user_id. Hi, I'm Lance. I like to bike around San Francisco.

Let's go ahead and run this. Cool. Now let's go ahead and check our store to see what memories got written. Again, pass the user Id, create the right namespace. Pass it into the store. [00:04:00] There we go. User_name Lance. Interests biking, San Francisco. As I noted here in the chat. We can even try modifying this.

Let's try, My name is Lance. I like to bike in San Francisco and eat at bakeries. Cool. I'll run that. Let's check our memory now. There we go. So our interest has been amended with bakeries. So you can see what's really cool about this is it can look at chat history from a conversation and compress that into a particular schema, and then indeed write that to our store. And we can see then our memory has this type of schema that can be continuously updated.

Now with_structured_output is extremely useful. But what happens if we want a really complex memory schema, which you could want for certain applications? So in this particular case, I'm going to take kind of a complex Pydantic model that has a bunch of nesting. [00:05:00] Okay? So this is like an example of a complex schema that I may or may not want to work with at some point in time.

Okay? Don't worry about the specifics of what this is. It's more an example to show a complex schema that may or may not work when you use structured output. So just like before I take a, kind of, open-ended conversation. I'm going to create model.with_structured_output, pass in my schema just like before.

And let's invoke that here. And I'm going to put this in a try, accept for validation errors and I'll try to run this. It takes a little bit because this is a very complex schema that's trying to kind of map this conversation into. And we can see it throws this validation error. Now this is because with_structured_output is not always reliable, as the schemas get increasingly complicated.

Now this is the motivation for our library we want to introduce called Trustcall. So we at LangChain have been working on memory for quite a while and we've encountered this exact issue. So it can be very difficult to extract complex schemas when [00:06:00] using chat models. That's kind of problem one. Now I want to introduce a second important problem.

In the above chatbot, remember we've regenerated the profile schema from scratch every time we ran to save a new memory. Now this is inefficient because we're wasting model tokens to regenerate entire schema, even if something very minor changes. But actually think about another case. What if I want to make a small update and the schema's really big?

I can lose information from regenerating that entire schema every time I'm rewriting the memory. So this is actually a second motivation for Trustcall. The first, as mentioned, is that complex schemas can be actually hard to generate or extract. And two, extracting entire schemas from scratch can be inefficient or, even worse result in the loss of information.

So Trustcall is an open source library for updating JSON schemas developed by one of the engineers here at LangChain, Will. And it's motivated by these [00:07:00] challenges. Now let's show simple usage of extraction with Trustcall. And I'll tell you a little bit about what's happening under the hood.

So first I'll create a mock conversation and I'm going to use this create_extractor from TrustCall, import create_extractor. And here's what I'm going to do. I am going to create a schema. In this case, it's going to be a simple Pydantic model, username, interests. Now I create an extractor pass in my model. Just like before, I'm going to pass in my schema as a tool, and I'm going to pass in this tool_choice parameter, which tells the model to always attempt to extract for this schema.

Now in some of the prior modules of LangChain Academy, we talked a lot about tool calling. Also refresh really quickly here. With tool calling, we can bind, for example, a schema to a chat model, and the chat model can be enforced to produce outputs that adhere [00:08:00] to this schema. Now this is exactly what we're doing under the hood when we're using with_structured_outputs.

Under the hood, in many cases, for example, we can look at here. Under the hood in many cases with this particular method, is defaulting to use tool calling. Depends on the provider, but in many cases, just tool calling. Sometimes it's using JSON mode, which because certain providers provide JSON mode, but as a general rule of thumb, you can think about with structured output, under the hood, is going to use tool calling.

And again, all that's going to really happen here is that I supply a schema to the model. And the model is particularly when this tool_choice is supplied, the model is enforced to produce an output that adheres to this particular schema. That's all that's going on.

Now let's go ahead and try running this. I create my extractor, I pass in the model, I pass the profile, and I tell it. Enforce, uh, the output to adhere to this particular schema. Give it a system prompt, extract the user profile from the following conversation. I pass in my conversation, which I had to find up here, [00:09:00] and I just invoke. Cool. Now, when we run that, we get three things out.

We get this result. Now, this result is a dict, which has messages. This is a list of AI messages that contain the tool calls that it made. So let's look at that. Cool. So there's the tool calls. So you can see this is actually just like we saw above with structured outputs.

Here's my conversation. I'm Lance. I like biking around San Francisco. The schema is username, interests. The tool call, again, just creates the payload that matches this schema. That's all that's going on. So you can see the schema supplies, username and interest as a list. The tool call has arguments that adhere exactly to that schema. Easy. Now, what's kind of nice is this response, is actually, is the resulting parsed tool calls that match our schema.

So in this case. If we get out the actual object UserProfile rather than just the raw, for example, arguments. [00:10:00] And this is a Pydantic object, so it basically parses it back to whatever scheme we supply. In this case, it's a Pydantic model, so that's cool. So again, it's going from raw tool calls, parsed back to Pydantic model.

It's pretty nice. We can take Pydantic model and dump it to a dictionary. So there we go. Just calling model_dump with Pydantic V two. And we finally get out metadata. Now, metadata isn't really relevant yet. It's really applicable only for doing updating, which we're going to be talking about in a little bit.

Now, I want to motivate the idea of updating. So we talked about before, what happens if we regenerate a schema from scratch? Well, that can be pretty inefficient and it can lose information. Now the magic of Trustcall is that when it performs updates, it actually will prompt the model, only to produce a JSON patch.

So that is only a subset of the schema that we want to update. Now what's nice is it's actually going to do the operation to take the JSON [00:11:00] patch, incorporate it with the schema, perform a validation and retry for us if there's mistakes. That's where Trustcall is extremely useful. Now let's go ahead and try this out.

Here's an updated conversation. I can supply a system message. Update the existing JSON doc to incorporate new information. So again, schema came from here. We can run schema.model_dump to get a dict, or indeed a JSON back. Pretty easy. Just to refresh you, schema itself was a list of parsed tool messages that adheres exactly to the schema supplied, which is this UserProfile, Pydantic model.

We can then just dump those as a dict, pass those back to Trustcall in this [00:12:00] existing field. We pass that with this UserProfile. Which is just the name of the tool as you see right here, UserProfile. So this is the schema resulting from use of the UserProfile tool. Here is our new set of messages, and we want to do is actually update the schema here.

Here's the updated schema. Now let's also use Trustcall to test the challenging schema, we worked with you earlier. Again, we create an extractor where we supply the schema, we enforce the tool choice, passing in a conversation. So it works. Now, let's open up LangSmith to build a little bit more intuition about what's actually going on here under the hood.

Now here I'm in LangSmith. I'm looking at the trace. Let's go and open up Trustcall. We can see this is a bunch of different steps. Let's look at extract. Here's our open AI call. Now this is what's interesting. [00:13:00] This is the prompt we supplied. Here's the conversation. Here is the schema we provided. It looks like it's just calling this tool basically, and it's going to attempt to just output basically a structured object that adheres to our schema.

So there we go. Now this is where some interesting stuff is going to happen. You can see it goes to validate. Now here it's actually going to attempt to validate our schema, and you can see there's some validation errors, so you get some issues here. So it's actually attempting to see if the schema, if the resulting structured output adheres to the schema we supply.

So it's doing this validation for us. Now, this was pretty cool. This patch then, it's calling a model again. Now you can see it's calling this new tool PatchFunctionErrors. This is a tool that's within Trustcall that we didn't, we didn't provide explicitly, but it's basically attempting to correct from the validation errors resulting from the original extraction.

Now, this is where we're getting into really the guts of what's happening in Trustcall. [00:14:00] Which you may or may not be interested in, so I'll let you dig into this further if you really care. But the intuition is simply that it's going to attempt to create a JSON patch to fix the error specifically that is presented.

So again, you can see the error here and it's going to attempt to, basically can see the planned_edits are articulated. It tells, kind of the JSON, the telegram field was incorrectly set, and it's going to do a bunch of work to attempt to create a JSON patch. It will apply the patch and then we can go.

We'll see it, reruns validation and validation passes and it ends. So this ability to run validation, catch errors, patch them using JSON patches, then further validate is what allows it to self-correct, in cases where raw extraction fails. That's the main intuition as to why Trustcall is really useful for working with complex schemas.

So we're back in the notebook and we've introduced Trustcall, which is a really nice library [00:15:00] for updating and creating structured outputs. Now let's take our chatbot and modify it slightly. Now, the only differences are that in the chat model instructions, call_model node, it's all the same. We're going to provide our Trustcall instructions in the write_memory node, and here we're just going to do what we showed above already.

Get existing memories. Format them for Trustcall. Again, this is the schema name, IE, the tool name. This is the existing memory and invoke the extractor with the Trustcall instructions, as well as the message history and an existing schema. It performs an update and under the hood Trustcall does everything we just showed.

It's going to attempt to extract per our schema. If any mistakes are present, it will try to [00:16:00] patch and correct them. We get an updated profile simply dumping the Pydantic object to a dict. And we put that dict in our store. That's really it. So let's just see that in action. The functionality should mirror exactly what we saw previously.

I'm Lance, great to meet you. I like to bike around San Francisco. Cool. And in this case, you can see that the memory is just this structured object. That's really the only difference. We can look at the value itself. Again, this is the dict. Now, just like we saw before, we can start a new thread, same user.

What bakeries do you recommend? And it gives me a bunch of good recommendations that are localized to me being in San Francisco, which of course it only knows from this structured memory, which is propagated to the new thread. We can see this in LangSmith.

Open up the call_model node, open up the ChatOpenAI call, [00:17:00] and we can see that profile is present here in the system message, which is why when I just ask an open-ended question about bakeries, it knows my location and it conditions the response based on the location. We can see the same thing in action in Studio. Just like before the graph looks the same, kick it off. Cool. Write some memory.

I create a new thread. I ask a question about bakeries. And it knows I'm in San Francisco, it offers a bunch of good suggestions that are local to San Francisco. Now this is the same as we saw before. Not too different, but we've upped the complexity a little bit because now if you look at our memories, our memories adhere to a structured schema.

These memories are also being updated using Trustcall to both create the schema and update it accordingly. So we're going to keep building from here, upping the complexity, getting to our memory agent. 



=============================================
File: LCA-LangGraph-C1-M5-L4-V2-Memory-Collection-(1).txt
=============================================

[00:00:00] So we are building up to our memory agent. We talked about the concept of long-term memory. We talked about using the LangGraph store to save long-term memories. We just showed how to save memories as a single structured object profile. Now let's just extend that a little bit to also talk about memory collections.

Remember we talked about profiles and collections profile being a single representation of, for example, a user that you save as a memory. A collection being a collection of different memories that you can propagate and update. So just like before, we'll go ahead and use LangGraph here. So just like before, we'll go ahead and set our LangSmith API key.

Let's talk a little bit about collection schemas. So we introduced it a bit previously. But sometimes we want a flexible collection to store information over time and allow it to grow. So, [00:01:00] for example, go all the way back to our kind of motivating agent example. We're going to build a ToDo list application or agent, and we want items of the ToDo list to kind of grow over time.

Now, alternatively, we also want to maintain like a user profile, for example, like information about a user, and that's a single representation. And we just talked about that previously. That is kind of continuously held and updated. Collections are very useful for cases where we want an open-ended edition of new memories over time.

So let's consider a simple schema for memories. Here's an example of a memory. It just contains some content. And a collection is just a list of memory objects. We can use with_structured_output to produce outputs that adhere to this schema. So let's go ahead and try that out. Invoke a model with structured output, pass in a message and cool.

We get two memory objects out. One is my name's Lance. The second is I like to bike. Cool. And those are both Pydantic models. Just like we saw here. [00:02:00] We can get one from our collection, dump it to a dict. Cool. We saw that before, and of course we can save each of those to the store. Now here, what's a little bit interesting.

Remember before, we saved to the store namespace, key, value. Before when we had a single profile as our memory, the key was always the same. We just overwrote every time. Now here we're saving different memories, and so we want a collection of different keys, and in this case we'll just have each being a unique UUID.

So there we go. And we can just use search to get them all per our namespace and we can print them out. There we go. You can see the value is just the dict that we're storing the namespace and some metadata. Now, when it comes to updating the same challenge we saw previously also apply. For example, each memory in our collection could have a complicated schema.

We need the ability to update any given [00:03:00] memory, even if the schemea is complicated. That's the same challenge we had with a single profile that motivated use of Trustcall. So we talked about Trustcall as being a very nice way. To both generate complex schemas and update them. The intuition under the Trustcall was the use of JSON patches, and validation to check that the extracted object actually meets the provided schema.

If it fails validation, Trustcall will attempt to JSON patch incorrections and will do, kind of, a patch and validation loop until validation passes. Now what's really neat is Trustcall also enables the ability to create a list or collection of memories. All we do is just provide this enable_inserts.

So what's going to happen here is we can basically allow Trustcall to update elements in a list or add new elements [00:04:00] to the list so it has that flexibility. So just as we did before, we create a trustcall_extractor. We provide the Memory schema to it, and we just flag enable_inserts to be true. So let's kick this off.

We'll provide some basic instruction, extract memories. We'll provide a conversation just like before. Run that. And we can look at the output. Here's a tool call, so it extracts basically a memory that adheres to our schema, which is just very simple. It's just basically a content field. And we can see here is the resulting object, itemized bike ride.

Cool. Metadata is just the tool call in this particular case. And I also update the conversation. I had a little bit more detail about what I do, about, I did after my ride. I ate a croissant, and then I also, you know, provide some reflection about what I've been thinking about. I provide this so updated instructions and my existing memories.

[00:05:00] Okay. You can see the existing memories are supplied as a tuple with an id, the tool name Memory, and just the memory itself as, for example, a dict. Here we go, right there. Invoke the extractor with the updated conversation and the existing memories. Cool. Runs and nice. We can see now there's two different tool calls out.

And we can look at the responses. Cool. So it looks like, one, is kind of updating my initial memory about the bike ride with some additional information about what I did. And it also stores that I ate a croissant at Tartine, and I'm thinking about a trip to Japan as an independent separate memory.

Now what's cool here is you can also see in the metadata now. It amended the initial memory you can see by this json_doc_id identifies the memory that is being amended by this tool call with new information. [00:06:00] And it creates a new memory, which is noted as this kind of new tool call.

So that's a way to think about this. Basically, you're running the extractor providing a list of existing documents to it, and basically the tool calls will update any existing documents, and insert new ones. That's really it. It's very simple. So this is really nice for working with collection schemas because it allows us to pass an existing collection. And basically, it uses tool calling to update elements in the list and add new elements to the list. All done for us.

So again, I'm just going to add this to my chatbot now. Here's my memory schema, just like we defined above. Here is our trustcall_extractor, just like we defined above. And this is actually the same as we saw before. We have some chatbot instruction that's going to load any existing memories.

Now we're just going to have the Trustcall instruction, which is going to match what we saw previously. Now you can modify these instructions [00:07:00] accordingly, but um, in this particular case, we'll keep it fairly simple. Now really the only differences from what we saw with the profile is that I'm just going to call store.search with the namespace to pull out all of my memories as a list, and I'll format them accordingly.

Here, I'll pass them into the prompt just like this. So it's very similar. Only minor difference is before we were using get to extract a single memory object, whereas now we're using search to get a list out. That's really it. Likewise, when we write memories, we just invoke our extractor just like we saw above.

We pass in the existing memories, which we'd get right here from the store. That's it. Really simple. Format them as a tuple, just like we saw above, with a key, a tool name and the dict itself. Now, here's where the magic of Trustcall is quite useful. Remember, we're saving each [00:08:00] memory to the namespace. The namespace in this case is just memories and user_id, so that's unchanged, but each memory has some key, okay?

Now that key is going to be just a UUID if it's a new memory, but if the Trustcall extractor provides a document ID in the response metadata, then we know that we want to update an existing memory. So we're just going to overwrite per that doc id. That's really it. Now, remember. How does that json_doc_id get provided?

Well, when we provide Trustcall with existing memories, we're providing it with the IDs right here. That ID we provide it with is just the key. So this is pretty nice. Let's walk through it [00:09:00] logically. If we do it from scratch. We use the extractor, we have no memories. Uh, in our store, we do extraction. Each memory has some UUID defined here and we write it to the store.

That's great. So then on a second time around, we get all the existing memories. The existing memories all have a unique UUID, as the key here. We provide them to Trustcall as tuples. Trustcall makes the decision to update any existing memories and it retains that UUID of the memory. So what's really cool is, if it decides to update any existing memory, that UUID is present here in this JSON doc id, we get it from the metadata and we just write that back to the store with the corresponding doc id.

That's it. So that's how we can update existing memories really nicely. In the LangGraph Store. Now let's go ahead and create a graph here. There we go. This is the same flow we've seen before. Let's go and kick it off. Hi, my name's Lance. So the [00:10:00] functionality here won't look very different. I like to bike around San Francisco.

Cool. We can look at the memories we've written here. So you know, basically, it has a memory, I'm Lance. It has a memory, I like to bike around San Francisco. And again, you can really tune the way, you know, or kind of the, the rules by which it writes memories. Uh, in that system prompt and in the schema. But this is like a very simple example where the schema is just content and then like some information about the memory.

And again, I'll just continue the conversation. Biking and bakeries. Now, let's say I create a new, uh, thread, same user. What bakeries do you recommend? Same deal as before. Cool. It recommends a bunch of bakeries in San Francisco because it's pulled that from memories. Now let's look at this a bit in Studio quickly just to, uh, explore a bit more about what those memories look like when they're saved at the Store.

So we're in Studio. We open up chatbot_memory_collection, [00:11:00] and I interact with it and I give it something that I did. Right. Takes care of memory writing. I also went biking in Marin Headlands. Cool. And it's conversing with me as normal, and it's also doing this memory writing. I can see that memory node being called. Um, this null just means that the node itself, like it's not returning anything to the user.

That's fine. Now I can open up a new thread. I can ask, What bakeries would you suggest after biking. Call model. Cool. And it suggests a bunch of SF bakeries because it has those in memory. Pretty nice. We can look at memories here and now we can see instead of a single profile, there's a list and each memory just has different information about what I did.[00:12:00] 

Those were of course, pulled into context. To answer the question, we can open up the run in LangSmith. And we can see in that model call, go to ChatOpenAI, and we can see existing memories are loaded right here. So pretty nice. And that's exactly why it's able to condition the response according to my individualized preferences.

So we're building up pretty nicely to our memory agent. We initially talked about general concepts for memory. We talked about using a LangGraph Store with a simple chatbot. We amended the chatbot with a structured schema for a user profile, and now we amended it to include a schema for memory collection.

We can bring this all together into our agent.



=============================================
File: LCA-LangGraph-C1-M5-L5-V2-Build-an-Agent.txt
=============================================

[00:00:00] So we've built a simple chatbot that saves semantic memories to a user profile or a collection, and we've introduced Trustcall as a way to update either schema. Now I want to pull these ideas together to build an agent with long-term memory. So our agent, which we introduced briefly at the start of this module is called task_mAIstro.

And I actually use this to help me manage my own ToDo list. So let's talk about some differences between this agent and the prior chatbots. The prior chatbots always reflected on the conversation saved to memories. Task_mAIstro will decide when to save memories, so that makes it an agent relative to the chatbot that always follows the same control flow.

This has the ability to make decisions that dictate the control flow of the application. Now also the chatbots either save to a profile or a collection depending on the variant we are working with. Task_mAIstro can save to multiple types of memory. It can save to a profile to collect user information. [00:01:00] It can save to a collection of ToDo items.

So in addition to semantic memory, task_mAIstro can also save procedural memory, which means it can save instructions about how to create ToDo items, that are provided by the user. So let's dig into the code here a little bit. First, we'll use LangSmith just like before. Now, I wanted to do something important before we dig into the agent itself.

So we've been using Trustcall quite a bit. Trustcall is a great way to create and update JSON schemas. Now what if we want a little more visibility into the specific changes Trustcall's making. Like we talked about Trustcall using JSON patching, for example, to update a memory. But what if I want to know very precisely the patch that Trustcall added?

You'll see this is very useful in our agent because we want our agent to be able to report back to us if an update is made. Here's this precise update I made to the memory. And for this we need to dig a little bit into how we can get visibility into the [00:02:00] updates that Trustcall makes. So this will be review for the most part at the start.

I'm going to create a memory schema, and I'll have this list, which is a collection of memories. Cool. Nothing new here. Now here's the only new thing. What I'm going to do below is I'm going to create a listener for the Trustcall extractor. Now this listener is something that LangChain provides. You can see this with_listeners and you pass in, uh, for example, a class here that we're going to define.

And I've shown some documentation here, but the key intuition is this. The trustcall_extractor is a LangChain runnable object, which means that we can add this listener and basically get the runs, which you can think of as like the steps in the execution of this extractor. We actually can fish them out. And process them accordingly. In particular, we can get any time that Trustcall makes a tool call.[00:03:00] 

Now we talked about Trustcall doing all its JSON updating using tool calling under the hood. So this is a very nice clean way to extract very precisely what Trustcall's actually doing. So all I need to do is create this class Spy. This is all customizable. Initialize it, initialize my model. Create the Trustcall extractor just like before, and I just add this with_listeners spy to it.

Now let's run Trustcall like before. I pass in some instruction, a conversation. And you see in this case, I'm not using the listener stuff. This is just my trustcall_extractor. It's not this trustcall_extractor.with_listeners. This is just like what we had done before. I can see the tool calls here. That's all great. Responses. Response metadata.

Now let's update the conversation. And we'll go ahead and format any existing memory, just like we did before. So there we go. Lance had a nice bike ride in San Francisco this morning. Cool. Now we're going to [00:04:00] call our trustcall_extractor with this see_all_tool_calls.

Okay. So this is our trustcall_extractor with the listener on the end. So we run Trustcall and now we're using our extractor that basically has the visibility into all tool calls made under the hood within Trustcall. Now you can see, if you look at the response metadata, you see two things. Basically, you see that the initial document that we passed, this json_doc_id zero, which is right here. You can see ID zero was edited, and then we have also a new memory that's created.

Okay, so that's what the response metadata tells us whenever it passes the json_doc_id. It's indicating that it did make an edit to an existing memory. And we can see that these tool calls are parsed to the final responses. So that's great. But if we want to see under the hood all the edits that Trustcall made in order to produce this edited initial [00:05:00] memory.

That's where we basically use that spy.called_tools. And this gives us very specifically, you see this PatchDoc tool. This is a tool that's within Trustcall. So it's not something that we passed in externally. It's not one of our schemas. This is a tool that Trustcall has and we can see, it actually tells us the planned edits that it wants to make to that particular memory.

This is going to be really useful in our agent because it gives visibility into what changes specifically Trustcall makes to any existing records or documents, and in our case, it's going to be ToDos. That's really all I want to show you here. Now what's kind of nice is we can just write a simple little function to kind of format this in a nice way.

We can call it on spy.called_tools and the schema name, for example. And what this gives us is a really nice string that just says, here's what Trustcall did. It planned to update this document with these changes, and here's the added content. Cool. [00:06:00] Uh, and then also it created a new memory.

So this gives a nice kind of string representation of what we actually did to either create or update memories coming directly from Trustcall. Which is really nice when we build our agent, 'cause it allows us to communicate to the agent. Here's what Trustcall did under the hood. The agent then can respond back to the human with a sensible response based upon these actual changes from Trustcall. So I just want to flag that before we get going.

Now let's get to actually building our agent. I mentioned that task_mAIstro is going to have access to three types of memories. One will be a profile for the user, one will be a collection of ToDo items, one will be instructions about how to create ToDo items. Okay? So in order to enforce the agent to make a decision about one of these three memory types, what I'm going to do is a nice trick here.

I'm going to create a memory tool. Which is a [00:07:00] TypedDict that returns one of three memory types. So our agent is going to call this tool when it wants to update memory, and it's going to return to us one of these three options. This is a very nice little trick for enforcing a decision and a particular output schema from an agent.

Now let's lay out the logic of the agent. So we've done very similar things in the past, with our chatbots. We have first defined different types of schemas. In this case, we're going to define a schema for a user profile and a collection. In this case, user profile is going to have a few different fields. Now, this is entirely customizable though. Name, location, job, connections, interests.

And then our collection schema in this case is going to be a ToDo list. So it's going to have tasks, time to complete, a deadline, solutions and status. Now here's what's nice. This is entirely customizable. You [00:08:00] can modify these any way you want. These are just the particular memory schemas I personally like to use when I build this original agent for myself.

And now let me show the flow of the agent before we talk through each node individually. So here's what's going to happen. I'm going to provide user input. And it's going to go to this task_mAIstro node. Now this is kind of the core, logical component of the agent, where we're going to have a model that has that memory tool bound, and it's going to make a decision based upon the input to do one of four things.

One, update the ToDo list, two, update instructions, three, update profile, or four, just respond directly. And it's going to use tool calling to return a structured output that it routes to one of these three nodes. And if the tool call is not made, it just responds directly. It goes here to end.

Now let's look [00:09:00] at this task_mAIstro node in detail. So here's the node, and you'll see this actually looks a lot like we worked with before. We pull in state, we pull into configuration, we pull in the store. Cool. We get the user_id. Now here you're going to see something kind of cool. We have a few different memory types. Okay? But that's not a problem.

They're all just namespaced in our store slightly differently. User ID profile, user ID ToDos, user ID instructions, no problem. For each type, we pull out the existing memories. Cool. And we just format them accordingly. So if there's memories present, we'll set that to the user profile. If there's ToDos, present, format them. If instructions present, just pull them out, no problem.

Now here's what's pretty nice. Take our model system message, format it with the profile, toDos and instructions we just pulled out from memory. Let's look at this model system message. You can see [00:10:00] it's defined up here. And you can look at all the text here in detail on your own, but here's the intuition.

You're just going to say, look. Here's the current user profile you have. Here's the current ToDo list. Here are current instructions. You give it information about, here's the types of long-term memory you're tracking. The profile, toDos, and general instructions. Cool. Now, here's where you instruct it about tool calling.

You basically say, look at the interaction. Decide whether you want to update your long-term memory, and if so, if it's personal information, call the update memory tool with user. If it's ToDos, call update memory with ToDo, if it's general instructions, call update memory with instructions. That's really it. And this is just some simple instructions about what to communicate back to the user.

Don't tell the user you updated the profile. Tell them if you updated the ToDos, don't tell them if you updated [00:11:00] instructions. So this is all customizable. That's what's cool about it. You can modify this prompt any way you want. This is how I like to do it. Err on the side of updating the list, respond naturally to the user after the tool call was made.

That's it. Pretty simple. So all we do, again, we bind our update memory tool to the model. We invoke it with our prompt as well as any messages that are in state. We respond. That's it. If we look at our flow here, when we respond, we can go a few different places, right? We can go to end, we can go to update_todos, update_instructions, update_profile.

Now, how's that done? That's done in this conditional edge here. So you can see we start, we go to task_mAIstro, then we go from task_mAIstro to route_messages. That's a conditional edge here. Now we can even look at that right here. So all that's going to happen is, it's very simple.

We look at the last message from the model. Is it a tool call? If so, we just extract the argument. [00:12:00] It's either user, todo, or instructions based on the type of memory that the agent decides it wants to update, and we route to each node accordingly. That's it. Super simple.

So let's look at those nodes. Go to update_profile node that's right here. So again, all we're doing here, get the user Id, get the namespace, get existing items from the store for profile. Format the current profile and we invoke the Trustcall profile_extractor with our messages as well as the existing profile. We then save profile updates to the store.

And this is one subtle but important point, we return a tool message to our agent indicating that its original tool call, noted here, to update the profile has been done. This is a very important thing with tool calling. When an a chat model makes a tool call, it [00:13:00] expects a tool message back, verifying the tool call was actually performed. This is a very important point in, kind of, tool calling workflow communication. And when we build these agents graphs, it's very important to ensure that whenever our agent makes a tool call, we respond with a corresponding tool message saying yes, that tool call was performed.

Now that point also motivates what we talked about before with the spy. So let's talk about our update_todos. Now, the profile is typically going to be a single document. You can modify it to make it multiple documents just by changing the Trustcall extractor if you want to. This profile_extractor, you can basically add the flag to enable updates, but currently it's just going to be JSON patching a single profile.

Now with ToDos, this is going to perform updates on a collection of ToDo items. So the complexities increased a little bit. We talked about this with Trustcall already. The start of the flow is just like [00:14:00] before, get the user ID, namespace, existing items, and then format those existing items accordingly.

This is effectively the same prompt as before. Now, here's where it's a little bit different. We talk about our spy. Let's define our spy again, initialize it, and we're going to create our extractor here with a spy as the listener. So you can see here, he passed the tool ToDo the tool name again, as noted is ToDo.

And now we're enabling insertions, okay? Into our ToDo list. So what's going to happen is extract. Run our extractors like before with the messages and the updated memories, existing memories, updated messages. Now save everything to the store. No problem.

Here's where the trick comes in. Take the initial tool call that was made and remember that method that we, or that function we defined, extract_tool_info? Call that with spy.called_tools. [00:15:00] This is going to basically dump all the information about what Trustcall did in terms of its patching slash updating of memories or ToDos or creation of new ToDos into a nice tool update message that we pass back to our agent.

This is the key point and that's why I went to all the trouble to introduce this at the top. Because when you provide the specific updates made by Trustcall back to the agent, the agent provides much nicer, more sane kind of feedback back to the human saying. Okay, I've made these precise updates to these ToDos. So it really closes the loop in terms of what Trustcall is doing under the hood in memory management and updating, and then what the agent is communicating back to the user. That's the key point here.

Now, let's finally talk about update_instructions. Remember, this is like the procedural memory, which is basically allowing the user to specify to the agent how it wants to save ToDos. So just like [00:16:00] before, we'll get the user id, we'll get the namespace and we're basically going to get any existing instructions, okay. Which is namespace and then user_instructions is the key here.

Now we go ahead and pass those to our system prompt, formatted accordingly. We then invoke the model with the system prompt and the current chat history. We generate any new instructions, we then just save them back under that same key of user_instructions.

Okay, so it's namespace being instructions, user id. Key being user instructions, and then accordingly, just whatever the new instructions are. Again, we return tool call back indicating that instructions have been updated. We already covered the router briefly. Now this kind of outlines the overall flow of task_mAIstro.

So let's see this in action a little bit. Again, I'm just going to pass a thread_id for short-term memory [00:17:00] as well as my user_id for cross-thread or long-term memory. I pass on the input. My name is Lance. I live in SF with my wife. I have a 1-year-old daughter. Let's try this. Cool, nice and quick. We can see the tool calls are made to the user profile, profile's updated. And we don't return anything to the user about how we updated the profile. So that's it.

Now I want to update a ToDo. I say, my wife asked me to book swim lessons for the baby. Okay, now you'll see a little bit of the magic of what we did here. Okay? So it's going to call the ToDo tool, which is exactly right.

Now, here's what's pretty cool. This tool message we talked about above quite a bit, this is what we are directly getting from Trustcall. Okay? So in this case, we know precisely what Trustcall did and we're annotating it as a new ToDo is created. And then the model's nice response to the user with what actually happened. I've added a new [00:18:00] task to your ToDo list. Cool.

Now let's try updating instructions. So basically I'm going to give some instruction when creating or updating ToDo list include specific or local businesses. Cool. So now the instructions tool's called and the model responds kind of nicely.

Now let's go ahead and search and see what I have in my memory. Cool. So for instructions, it basically has these updated instructions I just gave it. So that's fantastic. I can give it a new ToDo, and this is pretty neat. So we get a very detailed overview of what Trustcall actually did under the hood. It created a new ToDo and it actually updated our prior ToDo with some local businesses.

That's pretty neat. Cool. It actually includes this La Petite Belene swim school, which is exactly the swim school that I've been looking into. So that's really nice. So that's pretty cool. It gives us kind of a nice response of what it added to our ToDo list. Great.

I can check the ToDo list here in memory. I can see I have two [00:19:00] tasks. Book the swim lessons, fix the door lock, and now I can interact with it and say, for swim lessons I need to get that done by end of November. Now let's look at the trace to see what happened under the hood, just to reinforce this a little bit. So here I'm at the trace. Here's my most recent input, and this gives me the whole kind of history and all the outputs and basically that final output.

But let's walk through this step by step. So first, this was my input. Task_mAIstro received it. So here's the chat model call. We can see that it has this UpdateMemory tool bound. And this is really nice to note in LangSmith, you can see when a tool is called by this called flag right here. So we know that this tool has been called, based upon our user input, and we can scroll down and see that it was indeed called and it returns todo.

So, basically, the model looked at this input and said, yes, I want to update memory and I want to update my [00:20:00] ToDo list. Cool. So you can see then it routed to update_todos. That node. Let's open that up and inside here is where we call Trustcall. Go down here to the ChatOpenAI call within Trustcall. And we can see here it calls our PatchDoc tool.

So this is a tool that lives within Trustcall. We talked about a little bit previously when we talked about using SPY to extract tool calls being made under the hood within Trustcall. It calls this PatchDoc tool and we can see down here the planned edits to the existing ToDo. Great. And we just see it decides to add a deadline.

That's really it. So what's really nice is you can kind of work hand in hand with LangSmith to really see what's happening under the hood when you're running your ToDo list application here. Now I can continue to interact with the app. I can add new ToDos, I can check my current memory.[00:21:00] 

Now what's nice is I can create a new thread here. Okay, so this is kind of starting a new conversation, same user id, and I can ask the app in an open-ended way. I have 30 minutes. What task can I get done? And cool, it gives me some suggestions based upon the estimated time to complete, and I can follow up with it and just chat. Cool. In this case, it responds directly. No tool calls being made. So this shows kind of in detail how I put together this agent.

Now I'm back in Studio. Let me show you just some example further usage that I actually do with my task_mAIstro app. If we open up the memory tab, we can see default user. That's just me. Whenever I open up LangGraph Studio, and I don't, in the configurable field, specify a user id, it'll default to just default-user for convenience.

So this captures a bunch of ToDos for me. I can, in an open-ended way, ask. It looks [00:22:00] at my tasks. I can update these just in natural language. I can add new tasks. I can quit out of the app and come back to it at a later point in time. My memories are all saved locally. And I can basically ask, what do I have to do today or tomorrow? Let's say, is there anything I absolutely need to do tomorrow?

And the agent can look at what's due coming up and give me kind of reasonable responses. This is just one example of a popular use case for an application managing ToDos that is significantly enhanced by long-term memory. Because, I really want an agent that can remember things I need to do over long periods of time and help me track them with a nice natural language interface.

That's really the intuition here. Now the key point is you can take this and modify it in any way you want. For [00:23:00] example, you can change the schemas of the ToDos, of the user profile. You can also take these principles and use them to go build other kinds of memory-centric applications. So this ToDo list app is just one of many things you can build that combines the power of, kind of, agents and long-term memory.

But hopefully this was a good jumping off point to lay the foundations to help you go forward and have fun building memory-centric applications.



=============================================
File: LCA-LangGraph-C1-M6-L1-V2-Concepts.txt
=============================================

[00:00:00] Welcome to module six of LangChain Academy. In the prior modules, we built a number of different and interesting graphs that showcase the capabilities of LangGraph. For example, we built a research assistant in module four. We built task_mAIstro in module five that showcase long-term memory.

Now, you might wonder how could you take any of those graphs that we've built and actually deploy them, which means serve them to users, build a front end for them. Are there other things you might want to do with a practical deployment of your graph?

Now, here in module six, we're going to show you how to do that all from scratch. Just to preview a little bit, we're going to show how you can build a deployment from any graph using the LangGraph CLI.

We'll show various ways to connect with the deployment. We'll show all the capabilities you get with the deployment, including everything we've talked about previously, like streaming, human-in-the-loop, but also some new capabilities like assistants as you see here, and handling of complex user interactions like double texting.

So overall, by the end of this module, you're going to have a deployed version of the task_mAIstro app, which you could actually connect to a front [00:01:00] end and actually serve to users with different configurable assistant versions. For example, we're going to show a work assistant for work to-dos and a personal assistant for a personal to-dos, which are configured differently, but all stored in the same deployment.

Let's go ahead and get started first by talking about some conceptual foundations for building deployments with LangGraph Platform. So, as we've seen, LangGraph is a library for building controllable agents. We've seen lots of interesting examples of this, such as task_mAIstro, such as the research assistant. Now LangGraph Platform is a way to very seamlessly deploy these agents to production.

Now, what are some of the things you get with LangGraph Platform? Basically you can productionize all these LangGraph features we've talked about. We've talked about streaming, we've talked about checkpoints as short term memory. We've talked about human the loop. Most recently in module five, we talked about long-term memory. So all these things you can productionize seamlessly with LangGraph Platform. But you do get a few new things for [00:02:00] productionization that are unique that we haven't talked about before.

You get agent scheduling, for example, CRON Job if you want your agent to run on a schedule. You get background runs in task scheduling for long running agents. For example, if you want, basically, to kick off an agent and go away, LangGraph Platform has support for kind of long running or background runs. LangGraph Platform has support for practical challenges that come when productionizing agents, such as double texting if a user sends a request and sends a follow-up before the prior request finishes. We have many ways to handle this.

And finally, configurations. It's very common to have different versions of an agent that you want to run in different scenarios. LangGraph Platform has very nice support for agent configurations and versioning. Now first I want to talk about the components of LangGraph Platform.

So you have a graph. We built a lot of graphs in the course of the various modules. For example, in the [00:03:00] various Studio directories, you see graph dot py, you have some Python script that specifies the structure of your graph. Now what happens is, we use a LangGraph ClI to turn that graph into a deployment. And this deployment can be interacted with in various ways.

There's various clients you can use to interact with your deployment. We have a Python and JavaScript SDK. There's the Remote Graph abstraction within LangGraph interactive deployment you can interact with via HTTP. We also have development tools. For example, LangGraph Studio, which we've worked with quite a bit, as another way to interact with deployments.

Now let's talk about the components here. So first, the CLI. So basically this builds a docker image with code to run a server, your graph, and any dependencies. Now, when we use the CLI to build a deployment, what's actually going on? There's a few concepts to understand here. First, let's talk about the server itself.

So the server [00:04:00] is composed of two different types of workers. One is an HTTP worker. This handles all client communication. Second is the Queue worker. This is actually what's executing your graph. Now we have these two workers in the server, but how do they actually communicate? That is what we use Redis for.

So Redis basically allows for communication between workers, which sets up a few important use cases. One primary one being streaming, another being things like run cancellation. There's also Postgres, which is a database that supports short and long-term memory. It stores a task queue.

So if you kinda walk through this, if a client issues a request to execute a run, the HTTP worker receives that, writes a run ID to Postgres. Then queue workers can poll Postgres for new runs, execute them, publish updates from the run to Redis. The HTTP worker [00:05:00] can subscribe to the associated run topic in Redis, get updates, stream them back to the client. So that gives you a sense for how these databases work in concert with the server to manage how the deployment actually executes runs, and provides updates back to the client.

Now, we do have some very nice development tools for working with deployments. One being LangGraph Studio. You've worked with LangGraph Studio quite a bit over the course of the prior modules, largely using the desktop app. I do want to mention LangGraph Studio is also available in cloud, which we'll talk about at the end as one means of deployment.

And this is a visualization of Studio. This is actually showing the prior app that we built, task_mAIstro. And we've worked with Studio quite a bit. It's a very nice development environment for visually inspecting, debugging, and running your graphs. Now clients are something we talk about a little bit less.

So there's clients for working with LangGraph Server API in [00:06:00] Python and JavaScript, which are quite useful. There's also the Remote Graph abstraction for working with deployed graphs in the LangGraph library. So the difference is if you already in LangGraph and you want to do things like take a deployed graph and for example, create a sub graph from it in a new graph that you're building, you can use Remote Graph to do that. If you just want to interact with the graph, then you can use the SDKs.

Now how would I get started with deployment? So we have Self-Hosted Light, which is free. It's limited to up to 1 million nodes per year, executed per year, and it does have some missing features, but it's very easy to get started. Just download the CLI.

Now there's a few production options, so one being LangGraph Cloud. So that's actually managed for you on our infrastructure and it's the easiest in terms of onboarding. There's also BYOC, which is managed for you, but uses your infrastructure. So for example, if you want data privacy, but a managed service. There's also [00:07:00] Self-Hosted, which is managed by you on your infrastructure, which gives you full control.

So those are the productionization options that you have available to you with LangGraph Platform.



=============================================
File: LCA-LangGraph-C1-M6-L2-V2-Creating.txt
=============================================

[00:00:00] So now we're going to actually talk about building a deployment, and I want to just preview this really quickly first to show how easy it is, and I'm going to go back and walk through everything in a lot of detail. So really to build a deployment, there's only two things you need to do. If you go to module six, go to that deployment directory. This is everything necessary.

So we're going to run langgraph build -t my-image, to build a docker image for the LangGraph server that actually runs your code. Once the image is built, you're going to create your own Docker compose based on the example. Then just run docker compose up, to actually launch your deployment.

And there it is. We have a deployment running locally. It contains three containers. The server, which we just talked about, built from our image that we just made, and Postgres, Redis, as we discussed in the prior lesson. Now I want to walk through this in a lot more detail. But I did just want to show you [00:01:00] how easy it is to do. It's really only two commands.

Now let's go ahead and get hands on here. Let's create a deployment for the task_mAIstro app that we built in module five. Recall, this is an app that I can use for to-do list management. It uses long-term memory, and we're going to show you how you can very easily deploy this using LangGraph Platform.

Now, first, it's important to recognize that when you create a deployment, you need a very particular code structure. In particular, you need a langgraph.json file. That's basically the API configuration file. You need your code, you need requirements.txt, and you can optionally supply environment variables with an .env file or docker-compose.

We'll talk about that a little bit later. But first for you, really the key to recognize, you're going to need this configuration file langgraph.json. You need your code, which specifies your graph. You need to specify any dependencies. Now you can customize requirements.txt, of course, any way you want.

Let's actually show this [00:02:00] code over in the repo right now. So here we are in the LangChain Academy repo. We're looking at module six. Now I'm just going to open up that deployments folder, and you're going to see here, this actually already has all the code you need to create a deployment very conveniently there for you.

So first, this task_maistro.py is our graph. This is the same graph we worked with in the prior module. So there we go. Nothing too surprising there. So let's have a look at the API configuration file. Now, we've worked with this quite a bit before. You've seen this, for example, when we're using studio, we also need this langgraph.json.

Why is that? It's because this langgraph.json file is needed for the CLI to create a deployment and when we're using LangGraph Studio, the CLI was being run for us under the hood to create a local deployment. So actually the CLI was being used and all those cases, we just weren't aware of it because we weren't working with it directly.

But in any case, this specifies the graphs you want to [00:03:00] work with. In this case, we're going to name it task_maistro. It's going to point to our code, and this is the final compile graph name In task_maistro.py. We can specify added dependencies here, we can specify Python version, and this is actually going to build a docker image using the CLI, and you can supply dockerfile_lines here if you want to further customize that process.

Now requirements.txt just specifies the libraries we are going to be using. Now this is entirely customizable. We can add whatever you want here, of course. Now, as we mentioned, we've seen that we have the necessary files to create a deployment. We're going to use the LangGraph CLI to create a deployment. And what's actually going to happen here, we're basically just going to build a docker image for the LangGraph server.

So this is going to package our graph. In our case, it's going to be that task_maistro.py and all dependencies into a Docker image. So the Docker image is basically [00:04:00] a template to build a Docker container that contains all the code dependencies required to run our application. So in order to do this, just make sure Docker's installed.

Now if you've been using LangGraph Studio, it should already be installed because Studio's using it. But make sure you actually have it. If you don't, you can follow the link here in the notebook. And then we're just going to build a Docker image for our application. And we're basically just going to run langgraph build, and you can name your image, whatever you want.

So here I'm in the deployment folder. Let's just kick that off, langgraph build -t my-image, and this is going to run. Cool. Great. So now you've finished. We built the image. Now once we have the image, remember that this is only the server component. Remember, deployment has also Redis and Postgres for communication and short and long-term storage.

Now, how do these come in? So if you already have Redis and Postgres running, you can very simply just use [00:05:00] docker run to create a LangGraph server container by itself and pass the URIs for Redis and Postgres in. Okay? So that's one option. Now if you don't have Redis and Postgres already running, you can also use the docker-compose.yml file, which we provide to create three separate containers based on the services defined.

So all you're going to do, go over here and look at that docker-compose-example.yml, and all you need to do is just modify the image name to whatever image you decide to create. So in my case, we just called it my-image. And just add your OPENAI_API_KEY your LANGSMITH_API_KEY. That's all you're going to need.

And once all that's done, you have your docker-compose.yml created based upon the example. You have your image built. Just run docker compose up to launch your deployment with the three containers we talked about. One container for the server, one container for Redis, [00:06:00] one container for Postgres. And there you go.

Now we're going to be working with our deployment in the next lesson.



=============================================
File: LCA-LangGraph-C1-M6-L3-V2-Connecting.txt
=============================================

[00:00:00] So now that we've created a deployment, how do we actually connect with it and work with it? So let me just refresh what we did. We created deployment for our task_maistro app that we built in module five. We used LangGraph CLI to first build a Docker image for the LangGraph server that contains our task_maistro graph itself.

And then we use docker-compose.yml to create three separate containers based upon the three services that we need. We need the langgraph-api server, that contains the task_maistro app itself. We build that container from the Docker image. We also build containers for Redis and Postgres. That's it. We then ran docker compose up.

We launched our server that way, so now our server is actually running and we can actually connect with it very easily using these links. Now, let's just kind of refresh a little bit about what the server actually provides us. So the server gives us many different [00:01:00] API endpoints for interacting with our deployed agent.

Okay, so you can kind of group these endpoints into a few common needs. So think about first, Runs, atomic agent executions. Then there's also Threads, thread support, multi-turn interactions or human-in-the-loop. And there's also Store for long-term memory stores. These are kind of three types of needs or ways of interacting with our agents.

Now once a deployment's running, we can very simply go here as noted in the notebook and actually look at the API endpoints very nicely grouped into a few categories. We talked about runs. Now let's go ahead and look at them. So Stateless Runs and Thread Runs are two different way to execute runs.

Stateless runs do not save anything to a thread. So stateless runs are really useful when you just want to hit your agent, get a response, but you [00:02:00] don't care about a multi-turn conversation. Now you can browse through all the different endpoints that are available within stateless runs. For example, you can create a run and stream the output, which is obviously a very common use case.

You can create a run and just wait for the output. If you don't care about the output immediately, you can just kick off a background run. You can create a batch of runs. Now thread runs are a bit different. These are relevant when we actually want to engage in a multi-turn conversation. So we want to save each run to a thread, which is persisted in what we call short-term memory.

So for given thread_id, we can list all the runs. Just like before, we can create background runs. We can stream the output, we can wait for the output, we can delete runs. Now we can also do join run, which is basically waiting for a run to finish, and we'll see examples of this in the following lessons. We can also cancel runs as an [00:03:00] example.

Now, as mentioned, threads are what we use to save information across some set of runs that we want to persist for very commonly chat applications or multi-turn conversations. Now we can get lots of information about a thread. We can create one, we can search for all the available threads that are saved to Postgres.

We can get the thread state, so we can see the runs that have been saved to the thread. We can get a listing of all the updates that are present. We can get things like thread history. We can copy a thread if we want to fork it, and we're going to see some interesting examples of this a bit later. And we can also delete threads.

Now we spent a lot of time talking about the Store in module five, and the store is of course available to us in our deployment. We have Postgres, and just like we saw previously, we can update items, delete, [00:04:00] retrieve a single item, search for items, list namespaces. So very intuitive functionality that you might imagine.

It's all accessible to you through the deployment APIs. Now, one thing that's really cool, is you can also create crons, and this is available only in enterprise. But you can basically create periodic runs that recur on some given schedule. You can create search, delete crons, just as you might expect.

And finally, we can talk about assistants. So we're to be covering this in detail in the final lesson of this module. But assistants allow you to create different versions of your agent. So you can see, we can create search for assistants, delete them, and so forth. So you're going to be learning a lot more about assistants later on as this module progresses, and we're going to see it's a very powerful way to create different versions of your graph that are all stored within your deployment.[00:05:00] 

So we've seen that we can browse the various API endpoints available to us very easily. Now let's actually showcase using the SDK to connect with our deployment. Now recall, if you go back to our diagram here, we have our deployment. We create it with the CLI, now we can connect with it with various clients, one being the SDK, the other being remote graph. It can also directly use HTTP or CURL.

So with the SDK, we're just going to make sure it's installed. Use get_client. Now we just can connect to the URL that our local deployment's running on and run this. There we go. That's all we need to connect to the client. Now, we'll be working with this a little bit below.

Alternatively, I also want to show remote graph. Remote graph is useful if you're actually working in LangGraph and you want to do things like compose or deploy graph within another graph that you're building as a subgraph, or you just want to run your graph [00:06:00] directly within the LangGraph library. You can use this remote graph abstraction within LangGraph to interact with your graph.

And again, you're supplying the URL, the graph name, and that's all you need. Now let's showcase a few of the types of interactions that are available to you through deployment. Starting with runs, we introduced runs a bit previously. Runs our single executions of your graph, and we talked a little bit about what happens when you make a run request. The HTTP worker generates a run ID. It's stored in Postgres. Queue workers can poll Postgres, get runs assigned, run, send results to Redis. HTTP worker can be subscribed to those updates and return them to the client.

Now, the server reports two types of runs. One is, you might think of fire and forget, launch a run in the background, don't wait for it to finish. Another is waiting on our reply or blocking, so launch, run, and [00:07:00] wait to stream, for example, the output. Now background runs are really useful when working with long running agents. So let's actually showcase this.

So first we're going to create a new thread and we can look at this. So this is basically creating a dict. We have some metadata and you have a thread_id. Now this is just a UUID. So it's actually interesting to note that the server saves threads as UUIDs. Now I can also check if there's any existing runs on a given thread very easily. I'd basically just pass in the thread_id to client.runs.list. Print that out. There's no runs on this current thread.

Now, here's where we're actually going to interact with our app. Remember, the app we deployed is task_maistro. We showed that in detail in module five. This is an app for managing a ToDo list. So what I can do is I can give it ToDos. In this case, let's give it two ToDo's. I'm going to give it a ToDo to finish booking travel to Hong Kong and to call my parents back about [00:08:00] Thanksgiving. Okay, now I'm going to supply a user ID as Test. Remember that the task_maistro app namespaces things that it remembers, in this case ToDos, to a user ID. And since we're just testing here, I'm not going to put my actual name, I'll just put Test as like a temporary name.

The graph name is task_maistro, and I can basically just call client.runs.create with the thread_id, graph name, and the input. With this provided input. Kick this off as a run. Now that first run finished. Let's kick off a new thread, and I'm just going to ask to give a summary of all my ToDos. We created some ToDos here.

Now I'm just going to ask for a summary. Same as before, I kick off a new run. And what's neat is I can actually get run status at any point in time by passing in the thread_id and the run_id, which we have here. And in this case, you can see the status is pending. Now sometimes we want to wait until a run completes before we do anything else.

That's called making it a [00:09:00] blocking run. And to do that we can just run, runs.join, and then I can check the status again. So we can see status is success, meaning that the run's been completed. So those are some very simple examples of creating runs, making them blocking, checking on their status. Now let's showcase some other features such as streaming.

Now, streaming is very important, particularly for higher latency agents because we want to be able to present the user information while the agent's actually running. Now, remember under the hood with the deployment, a few things are happening when we stream. First, HTTP worker gets the request, creates a run ID, queue worker begins working on that run.

During execution, the queue worker publishes updates to Redis, HTTP worker subscribes to updates from Redis for the run, returns updates to the client. So that's really what's happening under the hood. Now there's a bunch of different stream modes, but let me show kind of one of the more [00:10:00] interesting ones, which is streaming individual tokens.

Here you can see all I need to do is I run client.runs.stream, and right here I can set the streaming mode. In this case, I'm going to do messages-tuple to stream individual tokens. And all I need to do is just some very simple logic here to basically fish out individual tokens, format and print them as we run.

So here I'm going to ask the app, What should I focus on first?

I'm passing in the same config that we had previously. Which should config to, if you recall here, can go back and look, the user_id, Test. So all the ToDos we created up here were namespace to this particular user, so they should be available to me when I request, What should I work on here? There we go.

Pretty cool. So you can see it streamed the tokens back out to me, which is pretty nice, and it tells me, Focus on, Call [00:11:00] my parents back for Thanksgiving. Now remember the difference between runs and threads. Runs are single executions of your graph, whereas the thread supports saving runs to support multi-turn interactions.

So what happens is when a client makes an execution request with a thread_id. The server saves all checkpoints in the run to the thread, which is in Postgres. So the thread then contains all the updates within that run. So what's pretty nice is, I can call client.threads.get_state, pass in a thread_id, and I can get the entire state written to the thread from all runs on that thread.

So let's actually talk about what's actually happening here. What happens here is I kick off a run. That run has some number of steps. Every step, the [00:12:00] graph is writing the state to the thread. So in this case, the graph state has a key messages. When I ran the graph, on this thread_id. First, I provided a human message, which was my request, and our assistant task_maistro responded with an AI message telling me what to focus on.

So let's actually look at that. So this is the full history saved to that thread. First I asked, Give me a summary of ToDos. Cool. Then I later asked, What ToDo should I focus on first? And it gave me that response. So this is saving basically everything passed to the messages key. All the state updates. I can replay them or view them here very easily just by using get_state and passing the thread_id.

Now another nice thing I can do is I can just copy a thread. So I have [00:13:00] this current thread_id, I can just call client.threads.copy, and I get a copied thread over here and let's check the state of the copied thread. It's identical, so that's great. So let's just rewind what happened here very carefully. My graph has a few different state keys, one being messages. When I run the graph, I pass in a message and that state key's updated with the graph's response.

Now updates to that messages state key that are made in each run are saved to the thread that we pass. And as I kick off more runs on that same thread, all those state updates are being saved to the thread. That's why when I basically just get_state passing the thread_id, I have a full history of all the updates to that messages state key.

That's all that's going on here, [00:14:00] which is obviously really nice because I can see the entire chat history. And more importantly, when we're using threads, the model or the assistant has access to that full history when composing responses. That's really the motivation for threads and persistence. And we talked about a lot of this previously in module two and three, but I just want to rehash it a little bit here.

Now, likewise, we covered human-in-the-loop a whole lot in module three, but I do want to show again that the server supports many different human-in-the-loop style interactions. Searching, editing, continuing graph execution from any prior checkpoint that was written to the thread. So for example, we can take our thread and you can call, get thread history.

Okay? This gets all the state updates that were made, which we saw up here as well. But this is another way to get them, and let's just pick a state update that we want to work with. So we ask, Give me a summary of all ToDos. Okay. [00:15:00] Now, let's say I want to fork this. So this is one thing you could do with human-in-the-loop interactions.

You can take a prior state and you can basically fork from it to create kind of a new execution of your graph. So here's the state update I want to fork. We have an ID, we know where it's going next, we have a checkpoint ID. Now I'm going to create a new payload that I want to apply to my fork. In this case, I'm going to ask, Give me a summary of all ToDos that need to be done in the next week, instead of just give me a summary of all ToDos.

Okay? Now all I need to do is pass the ID of this particular. Message. Now, remember, this gets into the guts of how the reducer works within LangGraph, and we talk about this a lot in module three, but I just want to refresh slightly.

So the messages reducer, on this particular state key of messages, [00:16:00] will append new messages, if we don't supply message ID. If we do supply message ID, we can overwrite a prior one, and that's exactly what we're going to do here. We're going to overwrite the prior message rather than appending a new one.

So here is what we're going to basically call our forked_config, and we're going to run threads.update_state. We're going to pass in this new input, the thread_id and the checkpoint_id. So now we forked one particular state in our thread and what's cool is you can actually run then your graph from that fork update. All we're doing is we're supplying that forked checkpoint_id, stream_mode, just like normal, and we're basically going to run our graph from that new checkpoint.

Cool, and we get an updated summary, ToDos that need to be done only in [00:17:00] the next week. So again, you should go back to the time travel lesson if you really want to dig in under the hood. That's in module three, but I just want to refresh and remind you that many human-in-the-loop interactions, even sophisticated ones like this, that involve modifying prior checkpoints and what we call time travel, in this case, are all available to you using the deployment.

Let's talk a little bit about a cross thread or long-term memory. So in module five, we saw that the task_maistro app uses LangGraph's memory store interface to save memories across threads. In particular, those memories are our ToDos. They're also, user profile and instructions, but really think about it as the task_maistro app is using the store to save certain information about the user across all threads with that user.

Okay. Now, previously we used a different implementation of the memory store. [00:18:00] In notebook, we just used kind of an in-memory implementation of the store. But here we have a deployment. Our deployment gives us access to Postgres. Postgres can be used to provide our store, which is actually quite nice 'cause that's saved for perpetuity on, for example, our laptop using Postgres.

Now we can very easily interact directly with a store in our deployment using the SDK. So for example, the task_maistro app used the store to save ToDos namespaced by todo, todo_category and user_id. Now the category by default is set to general, the user ID was passed in our config, and we can just search for what ToDos are present based on the work we just did.

And there we go. There's our ToDo's, Finish booking travel, um, Call the parents. So there we go. Now we can use the put method to add items, and here's just like a test case. Again, remember that the put method, or in this case, put_item with the [00:19:00] SDK, uses a tuple to namespace what you want to put, key, value.

We ran through this module five. I'm just showing you how to do it with the SDK, with our deployment. But in any case, tuple key value, we can then search. We can see that, there we go. We have two new memories created. We can also delete items simply by passing in the key. Let's copy the key over here. Now we can actually see them right here. Nice. Let's delete one of them first for fun. And there you go. We can then search and see it deleted.

So the key point is just to show you that, the deployment has first class support for long-term memory using the store interface. That's all saved to Postgres in the deployment and you can very easily interact directly with it using the SDK.

So we're going to keep building on this and show additional functionality that you can achieve using the deployment. But now you at least have a sense for how to browse the API docs, [00:20:00] how to work with runs, threads, human-in-the-loop, and the LangGraph Store.



=============================================
File: LCA-LangGraph-C1-M6-L4-V2-Double-Texting.txt
=============================================

[00:00:00] I want to talk a little bit about how LangGraph Platform is very useful for dealing with practical challenges that come with deployed applications. So this is a very common one, what we call double texting. So simply what happens is this, a user submits a run, for example, we see this on the left, the human message comes in at the start time.

And before that first run is completed, user submits a second run. And this can happen if, for example, the agent has slightly high latency, it's making a number of LLM calls, the user can get impatient and issue another run request. Now we have four different approaches for dealing with this in LangGraph Platform, which I want to walk through and then show you in the code.

So the first and most intuitive is what we just call reject, which is simply that until the first run finishes, we reject any double texting. Now, the second is kind of an intuitive extension of the first idea, which is what we call enqueue. So instead of [00:01:00] rejecting the second run, we just wait until the first run completes, and then kick off the second run.

Now the third type is interrupt. Again, this is pretty intuitive. If a second run request comes in before the first is done, just interrupt the first one. Now, one interesting thing about interrupt is that we still preserve that first run even though it's partially completed. Now if you don't want to preserve that first run, you can use rollback.

So it is that same kind of interrupt functionality, but the first run is deleted, so it's no longer preserved. You can't access it. So there'd be some subtle reasons why you may want to keep or not keep that initial run. But the point is we allow you to choose with two different ways of handling double texting with interrupt versus rollback.

Now let's walk through these various ways to deal with double texting using the deployment specifically. So first, just like before, I'm going to use the [00:02:00] SDK to connect to my deployment. Now let's look at reject. So again, reject was just basically reject any new runs until the current run completes. So I'm creating a new thread, create two ToDos for our task_maistro app.

And what I'm going to do is I'm going to issue them in very quick succession. So this first run won't be complete by the time this second run request is made. And because I specify multitask_strategy of reject on that second run request, we should see that it's rejected because the first has not completed yet. So let's have a look at that, and we can see we get this error as expected.

We can also go ahead and look at the initial request to confirm that it was made. So what has happened here is that our initial run request is made as expected. Our second one is rejected because we've set multitask strategy to reject when we make that request, meaning that [00:03:00] because the first run was still running, this second run is rejected.

Now let's show enqueue. So again, this approach is a bit different than before. Here we just wait for that first run request to end, and then issue that second request. And again, all we need to do is just change multitask_strategy here to enqueue. And you saw this previously when we were going over the API, but you can just call client.runs.join to basically block until the second run finishes before moving on and getting the state of the overall thread.

So both of these runs are going to be written to this thread, which we're going to look at the end. So very nice. In this case, you can see something slightly different. So if you look at the thread, we can see that both requests were made. We can see the thread contains both of the runs. So again, this is all the state updates for [00:04:00] each run. This is our second run. This is our first run. All added to our thread because, we passed in enqueue, which meant that second run just waited for the first to finish and then it was executed. So that's great.

Now let's show interrupt. In this case, I'm going to issue two requests, run one, run two. But the key point is that when I issue the request for run two, everything that was done in run one is still saved to the thread. So it's persisted for me, and we can see that right here. So what I'm going to do is I issue run one right here. And I'll just go ahead and wait. In this case, just wait a second, and then issue run two. So, at least a second's worth of work from run one should be saved to the thread and we can have a look at that here.

So basically, I passed in my initial message. Give me a summary of the ToDos for tomorrow. That was my first request. And then I follow up very quickly with, Nevermind, create a ToDo to order ham for Thanksgiving. Now we can see that initial [00:05:00] message from one is still saved to the thread. But then it was interrupted. Then we see that second run is completed. So we can simply get any state updates from that first run saved to the thread up until the interruption occurs. We can also see that the status of the first run is interrupted.

Now, rollback is similar to this, except we're not going to save anything from the first run to the thread. So you can see this is pretty intuitive. We go ahead and simply call rollback with our second run, and nothing from that first run is present in our thread. So we can see this is only the second input that we pass and the response. So nothing from that first run, here, is present in our thread, and we can confirm that the first run is indeed deleted. So rollback is similar to interrupt, but [00:06:00] we don't save any state updates from that first run up until the interruption to the thread. So it's a little bit cleaner.

And we can see a visualization of this. So reject, we very simply just did not allow for double texting. We rejected that second run until the first run completed. Enqueue, we just waited until the first run was done before kicking off the second run. Interrupt, we still saved everything from that first run to the thread up until the interruption point. Rollback, we don't save anything from that first run to the thread, so it's a little bit cleaner.



=============================================
File: LCA-LangGraph-C1-M6-L5-V2-Assistants.txt
=============================================

[00:00:00] Now I want to talk about assistants, which give developers a very easy way to manage different agent versions. So what's a practical motivation for this? Well, we've taken our task_mAIstro app and we've deployed it. Now, what if I want different versions of this app? For example, I might want a work assistant to help me with my work tasks. I also may want a personal assistant to help me with personal tasks or personal ToDos.

Importantly, these assistants are going to be configured a bit differently. So not only will the memories or ToDos be stored in different namespaces, but also they're going to have different roles. We can very easily create different assistants for these two use cases using the assistant API as part of our deployment.

We're going to show you how to do that all here from scratch. And ultimately what we're going to have is a deployment with different assistant versions, which we can easily interact with and manage via the SDK. So let's hop into the code right now.

Now let's walk through [00:01:00] the code and I'm going to show on concrete terms how to build the two assistants. So first we have to talk about configurations. Let's go over to code to look at that. I'm in the langchain-academy repo, module six. Open up deployment, and you're going to see configuration.py. Open that up. So here you can see I have three categories, user_id, todo_category, task_maistro_role. User_id is just the name of the user.

And now what's nice is we can set defaults. So in this case, default-user is the default. Now the todo_category, this is for namespacing ToDos in the store. I want ToDos for different assistants to be namespaced to different parts of the store to keep my ToDos separate. So by default this is going to be general, but you'll see when we create assistants, I'll have one for personal and one for work.

Now this third configuration is task_maistro_role, and this is just an open-ended prompt that I can provide to give instructions about how I want the particular [00:02:00] assistant to function. Now, what's nice about this is I want my personal assistant and my work assistant to do things slightly differently, and I can supply that through this third configuration field.

Now, one other thing I want to show you. I'm in the graph definition itself, and if I look at my nodes, you'll see that if I go over to the call here, we can pass in the config to a node and we can actually access the values we just talked through very easily, just like this. And then we can use them in the code accordingly.

So for example, you see, user_id and todo_category are used to namespace the ToDos, or memories that we search for. We can see that here for the different types of memories I'm saving, ToDos, instructions and backup of profile. We can also see that we pass task_maistro_role into the prompt, so we use it as part of the system prompt for the model, instructing it how we want [00:03:00] to respond to the user.

Now let's go about creating our assistants. So like I said, I want to work assistant and a personal assistant, and these are very easily configurable using the todo_category and the task_maistro_role fields. So let's actually go ahead and do this. So just like before, I'm going to connect to my deployment.

And now let's create that personal assistant first. Now, creating an assistant is very easy. All I'd call is client.assistants.create. Pass in, this is the name of the graph, and then I just supply the configuration default values that I want. So here's an example of just a configuration that sets the todo_category to personal.

Okay. I can create that assistant right here. There we go. We get assistant_id. We know the graph_Id already, get some metadata, and we can see the default configuration used with this assistant. Okay. Now what's pretty nice is I can also update any existing assistant, and that's just using this client.assistants.update.

Now, here's where things get interesting, and this is kind of a [00:04:00] realistic use case for me. I want my work and personal assistants to operate a bit differently, so the prompt I supply when creating the assistants are going to be a little bit different. So in the prompt here for personal tasks, I lay out some preferences. If I ever ask for ToDo summary, basically list all my tasks by deadline, highlight tasks that are missing deadlines, and gently encourage adding them. Note any tasks that seem important, but lack time estimates.

So one of the challenges I have with personal tasks is I let things slip, and so I provide some gentle reminders to add deadlines and time estimates to personal tasks. Now of course, the nice thing is, this is entirely customizable. You can modify this in any way you want, but what's cool is when I create my assistant, all this logic is default added to the system prompt for me.

So here we go. I set the configurations, run update. There we go. We've created a new [00:05:00] assistant and I can see the default configurations for this new personal assistant, so that's great. And I have my assistant_id. Now I create my work assistant, and here I want the role to be a little different. I don't necessarily want it to be, kinda like, supportive and soft.

I want it to really force me to input realistic timeframes for every new task. And I also want to really enforce that timeframes and deadlines are actually added to tasks I create. This is a classic problem with work tasks. People misestimate deadlines all the time. They misestimate, kind of, the amount of work necessary.

So in my instructions here, I really tell the assistant to be rigorous about, kind of, deadline and timeline setting in terms of task creation. So there I go. I've set my assistant here. Now I can very easily search for assistants that I've created. So there we go. I have now my two assistants. I can very easily manage the assistant so I can just delete them by [00:06:00] assistant_id.

Very intuitive and simple. So I can set assistant IDs for my work and personal assistant. And now let's actually see this in action for my work assistant. Here's an example input. Create or update a few ToDos, Refilm one of the modules for this course by end of day today, Update the audioX by next Monday.

So that's a different project I'm working on. Kick this off. Cool, so that all runs. You create some tasks and let's try an example of me inputting a new task where I don't supply a deadline. Create another ToDo, Finalize a set of report generation tutorials. So there it is. It catches me, which is very cool, because my instructions say, don't let the user input tasks that don't have deadlines.

So basically, Could you provide a deadline for this? Great. And I say, okay, for this task, let's get it done by next Tuesday as an example. There we go. Very cool. Created my ToDo. [00:07:00] And there we go. So again, you can see what's really cool about assistants is that not only can I namespace different versions of the task_maistro app, but I can give them different instructions because I want different behavior. In this case, I want to be very rigorous about both deadline and timeframe estimation.

Now for my personal assistant, I can create some ToDos just like before. Great. And you recall in our instructions, we gave some formatting for ToDo summaries that I want so I can go ahead and ask that. Cool. And we can see the ToDos laid out in kind of a nice format here. Current summary, overdue tasks, tasks for today, non-scheduled tasks for this week. So great. You can see that the ability to supply prompts to your assistants gives you very nice flexibility in how you want the assistants to behave, which is, in this particular case, different between my work tasks and my personal tasks.

So I want to briefly review what we showed throughout this module. First we showed how to use a LangGraph CLI to create a deployment of our task_mAIstro graph. [00:08:00] We showed how to connect with the deployment using the SDK. We showed that the deployment allows us to do all the things we were previously doing with the LangGraph library, like streaming and human-in-the-loop and memory.

We also showed some new features you get specifically with deployments via LangGraph Platform, such as support for double texting and support for assistants, which we used to create a work and a personal assistant of our task_mAIstro graph, which is deployed and accessible to us for sharing with other users, creating a front end or other applications we actually want to build on top of our deployment.

So hopefully you enjoyed this module. You learned a lot more about actually building deployments using LangGraph Platform from your graphs, and feel free to leave any feedback. Thanks.



